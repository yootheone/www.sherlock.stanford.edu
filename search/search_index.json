{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","tags":true},"docs":[{"location":"","text":"","title":"Sherlock"},{"location":"#what-is-sherlock","text":"<p>Sherlock is a shared computing cluster available for use by all Stanford Faculty members and their research teams, for sponsored or departmental faculty research.  All research teams on Sherlock have access to a base set of managed computing resources, GPU-based servers, and a multi-petabyte, high-performance parallel file system for short-term storage.</p> <p>Faculty can supplement these shared nodes by purchasing additional servers, and become Sherlock owners. By investing in the cluster, PI groups not only receive exclusive access to the nodes they purchase, but also get access to all of the other owner compute nodes when they're not in use, thus giving them access to the whole breadth of Sherlock resources.</p>","title":"What is Sherlock?"},{"location":"#why-should-i-use-sherlock","text":"<p>Using Sherlock for your work provides many advantages over individual solutions: hosted in an on-premises, state-of-the-art datacenter dedicated to research computing systems, the Sherlock cluster is powered and cooled by installations that are optimized for scientific computing.</p> <p>On Sherlock, simulations and workloads benefit from performance levels that only large scale HPC systems can offer: high-performance I/O infrastructure, petabytes of storage, large variety of hardware configurations, GPU accelerators, centralized system administration and management provided by the Stanford Research Computing Center (SRCC).</p> <p>Such features are not easily accessible at the departmental level, and often require both significant initial investments and recurring costs. Joining Sherlock allows researchers and Faculty members to avoid those costs and benefit from economies of scale, as well as to access larger, professionally managed computing resources that what would not be available on an individual or even departmental basis.</p>","title":"Why should I use Sherlock?"},{"location":"#how-much-does-it-cost","text":"<p>Sherlock is free to use for anyone doing sponsored research at Stanford.</p> <p>Any Faculty member can request access for research purposes, and get an account with a base storage allocation and unlimited compute time on the global, shared pool of resources.</p> <p>Stanford Research Computing provides faculty with the opportunity to purchase from a catalog a recommended compute node configurations, for the use of their research teams. Using a traditional compute cluster condominium model, participating faculty and their teams get priority access to the resources they purchase. When those resources are idle, other \"owners\" can use them, until the purchasing owner wants to use them. When this happens, those other owners jobs are re-queued to free up resources. Participating owner PIs also have shared access to the original base Sherlock nodes, along with everyone else.</p>","title":"How much does it cost?"},{"location":"#how-big-is-it","text":"<p>Quite big! It's actually difficult to give a definitive answer, as Sherlock is constantly evolving and expanding with new hardware additions.</p> <p>As of March 2022, Sherlock features over 5,200 CPU cores available to all researchers, and more than 36,200 additional CPU cores available to Sherlock owners, faculty who have augmented the cluster with their own purchases. With a computing power over 3.3 Petaflops, Sherlock would have its place in the Top500 list of the 500 most powerful computer systems in the world.</p> <p>For more details about Sherlock size and technical specifications, please refer to the tech specs section of the documentation. And for even more numbers and figures, see the Sherlock facts page.</p>","title":"How big is it?"},{"location":"#ok-im-sold-how-do-i-start","text":"<p>You can request an account right now, take a look at the documentation, and drop us an email if you have any questions.</p>","title":"OK, I'm sold, how do I start?"},{"location":"#i-want-my-own-nodes","text":"<p>If you're interested in becoming an owner on Sherlock, and benefit from all the advantages associated, please take a look at the catalog of configurations, feel free to use the ordering form to submit your request, and we'll get back to you.</p>","title":"I want my own nodes!"},{"location":"docs/","text":"<p></p>","title":"Sherlock documentation"},{"location":"docs/#welcome-to-sherlock","text":"<p>Sherlock is a High-Performance Computing (HPC) cluster, operated by the Stanford Research Computing Center to provide computing resources to the Stanford community at large. You'll find all the documentation, tips, FAQs and information about Sherlock among these pages.</p>","title":"Welcome to Sherlock!"},{"location":"docs/#why-use-sherlock","text":"<p>Using Sherlock for your work provides many advantages over individual solutions: hosted in an on-premises, state-of-the-art datacenter, the Sherlock cluster is powered and cooled by installations that are optimized for scientific computing.</p> <p>On Sherlock, simulations and workloads benefit from performance levels that only large scale HPC systems can offer: high-performance I/O infrastructure, petabytes of storage, large variety of hardware configurations, GPU accelerators, centralized system administration and management provided by the Stanford Research Computing Center (SRCC).</p> <p>Such features are not easily accessible at the departmental level, and often require both significant initial investments and recurring costs. Joining Sherlock allows researchers and faculty members to avoid those costs and benefit from economies of scale, as well as to access larger, professionally managed computing resources that what would not be available on an individual or even departmental basis.</p>","title":"Why use Sherlock?"},{"location":"docs/#how-much-does-it-cost","text":"<p>Sherlock is free to use for anyone doing sponsored research at Stanford. Any faculty member can request access for research purposes, and get an account with a base storage allocation and unlimited compute time on the global, shared pool of resources.</p>  <p>No CPU.hour charge</p> <p>Unlike all Cloud Service Providers and many HPC systems, there is no usage charge on Sherlock.</p> <p>When you submit your work on Sherlock, you don't need to keep an eye on the clock and worry about how much that run will cost you. There is no limit on the total amount of computing you can run on the cluster, as long as resources are available, and there's no charge to use them, no matter how large or small your computations are.</p>  <p>In case those free resources are not sufficient, Stanford Research Computing offers Faculty members the opportunity to invest into the cluster, and get access to additional computing resources for their research teams. Using a traditional compute cluster condominium model, participating faculty and their teams get priority access to the resources they purchase. When they're idle, those resources are available to use by other owners on the cluster, giving them access to virtually unlimited resources.</p>","title":"How much does it cost?"},{"location":"docs/#information-sources","text":"<p>Searching the docs</p> <p>If you're looking for information on a specific topic, the Search feature of this site will allow you to quickly find the page you're looking for. Just press S, F or / to open the Search bar and start typing.</p>  <p>To help users take their first steps on Sherlock, we provide documentation and information through various channels:</p>    Channel URL Purpose     Documentation You are here www.sherlock.stanford.edu/docs information to help new users start on Sherlock, and more in-depth documentation for users already familiar with the environment.   Changelog news.sherlock.stanford.edu announces, news and updates about Sherlock.   Dashboard status.sherlock.stanford.edu status of Sherlock's main components and services, outages, maintenances.    <p>To get started, you can take a look at the concepts and glossary pages to get familiar with the terminology used throughout the documentation pages. Then, we recommend going through the following sections:</p> <ul> <li>Prerequisites</li> <li>Connecting to the cluster</li> <li>Submitting jobs</li> </ul>","title":"Information sources"},{"location":"docs/#acknowledgment-citation","text":"<p>It is important and expected that publications resulting from computations performed on Sherlock acknowledge this. The following wording is suggested:</p>  <p>Acknowledgment</p> <p>Some of the computing for this project was performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results.</p>","title":"Acknowledgment / citation"},{"location":"docs/#support","text":"","title":"Support"},{"location":"docs/#email-recommended","text":"<p>Research Computing support can be reached by sending an email to srcc-support@stanford.edu and mentioning Sherlock.</p>  <p>How to submit effective support requests</p> <p>To ensure a timely and relevant response, please make sure to include some additional details, such as job ids, commands executed and error messages received, so we can help you better. For more details, see the Troubleshooting page.</p>  <p>As a member of the Sherlock community, you're also automatically subscribed to the sherlock-announce mailing-list, which is only used by the SRCC team to send important announcements about Sherlock,</p>","title":"Email (recommended)"},{"location":"docs/#onboarding-sessions","text":"<p>We offer regular onboarding sessions for new Sherlock users.</p>  <p>On-boarding session times</p> <p>On-boarding sessions are offered every first Wednesday of the month, 1PM-2PM PST, via Zoom</p>  <p>These one-hour sessions are a brief introduction to Sherlock's layout, its scheduler, the different file systems available on the cluster, as well as some job submission and software installation best practices for new users. They are a good intro course if you are new to Sherlock or HPC in general.</p> <p>If you can't attend live on-boarding sessions, you can still take a look at the  on-boarding slides as well as to this  session recording.</p>","title":"Onboarding sessions"},{"location":"docs/#office-hours","text":"<p>Sending a question to srcc-support@stanford.edu is always the best first option for questions. That way you can include detailed descriptions of the problem or question, valuable output and error messages and any steps you took when you encountered your error. Also, everyone on our team will see your ticket, enabling the most appropriate group member to respond.</p> <p>Office hours are a good place for more generalized questions about Sherlock, Slurm, Linux usage, data storage, queue structures/scheduling, job optimization and general capabilities of Sherlock. It's also useful for more technically nuanced questions that may not be easily answered with our ticketing system. In office hours some problems can indeed be solved quickly or progress can be made so that you can then work self-sufficiently towards a solution on your own.</p>  <p>COVID-19 update</p> <p>We'll be holding remote office hours via Zoom, for the time being.</p>   <p>Office hours times</p> <p>Click here to join the Sherlock Office Hours Zoom</p> <ul> <li>Tuesday 10-11am</li> <li>Thursday 3-4pm</li> </ul>  <p>You'll need a full-service SUNet ID (basically, a @stanford.edu email address) in order to authenticate and join Office Hours via Zoom.  If you do not have a full service account, please contact us at srcc-support@stanford.edu.</p> <p>If you can't make any of the Office Hours sessions, you can also make an appointment with Sherlock's support team.</p>","title":"Office hours"},{"location":"docs/#what-to-expect","text":"<ul> <li> <p>We cannot accomodate walk-ins: we're unfortunately not staffed to accommodate   walk-ins, so please make sure that you're planning to stop by during   office hours. We will not be able to help you otherwise.</p> </li> <li> <p>We can rarely help with application-specific or algorithm problems.</p> </li> <li> <p>You should plan your projects sufficiently in advance and not come to office   hours at the last minute before a deadline. Sherlock is a busy resource with   several thousand users and you should not expect your jobs to complete before   a given date.</p> </li> <li> <p>Not all questions and problems can be answered or solved during office hours,   especially ones involving hardware, filesystem or network issues. Sherlock   features several thousand computing, networking and storage components, that   are constantly being monitored by our team. You can be sure that when   Sherlock has an issue, we are aware of it and working on it.</p> </li> </ul>","title":"What to expect"},{"location":"docs/#user-community","text":"<p>Sherlock is present on the Stanford Slack Grid, and you're more than welcome to join the following channels:</p> <ul> <li><code>#sherlock-announce</code>, for announcements related to     Sherlock and its surrounding services,</li> <li><code>#sherlock-users</code>, as a place for Sherlock users to     connect directly with each other. If you have a general question about     software used on Sherlock, want to reach out to other Sherlock users to     share tips, good practices, tutorials or other info, please feel free to     do so there.</li> </ul> <p>For more details about the SRCC Slack Workspace, and instructions on how to join this workspace and its channels, please see https://srcc.stanford.edu/support.</p>  <p>Slack is not an official support channel</p> <p>Please note that while SRCC staff will monitor these channels, the official way to get support is still to email us at srcc-support@stanford.edu.</p>","title":"User community"},{"location":"docs/#quick-start","text":"<p>If you're in a rush1, here's a 3-step ultra-quick start:</p> <ol> <li>connect to Sherlock</li> </ol> <pre><code>$ ssh login.sherlock.stanford.edu\n</code></pre> <ol> <li>get an interactive session on a compute node</li> </ol> <pre><code>[kilian@sh-ln01 login! ~]$ sdev\n</code></pre> <ol> <li>run a command</li> </ol> <pre><code>[kilian@sh02-01n58 ~]$ module load python\n[kilian@sh02-01n58 ~]$ python -c \"print('Hello Sherlock')\"\nHello Sherlock\n</code></pre> <p>Congrats! You ran your first job on Sherlock!</p>","title":"Quick Start"},{"location":"docs/#replay","text":"<p>Here's what it looks like in motion:</p>    <ol> <li> <p>even in a rush, you'll still need an account on the cluster. See   the Prerequisites page for details.\u00a0\u21a9</p> </li> </ol>","title":"Replay"},{"location":"docs/concepts/","text":"","title":"Concepts"},{"location":"docs/concepts/#sherlock-a-shared-resource","text":"<p>Sherlock is a shared compute cluster available for use by all Stanford faculty members and their research teams to support sponsored research.</p>  <p>Sherlock is a resource for research</p> <p>Sherlock is not suitable for course work, class assignments or general-use training sessions.</p> <p>Users interested in using computing resources in such contexts are encouraged to investigate FarmShare, Stanford\u2019s community computing environment, which is primarily intended for supporting coursework.</p>  <p>It is open to the Stanford community as a computing resource to support departmental or sponsored research, thus a faculty member's sponsorship is required for all user accounts.</p>  <p>Usage policy</p> <p>Please note that your use of this system falls under the \"Computer and Network Usage Policy\", as described in the Stanford Administrative Guide. In particular, sharing authentication credentials is strictly prohibited.  Violation of this policy will result in termination of access to Sherlock.</p>  <p>Sherlock has been designed, deployed, and is maintained and operated by the Stanford Research Computing Center (SRCC) staff. The SRCC is a joint effort of the Dean of Research and IT Services to build and support a comprehensive program to advance computational research at Stanford.</p> <p>Sherlock has been initially purchased and supported with seed funding from Stanford's Provost. It comprises a set of freely available compute nodes, a few specific resources such as large-memory machines and GPU servers, as well as the associated networking equipment and storage.  These resources can be used to run computational codes and programs, and are managed through a job scheduler using a fair-share algorithm.</p>","title":"Sherlock, a shared resource"},{"location":"docs/concepts/#data-risk-classification","text":"<p>Low and Moderate Risk data</p> <p>Sherlock is approved for computing with Low and Moderate Risk data only.</p>   <p>High Risk data</p> <p>Sherlock is NOT approved to store or process HIPAA, PHI, PII nor any kind of High Risk data.  The system is approved for computing with Low and Moderate Risk data only, and is not suitable to process High Risk data.</p> <p> Users are responsible for ensuring the compliance of their own data.</p> <p>For more information about data risk classifications, see the Information Security Risk Classification page.</p>","title":"Data risk classification"},{"location":"docs/concepts/#investing-in-sherlock","text":"<p>For users who need more than casual access to a shared computing environment, Sherlock also offers Faculty members the possibility to invest in additional, dedicated computing resources.</p> <p>Unlike traditional clusters, Sherlock is a collaborative system where the majority of nodes are purchased and shared by the cluster users. When a user (typically a PI) purchases one or more nodes, they become an owner. Owners choose from a standard set of server configurations supported by SRCC staff (known as the Sherlock catalog) to add to the cluster.</p> <p>When they're not in use, PI-purchased compute nodes can be used by other owners. This model also allows Sherlock owners to benefit from the scale of the cluster by giving them access to more compute nodes than their individual purchase, which gives them much greater flexibility than owning a standalone cluster.</p>  <p>The majority of Sherlock nodes are owners nodes</p> <p>The vast majority of Sherlock's compute nodes have been purchased by individual PIs and groups, and PI purchases are the main driver behind the rapid expansion of the cluster, which went from 120 nodes to more than 1,000 nodes in less than 3 years.</p>  <p>The resource scheduler configuration works like this:</p> <ul> <li>owners and their research teams get immediate and exclusive access to the   resources they purchased,</li> <li>when those nodes are idle, other owners can use them,</li> <li>when the purchasing owners want to use their resources, jobs from other   owners that may be running on them are preempted (ie. killed and   re-queued).</li> </ul> <p>This provides a way to get more resources to run less important jobs in the background, while making sure that an owner always gets immediate access to his/her own nodes.</p> <p>Participating owners also have shared access to the public, shared Sherlock nodes, along with everyone else.</p>","title":"Investing in Sherlock"},{"location":"docs/concepts/#benefits","text":"<p>Benefits to owners include:</p> <ul> <li> no wait time in queue with immediate and exclusive access to the   purchased nodes</li> <li> access to more resources with the possibility to submit jobs to the   other owners' nodes when they're not in use</li> </ul> <p>Compared to hosting and managing computing resources on your own, purchasing nodes on Sherlock provides:</p> <ul> <li>data center hosting, including backup power and cooling</li> <li>system configuration, maintenance and administration</li> <li>hardware diagnostics and repairs</li> </ul> <p>Those benefits come in addition to the other Sherlock advantages:</p> <ul> <li>access to high-performance, large parallel scratch storage space</li> <li>access to snapshot'ed, replicated, enterprise-class storage space</li> <li>optimized software stack, especially tailored for a range of research needs</li> <li>tools to build and install additional software applications as needed</li> <li>user support</li> </ul>","title":"Benefits"},{"location":"docs/concepts/#limitations","text":"<p>Being an owner on Sherlock is different from traditional server hosting.</p>  <p>In particular, purchasing your own compute nodes on Sherlock will NOT allow:</p> <p> root access: owner nodes on   Sherlock are still managed by SRCC in accordance with Stanford's Minimum Security Standards. Although users are welcome to install (or request) any software they may need, purchasing compute nodes on Sherlock does not allow <code>root</code> access to the nodes.</p> <p> running permanent services:   permanent processes such as web servers or databases can only run on owner nodes through the scheduler, using recurring or persistent. Purchasing compute nodes on Sherlock does not provide a way to run anything that couldn't run on publicly-available nodes.</p> <p> direct network connectivity from the outside world: owners' nodes are connected to the Sherlock's internal network and are not directly accessible from the outside, which means that they can't host public services like web or application servers.</p> <p> bypassing the scheduler: jobs running on owners' nodes still need to be submitted to the scheduler. Direct shell access to the nodes is not possible outside of scheduled interactive sessions.</p> <p> hardware modifications: the hardware components of purchased nodes cannot be changed, removed, swapped or upgraded during the nodes' service lifetime.</p> <p> persistent local storage: local storage space provided on the compute nodes is only usable for the duration of a job and cannot be used to store long-term data.</p> <p> additional storage space: purchasing compute nodes on Sherlock does not provide additional storage space. Please note that SRCC does offer the possibility for PIs to purchase their own storage space on Oak, for their long-term research data needs.</p>","title":"Limitations"},{"location":"docs/concepts/#purchasing-nodes","text":"<p>If you are interested in becoming an owner, you can find the latest information about ordering Sherlock nodes on the ordering page. Feel free to contact us is you have any additional question.</p>","title":"Purchasing nodes"},{"location":"docs/concepts/#cluster-generations","text":"<p>The research computing landscape evolves very quickly, and to both accommodate growth and technological advances, it's necessary to adapt the Sherlock environment to these evolutions.</p> <p>Every year or so, a new generation of processors is released, which is why, over a span of several years, multiple generations of CPUs and GPUs make their way into Sherlock. This provides users with access to the latest features and performance enhancements, but it also adds some heterogeneity to the cluster, which is important to keep in mind when compiling software and requesting resources to run them.</p> <p>Another key component of Sherlock is the interconnect network that links all of Sherlock's compute nodes together and act as a backbone for the whole cluster. This network fabric is of finite capacity, and based on the individual networking switches characteristics and the typical research computing workflows, it can accommodate up to about 850 compute nodes.</p> <p>As nodes get added to Sherlock, the number of available ports decreases, and at some point, the fabric gets full and no more nodes can be added. Sherlock reached that stage for the first time in late 2016, which prompted the installation of a whole new fabric, to allow for further system expansion.</p> <p>This kind of evolution is the perfect opportunity to upgrade other components too: management software, ancillary services architecture and user applications. In January 2017, those components were completely overhauled and a new, completely separate cluster was kick-started, using using a different set of hardware and software, while conserving the same storage infrastructure, to ease the transition process.</p> <p>After a transition period, the older Sherlock hardware, compute and login nodes, have been be merged in the new cluster, and from a logical perspective (connection, job scheduling and computing resources), nodes attached to each of the fabrics have been reunited to form a single cluster again.</p> <p>As Sherlock continues to evolve and grow, the new fabric will also approach capacity again, and the same process will happen again to start the next generation of Sherlock.</p>","title":"Cluster generations"},{"location":"docs/concepts/#maintenances-and-upgrades","text":"<p>The SRCC institutes a monthly scheduled maintenance window on Sherlock, to ensure optimal operation, avoid potential issues and prepare for future expansions. This window will be used to make hardware repairs, software and firmware updates, and perform general manufacturer recommended maintenance on our environment.</p> <p>As often as possible, maintenance tasks are performed in a rolling, non-disruptive fashion, but downtimes are sometimes an unfortunate necessity to allow disruptive operations that can't be conducted while users are working on the system.</p>  <p>Maintenance schedule</p> <p>As often as possible, maintenances will take place on the first Tuesday of every month, from 08:00 to 12:00 Pacific time (noon), and will be announced 2 weeks in advance, through the usual communication channels.</p>  <p>In case an exceptional amount of work is required, the maintenance window could be extended to 10 hours (from 08:00 to 18:00).</p> <p>During these times, access to Sherlock will be unavailable, login will be disabled and jobs won't run. A reservation will be placed in the scheduler so running jobs can finish before the maintenance, and jobs that wouldn't finish by the maintenance window would be pushed after it.</p>","title":"Maintenances and upgrades"},{"location":"docs/concepts/#common-questions","text":"<p>Q: Why doing maintenances at all?</p> <p>A: Due to the scale of our computing environment and the increasing complexity of the systems we deploy, it is prudent to arrange for a regular time when we can comfortably and without pressure fix problems or update facilities with minimal impact to our customers. Most, if not all, major HPC centers have regular maintenance schedules.  We also need to enforce the Minimum Security rules instituted by the Stanford Information Security Office, which mandate deployment of security patches in a timely manner.</p> <p>Q: Why Tuesdays 08:00-12:00? Why not do this late at night?</p> <p>A: We have observed that the least busy time for our services is at the beginning of the week in the morning hours. Using this time period should not interrupt most of our users. If the remote possibility of a problem that extends past the scheduled downtime occurs, we would have our full staff fresh and available to assist in repairs and quickly restore service.</p> <p>Q: I have jobs running, what will happen to them?</p> <p>A: For long-running jobs, we strongly recommend checkpointing your results on a periodic basis. Besides, we will place a reservation in the scheduler for each maintenance that would prevent jobs to run past it. This means that the scheduler will only allow jobs to run if they can finish by the time the maintenance starts. If you submit a long job soon before the maintenance, it will be delayed until after the maintenance. That will ensure that no work is lost when the maintenance starts.</p>","title":"Common questions"},{"location":"docs/credits/","text":"","title":"About us"},{"location":"docs/credits/#srcc","text":"<p></p> <p>The Stanford Research Computing Center (SRCC) is a joint effort of the Dean of Research and IT Services to build and support a comprehensive program to advance computational research at Stanford.  That includes offering and supporting traditional high performance computing (HPC) systems, as well as systems for high throughput and data-intensive computing.</p> <p>The SRCC also helps researchers transition their analyses and models from the desktop to more capable and plentiful resources, providing the opportunity to explore their data and answer research questions at a scale typically not possible on desktops or departmental servers. Partnering with national initiatives like NSF XSEDE program as well as vendors, the SRCC offers training and learning opportunities around HPC tools and technologies.</p> <p>For more information, please see the SRCC website</p>","title":"SRCC"},{"location":"docs/credits/#credits","text":"<p>We would like to thank the following companies for their generous sponsorship, and for providing services and resources that help us manage Sherlock every day:</p>   <ul> <li> GitHub</li> <li> Hund</li> <li> Noticeable</li> </ul> <p>The Sherlock website and documentation also rely on the following projects:</p> <ul> <li> MkDocs</li> <li> Material for MkDocs</li> </ul>","title":"Credits"},{"location":"docs/credits/#why-the-sherlock-name","text":"<p>If you're curious about where the Sherlock name came from, we always considered that computing resources in general and HPC clusters in particular should be the catalyst of innovation, be ahead of their time, and spur new discoveries.</p> <p>And what better account of what's happening on a high-performance computing cluster than Benedict Cumberbatch describing his role as Sherlock Holmes in the BBC's modern adaptation of Arthur Conan Doyle's classic?</p>  <p>Benedict Cumberbatch, about Sherlock</p> <p>There's a great charge you get from playing him, because of the volume of words in your head and the speed of thought \u2013 you really have to make your connections incredibly fast. He is one step ahead of the audience, and of anyone around him with normal intellect. They can't quite fathom where his leaps are taking him.</p>  <p>Yes, exactly. That's Sherlock.</p>","title":"Why the Sherlock name?"},{"location":"docs/credits/#sherlock-of-hbo-fame","text":"<p>And finally, we couldn't resist to the pleasure of citing the most prestigious accomplishment of Sherlock to date: a mention in HBO's Silicon Valley Season 4 finale!</p> <p> </p> <p>Yep, you got that right, Richard Hendricks wanted to use our very own Sherlock!</p> <p> Kudos to the show's crew and a big thank you to HBO Data compression stars, Professor Tsachy Weissman and Dmitri Pavlichin, for this incredible Sherlock shout-out. This has been an everlasting source of pride and amazement for the whole SRCC team! </p>","title":"Sherlock, of HBO fame"},{"location":"docs/glossary/","text":"","title":"Glossary"},{"location":"docs/glossary/#whats-a-cluster","text":"<p>A computing cluster is a federation of multiple compute nodes (independent computers), most commonly linked together through a high-performance interconnect network.</p> <p>What makes it a \"super-computer\" is the ability for a program to address resources (such as memory, CPU cores) located in different compute nodes, through the high-performance interconnect network.</p> <p></p> <p>On a computing cluster, users typically connect to login nodes, using a secure remote login protocol such as SSH. Unlike in traditional interactive environments, users then need to prepare compute jobs to submit to a resource scheduler. Based on a set of rules and limits, the scheduler will then try to match the jobs' resource requirements with available resources such as CPUs, memory or computing accelerators such as GPUs. It will then execute the user defined tasks on the selected resources, and generate output files in one of the different storage locations available on the cluster, for the user to review and analyze.</p>","title":"What's a cluster?"},{"location":"docs/glossary/#cluster-components","text":"<p>The terms that are typically used to describe cluster components could be confusing, so in an effort to clarify things, here's a schema of the most important ones, and their definition. </p>","title":"Cluster components"},{"location":"docs/glossary/#cpu","text":"A Central Processing Unit (CPU), or core, or CPU core, is the smallest unit in a microprocessor that can carry out computational tasks, that is, run programs. Modern processors typically have multiple cores.","title":"CPU"},{"location":"docs/glossary/#socket","text":"A socket is the connector that houses the microprocessor. By extension, it represents the physical package of a processor, that typically contains multiple cores.","title":"Socket"},{"location":"docs/glossary/#node","text":"A node is a physical, stand-alone computer, that can handle computing tasks and run jobs. It's connected to other compute nodes via a fast network interconnect, and contains CPUs, memory and devices managed by an operating system.","title":"Node"},{"location":"docs/glossary/#cluster","text":"A cluster is the complete collection of nodes with networking and file storage facilities. It's usually a group of independent computers connected via a fast network interconnect, managed by a resource manager, which acts as a large parallel computer.","title":"Cluster"},{"location":"docs/glossary/#other-commonly-used-terms","text":"<p>To make this documentation more accessible, we try to explain key terms in a non-technical way. When reading these pages, please keep in mind the following definitions, presented in alphabetical order:</p>","title":"Other commonly used terms"},{"location":"docs/glossary/#application","text":"An application is a computer program designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user. In the context of scientific computing, an application typically performs computations related to a scientific goal (molecular dynamics simulations, genome assembly, compuational fluid dynamics simulations, etc).","title":"Application"},{"location":"docs/glossary/#backfill","text":"Backfill scheduling is a method that a scheduler can use in order to maximize utilization. It allows smaller (both in terms of size and time requirements), lower priority jobs to start before larger, higher priority ones, as long as doing so doesn't push back the higher-priority jobs expected start time.","title":"Backfill"},{"location":"docs/glossary/#executable","text":"A binary (or executable) program refers to the machine-code compiled version of an application. This is  which is a binary file that a computer can execute directly. As opposed to the application source code, which is the human-readable version of the application internal instructions, and which needs to be compiled by a compiler to produce the executable binary.","title":"Executable"},{"location":"docs/glossary/#fairshare","text":"A resource scheduler ranks jobs by priority for execution. Each job's priority in queue is determined by multiple factors, among which one being the user's fairshare score.  A user's fairshare score is computed based on a target (the given portion of the resources that this user should be able to use) and the user's effetive usage, ie the amount of resources (s)he effectively used in the past.  As a result, the more resources past jobs have used, the lower the priority of the next jobs will be.  Past usage is computed based on a sliding window and progressively forgotten over time.  This enables all users on a shared resource to get a fair portion of it for their own use, by giving higher priorty to users who have been underserved in the past.","title":"Fairshare"},{"location":"docs/glossary/#flops","text":"Floating-point Operations Per Second (FLOPS) are a measure of computing performance, and represent the number of floating-point operations that a CPU can perform each second. Modern CPUs and GPUs are capable of doing TeraFLOPS (10^12 floating-point operations per second), depending on the precision of those operations (half-precision: 16 bits, single-precision: 32 bits, double-precision: 64 bits).","title":"FLOPS"},{"location":"docs/glossary/#gpu","text":"A Graphical Processing Unit (GPU) is a specialized device initially designed to generate graphical output.  On modern computing architecture, they are used to accelerate certain types of computation, which they are much faster than CPUs at. GPUs have their own memory, and are attached to CPUs, within a node. Each compute node can host one or more GPUs.","title":"GPU"},{"location":"docs/glossary/#hpc","text":"High Performance Computing (HPC) refers to the practice of aggregating computing power to achieve higher performance that would be possible by using a typical computer.","title":"HPC"},{"location":"docs/glossary/#infiniband","text":"Infiniband is a networking standard that features high bandwidth and low latency. The current Infiniband devices are capable of transferring data at up to 200 Gbits/sec with less than a microsecond latency. As of this writing, the popular Infiniband versions are HDR (High Data Rate) with 200 Gbits/sec and EDR (Enhanced Data Rate) with 100 Gbits/sec.","title":"Infiniband"},{"location":"docs/glossary/#iops","text":"Input/output operations per second (IOPS, pronounced eye-ops) is an input/output performance measurement used to characterize computer storage system performance.","title":"IOPS"},{"location":"docs/glossary/#job","text":"A job, or batch job, is the scheduler\u2019s base unit of computing by which resources are allocated to a user for a specified amount of time. Users create job submission scripts to ask the scheduler for resources such as cores, memory, runtime, etc. The scheduler puts the requests in a queue and allocates requested resources based on jobs\u2019 priority.","title":"Job"},{"location":"docs/glossary/#job-step","text":"Job steps are sets of (possibly parallel) tasks within a job","title":"Job step"},{"location":"docs/glossary/#login-nodes","text":"<p>Login nodes are points of access to a compute cluster. Users usually connect to login nodes via SSH to compile and debug their code, review their results, do some simple tests, and submit their batch jobs to the parallel computer.</p>  <p>Login nodes are not for computing</p> <p>Login nodes are usually shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p>","title":"Login nodes"},{"location":"docs/glossary/#modules","text":"Environment modules, or software modules, are a type of software management tool used on in most HPC environments. Using modules enable users to selectively pick the software that they want to use and add them to their environment. This allows to switch between different versions or flavors of the same software, pick compilers, libraries and software components and avoid conflicts between them.","title":"Modules"},{"location":"docs/glossary/#mpi","text":"Message Passing Interface (MPI) is a standardized and portable message-passing system designed to exchange information between processes running on different nodes. There are several implementations of the MPI standard, which is the most common way used to scale parallel applications beyond a single compute node.","title":"MPI"},{"location":"docs/glossary/#openmp","text":"Open Multi Processing (OpenMP) is a parallel programming model designed for shared memory architecture. It's based on pragmas that can be added in applications to let the compiler generate a code that can run on multiple cores, within the same node.","title":"OpenMP"},{"location":"docs/glossary/#partition","text":"<p>A partition is a set of compute nodes within a cluster with a common feature. For example, compute nodes with GPU, or compute nodes belonging to same owner, could form a partition.</p> <p>On Sherlock, you can see detailed partition information with the <code>sh_part</code> or <code>sinfo</code> commands.</p>","title":"Partition"},{"location":"docs/glossary/#qos","text":"A Quality Of Service (QOS) is the set of rules and limitations that apply to a categories of job. The combination of a partition (set of machines where a job can run) and QOS (set of rules that applies to that job) makes what is often referred to as a scheduler queue.","title":"QOS"},{"location":"docs/glossary/#run-time","text":"The run time, or walltime, of a job is the time required to finish its execution.","title":"Run time"},{"location":"docs/glossary/#scheduler","text":"The goal of a job scheduler is to find the appropriate resources to run a set of computational tasks in the most efficient manner. Based on resource requirements and job descriptions, it will prioritize those jobs, allocate resources (nodes, CPUs, memory) and schedule their execution.","title":"Scheduler"},{"location":"docs/glossary/#slurm","text":"Simple Linux Utility for Resource Management (SLURM) is a software that manages computing resources and schedule tasks on them. Slurm coordinates running of many programs on a shared facility and makes sure that resources are used in an optimal manner.","title":"Slurm"},{"location":"docs/glossary/#ssh","text":"Secure Shell (SSH) is a protocol to securely access remote computers. Based on the client-server model, multiple users with an SSH client can access a remote computer. Some operating systems such as Linux and Mac OS have a built-in SSH client and others can use one of many publicly available clients.","title":"SSH"},{"location":"docs/glossary/#thread","text":"A process, in the simplest terms, is an executing program. One or more threads run in the context of the process. A thread is the basic unit to which the operating system allocates processor time. A thread can execute any part of the process code, including parts currently being executed by another thread. Threads are co-located on the same node.","title":"Thread"},{"location":"docs/glossary/#task","text":"In the Slurm context, a task is to be understood as a process. A multi-process program is made of several tasks. A task is typically used to schedule a MPI process, that in turn can use several CPUs.  By contrast, a multi-threaded program is composed of only one task, which uses several CPUs.","title":"Task"},{"location":"docs/orders/","text":"<p>For research groups needing access to additional, dedicated computing resources on Sherlock, we offer the possibility for PIs to purchase their own compute nodes to add to the cluster.</p> <p>Operating costs for managing and housing PI-purchased compute nodes are waived in exchange for letting other users make use of any idle compute cycles on the PI-owned nodes. Owners have priority access to the computing resources they purchase, but can access more nodes for their research if they need to. This provides the PI with much greater flexibility than owning a standalone cluster.</p>","title":"Ordering nodes on Sherlock"},{"location":"docs/orders/#conditions","text":"","title":"Conditions"},{"location":"docs/orders/#service-term","text":"<p>Compute nodes are purchased for a duration of 4 years</p> <p>Compute nodes are purchased and maintained based on a 4-year lifecycle, which is the duration of the equipment warranty and vendor support.</p>  <p>Owners will be notified during the 4th year that their nodes' lifetime is about to reach its term, at which point they'll be welcome to either:</p> <ul> <li>renew their investment by purchasing new nodes,</li> <li>continue to use the public portion of Sherlock's resources.</li> </ul> <p>At the end of their service term, compute nodes are physically retired from the cluster, to make room for new equipment. Compute nodes may be kept running for an additional year at most after the end of their service term, while PIs plan for equipment refresh. Nodes failing during this period may not be repaired, and failed hardware will be disabled or removed from the system.</p> <p>Please note that outside of exceptional circumstances, nodes purchased in Sherlock cannot be removed from cluster before the end of their service term.</p>","title":"Service term"},{"location":"docs/orders/#shared-ownership","text":"<p>Minimum order of one node per PI</p> <p>The number of nodes in a shared order must be greater or equal to the number of purchasing PI groups.</p>  <p>For operational, administrative as well as usability reasons, we do not support shared ownership of equipment. Meaning that multiple PI groups cannot purchase and share a single compute node. Shared orders have a minimum of one node per purchasing PI group.</p>","title":"Shared ownership"},{"location":"docs/orders/#compute-nodes-catalog","text":"<p>SRCC offers a select number of compute node configurations that have been tested and validated on Sherlock and that aim to cover most computing needs.</p>  <p>Sherlock catalog</p> <p>Complete details are available in the Sherlock compute nodes catalog 3</p>","title":"Compute nodes catalog"},{"location":"docs/orders/#configurations","text":"<p>We try to provide hardware configurations that can cover the needs and requirements of a wide range of computing applications, in various scientific fields, and to propose a spectrum of pricing tiers, as shown in the table below:</p>    Type Description Recommended usage Price range     <code>CBASE</code> Base configuration Best per-core performance for serial applications, multi-threaded (OpenMP) and distributed (MPI) applications. Most flexible and cost-effective configuration $   <code>CPERF</code> High-core count configuration Multi-threaded applications requiring higher numbers of CPU cores $$   <code>CBIGMEM</code> Large-memory configuration Serial or multi-threaded applications requiring terabytes of memory (genome assembly, etc...) $$$$   <code>G4FP32</code> Base GPU configuration Single-precision (FP32) GPU-accelerated applications (CryoEM, MD...) with low GPU memory requirements $$   <code>G4FP64</code> HPC GPU configuration AI, ML/DL and GPU-accelerated HPC codes requiring double-precision (FP64) and larger amounts of GPU memory $$$   <code>G4TF64</code><code>G8TF64</code> Best-in-class GPU configuration AI, ML/DL and GPU-accelerated HPC codes requiring double-precision (FP64), large amounts of GPU memory, and heavy multi-GPU scaling $$$$     Choosing the best node configuration for your needs <p>Although some configurations may appear cheaper when looking at the dollar/core ratio, this is not the only point to consider when determining the best configuration for your workload.</p>  Performance per core  <p>There are other factors to take into account, notably the memory and I/O bandwidth per core, which could be lower on higher core-count configurations like <code>CPERF</code>. With multiple times more cores than <code>CBASE</code>, they still provide the same total amount of bandwidth to remote and local storage, as well as, to a lesser extend, to memory.  Higher core-count CPUs also often offer lower core frequencies, which combined with less bandwidth per core, may result in lower performance for serial jobs.</p>   <p><code>CPERF</code> nodes are an excellent fit for multi-threaded applications that don't span multiple nodes. But for more diverse workloads, they don't offer the same level of flexibility than the <code>CBASE</code> nodes, which can run a mix of serial, multi-threaded and MPI applications equally well.</p>  Resources availability  <p>Another important factor to take into account is that less nodes for a given number of cores offers less resilience against potential hardware failures: if a 128-core node becomes unavailable for some reason, that's 128 cores that nobody can use while the node is being repaired. But with 128 cores in 4x 32-core nodes, if a node fails, there are still 96 cores that can be used.</p>   <p>We'll be happy to help you determine the best configuration for your computing needs, feel free to reach out to schedule a consultation.</p>  <p>Configuration details for the different compute node types are listed in the Sherlock compute nodes catalog 3</p>","title":"Configurations"},{"location":"docs/orders/#prices","text":"<p>Prices for the different compute node types are listed in the Sherlock compute nodes catalog 3. They include tax and shipping fees, and are subject to change when quoted: they tend to follow the market-wide variations induced by global political and economical events, which are way outside of our control. Prices are provided there as a guideline for expectations.</p> <p>There are two components in the cost of a compute node purchase:</p> <ol> <li> <p>the cost of the hardware itself (capital purchase),</p> </li> <li> <p>a one-time, per-node infrastructure fee1 that      will be charged to cover the costs of connecting the nodes to the cluster      infrastructure (racks, PDUs, networking switches, cables...)</p> </li> </ol>  <p>No recurring fees</p> <p>There is currently no recurring fee associated with purchasing compute nodes on Sherlock. In particular, there is no CPU.hour charge, purchased nodes are available to their owners 100% of the time, at no additional cost.</p> <p>Currently, there are no user, administrative or management fees associated with ongoing system administration of the Sherlock environment. However, PIs should anticipate the eventuality of modest system administration and support fees being  levied within the 4 year lifetime of their compute nodes.</p>","title":"Prices"},{"location":"docs/orders/#purchasing-process","text":"<p>Purchasing nodes on Sherlock is usually a 5-step process:</p> <ol> <li>the PI use the order form to submit an order,</li> <li>SRCC requests a formal vendor quote to finalize pricing and communicate it      back to the PI for approval,</li> <li>SRCC submits a Stanford PO to the vendor,</li> <li>SRCC takes delivery of the hardware and proceeds to its installation,</li> <li>SRCC notifies the PI that their nodes are ready to be used.</li> </ol> <p>The typical delay between a PO submission to the vendor and the availability of the compute nodes to the PIs is usually between 4 and 8 weeks (global pandemic-related supply-chain disruptions notwithstanding).</p>  <p>Minimum purchase</p> <p>Please note that the minimum purchase is one physical server per PI group. We cannot accommodate multiple PIs pooling funds for a single node.</p>   <p>Single-node orders may incur additional delays</p> <p>Some node configurations need to be ordered from the vendor by sets of 4 nodes (see the Sherlock catalog for details). So orders for quantities non-multiples of 4 need will to be grouped with other PI's orders, which may incur additional delays.</p>","title":"Purchasing process"},{"location":"docs/orders/#required-information","text":"<p>To place an order, we'll need the following information:</p> <ul> <li>The SUNet ID of the PI making the purchase request</li> <li>A PTA2 number to charge the hardware (capital) portion of the purchase</li> <li>A PTA2 number to charge the per-node infrastructure fees (non-capital)     It could be the same PTA used for the capital portion of the     purchase, or a different one</li> </ul> <p>Hardware costs could be spread over multiple PTAs (with a maximum of 2 PTAs per order). But please note that the infrastructure fees have to be charged to a single PTA.</p>","title":"Required information"},{"location":"docs/orders/#placing-an-order","text":"<p>To start ordering compute nodes for Sherlock:</p>   <p> check the Sherlock catalog 3 to review prices and select your configurations</p>  <p>Choose </p>   <p> fill in the order form 3 to submit your request and provide the required information</p> <p>Order </p>    <p>And we'll be in touch shortly!</p>   <ol> <li> <p>infrastructure fees are considered non-capital for cost   accounting purposes and may incur indirect cost burdens on cost-reimbursable   contracts and grants.\u00a0\u21a9</p> </li> <li> <p>PTA is an acronym used for a Project-Task-Award combination   representing an account in the Stanford Financial system.\u00a0\u21a9\u21a9</p> </li> <li> <p>SUNet ID required, document restricted to <code>@stanford.edu</code> accounts.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>","title":"Placing an order"},{"location":"docs/tags/","text":"<p>Here is a list of documentation tags:</p>","title":"Tags"},{"location":"docs/tags/#connection","text":"<ul> <li>Connection options</li> <li>Connecting</li> <li>Data transfer</li> </ul>","title":"connection"},{"location":"docs/tags/#slurm","text":"<ul> <li>Job management</li> <li>Submitting jobs</li> <li>Running jobs</li> </ul>","title":"slurm"},{"location":"docs/tags/#tech","text":"<ul> <li>Technical specifications</li> <li>Facts</li> </ul>","title":"tech"},{"location":"docs/advanced-topics/connection/","tags":["connection"],"text":"","title":"Advanced connection options"},{"location":"docs/advanced-topics/connection/#login-nodes","tags":["connection"],"text":"<p>Sherlock login nodes are regrouped behind a single DNS alias: <code>login.sherlock.stanford.edu</code>.</p> <p>This alias provides a load-balanced login environment, and the assurance that you will be connected to the least loaded login node when you connect to Sherlock.</p> <p>If for any reason, you want to directly connect to a specific login node and bypass the automatic load-balanced dispatching of new connections (which we don't recommend), you can use that login node's hostname explicitly. For instance:</p> <pre><code>$ ssh &lt;sunetid&gt;@ln21.sherlock.stanford.edu\n</code></pre> <p>This can be useful if you run long-standing processes on the login nodes, such as screen or tmux sessions. To find them back when you reconnect to Sherlock, you will indeed need to login to the same login node you started them on.</p> <p>The drawback is that by connecting to a specific login node, you will forfeit the load-balancing benefits, which could result in a crowded environment, or even in login errors in case that specific login node is unavailable.</p>","title":"Login nodes"},{"location":"docs/advanced-topics/connection/#authentication-methods","tags":["connection"],"text":"<p>Public-key authentication</p> <p>SSH public-key authentication is not supported on Sherlock.</p>","title":"Authentication methods"},{"location":"docs/advanced-topics/connection/#password-recommended","tags":["connection"],"text":"<p>The recommended way to authenticate to Sherlock is to simply use your SUNet ID and password, as described in the Connecting page.</p> <p>Passwords are not stored on Sherlock. Sherlock login nodes will delegate password authentication to the University central Kerberos service.</p>","title":"Password (recommended)"},{"location":"docs/advanced-topics/connection/#gssapi","tags":["connection"],"text":"<p>For compatibility with previous generations of Sherlock, GSSAPI1 authentication is still allowed, and could be considered a more convenient option, as this mechanism doesn't require entering your password for each connection.</p> <p>GSSAPI authentication relies on a token system, where users obtain Kerberos ticket-granting tickets, transmit them via SSH to the server they want to connect to, which will, in turn, verify their validity. That way, passwords are never stored locally, and never transit over the network. That's why Kerberos is usually considered the most secure method to authenticate.</p> <p>To connect using GSSAPI on Sherlock, you'll need to go through a few steps2:</p> <ol> <li> <p>make sure the Kerberos user tools are installed on your local machine.    You'll need the <code>kinit</code> (and optionally <code>klist</code> and <code>kdestroy</code>) utilities.    Please refer to your OS documentation to install them if required.</p> </li> <li> <p>download and install the Stanford <code>krb5.conf</code> file, which contains    information about the Stanford Kerberos environment:</p> <pre><code>$ sudo curl -o /etc/krb5.conf https://web.stanford.edu/dept/its/support/kerberos/dist/krb5.conf\n</code></pre> </li> <li> <p>configure your SSH client, by modifying (or creating if it doesn't    exist already) the <code>.ssh/config</code> file in your home directory on your local    machine. Using a text editor, you can add the following lines to your    <code>~/.ssh/config</code> file (indentation is important):</p> <pre><code>Host login.sherlock.stanford.edu\n    GSSAPIDelegateCredentials yes\n    GSSAPIAuthentication yes\n</code></pre> </li> </ol> <p>Once everything is in place (you only need to do this once), you'll be able to test that your Kerberos installation works by running <code>kinit &lt;sunetid&gt;@stanford.edu</code>. You should get a password prompt, and upon success, you'll be able to list your Kerberos credentials with the <code>klist</code> command:</p> <pre><code>$ kinit kilian@stanford.edu\nPassword for kilian@stanford.edu:\n$ klist\nTicket cache: FILE:/tmp/krb5cc_215845_n4S4I6KgyM\nDefault principal: kilian@stanford.edu\n\nValid starting     Expires            Service principal\n07/28/17 17:33:54  07/29/17 18:33:32  krbtgt/stanford.edu@stanford.edu\n        renew until 08/04/17 17:33:32\n</code></pre>  <p>Kerberos ticket expiration</p> <p>Kerberos tickets have a 25-hour lifetime. So you'll need to run the <code>kinit</code> command pretty much once a day to continue being able to authenticate to Sherlock.</p>  <p>Please note that when your Kerberos ticket expire, existing Sherlock connections will not be interrupted. So you'll be able to keep connections open to Sherlock for several days without any issue.</p> <p>You're now ready to connect to Sherlock using GSSAPI. Simply SSH as usual:</p> <pre><code>$ ssh &lt;sunetid&gt;@login.sherlock.stanford.edu\n</code></pre> <p>and if everything goes well, you should directly see the two-factor (Duo) prompt, without having to enter your password.</p> <p>If you want to destroy your Kerberos ticket before its expiration, you can use the <code>kdestroy</code> command.</p>","title":"GSSAPI"},{"location":"docs/advanced-topics/connection/#ssh-options","tags":["connection"],"text":"<p>OpenSSH offers a variety of configuration options that you can use in <code>~/.ssh/config</code> on your local computer. The following section describe some of the options you can use with Sherlock that may make connecting and transferring files more convenient.</p>","title":"SSH options"},{"location":"docs/advanced-topics/connection/#avoiding-multiple-duo-prompts","tags":["connection"],"text":"<p>In order to avoid getting a second-factor (Duo) prompt every time you want to open a new connection to Sherlock, you can take advantage of the multiplexing features provided by OpenSSH.</p> <p>Simply add the following lines to your <code>~/.ssh/config</code> file on your local machine to activate the <code>ControlMaster</code> option. If you already have a <code>Host login.sherlock.stanford.edu</code> block in your configuration file, simply add the <code>Control*</code> option lines in the same block.</p> <pre><code>Host login.sherlock.stanford.edu\n    ControlMaster auto\n    ControlPath ~/.ssh/%l%r@%h:%p\n</code></pre> <p>It will allow SSH to re-use an existing connection to Sherlock each time you open a new session (create a new SSH connection), thus avoiding subsequent 2FA prompts once the initial connection is established.</p> <p>The slight disadvantage of this approach is that once you have a connection open to one of Sherlock's login nodes, all your subsequent connections will be using the same login node. This will somewhat defeat the purpose of the load-balancing mechanism used by the login nodes.</p>  <p>Connection failure with <code>unix_listener</code> error</p> <p>If your connection fails with the following error message: <pre><code>unix_listener: \"...\" too long for Unix domain socket\n</code></pre> you're being hit by a macOS limitation, and you should replace the <code>ControlPath</code> line above by: <pre><code>ControlPath ~/.ssh/%C\n</code></pre></p>","title":"Avoiding multiple Duo prompts"},{"location":"docs/advanced-topics/connection/#connecting-from-abroad","tags":["connection"],"text":"<p>VPN</p> <p>As a good security practice, we always recommend to use the Stanford VPN when connecting from untrusted networks.</p>  <p>Access to Sherlock is not restricted to campus, meaning that you can connect to Sherlock from pretty much anywhere, including when traveling abroad.  We don't restrict inbound SSH connections to any specific IP address range or geographical location, so you shouldn't have any issue to reach the login nodes from anywhere.</p> <p>Regarding two-step authentication, University IT provides alternate authentication options when phone service or Duo Mobile push notifications are not available.</p>   <ol> <li> <p>The Generic Security Service Application Program Interface (GSSAPI,   also GSS-API) is an application programming interface for programs to access   security services. It allows program to interact with security services such   as Kerberos for user authentication.\u00a0\u21a9</p> </li> <li> <p>Those instructions should work on Linux    and MacOs  computers.   For Windows , we recommend using the WSL, as   described in the Prerequisites page.\u00a0\u21a9</p> </li> </ol>","title":"Connecting from abroad"},{"location":"docs/advanced-topics/job-management/","tags":["slurm"],"text":"","title":"Job management"},{"location":"docs/advanced-topics/job-management/#job-submission-limits","tags":["slurm"],"text":"<p>You may have encountered situations where your jobs get rejected at submission with errors like this:</p> <pre><code>sbatch: error: MaxSubmitJobsPerAccount\nsbatch: error: MaxSubmitJobsPerUser\n</code></pre> <p>There are a number of limits on Sherlock, that are put in place to guarantee that all of the users can have a fair access to resources and a smooth experience while using them. One of those limits is about the total number of jobs a single user (and a single group) can have in queue at any given time. This helps ensuring that the scheduler is able to continue operating in an optimal fashion, without being overloaded by a single user or group.</p>","title":"Job submission limits"},{"location":"docs/advanced-topics/job-management/#minimizing-the-number-of-jobs-in-queue","tags":["slurm"],"text":"<p>It's generally a good practice to try reducing the number of jobs submitted to the scheduler, and depending on your workflow, there are various approaches for this. One solution may be to pack more work within a single job, which could help in reducing the overall number of jobs you'll have to submit.</p> <p>Imagine you have a 100-task array job, where you run 1 <code>app</code> task per array item, which looks like this:</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-100\n#SBATCH -n 1\n\n./app ${SLURM_ARRAY_TASK_ID}\n</code></pre> <p>This script would create 100 jobs in queue (even though they would all be regrouped under the same job array), each using 1 CPU to run 1 task.</p> <p>Instead of that 100-task array job, you can try something like this:</p> <pre><code>#!/bin/bash\n#SBATCH --array=0-99:10\n#SBATCH -n 10\n\nfor i in {0..9}; do\n\u00a0 \u00a0 srun -n 1 ./app $((SLURM_ARRAY_TASK_ID+i)) &amp;\ndone\n\nwait # important to make sure the job doesn't exit before the background tasks are done\n</code></pre> <ul> <li><code>--array=1-100:10</code> will use job array indexes 0, 10, 20 ... 90</li> <li><code>-n 10</code> will make sure each job can be subdivided in 10 1-CPU steps</li> <li>the <code>for</code> loop will launch 10 tasks, with indexes from <code>SLURM_ARRAY_TASK_ID</code>   to <code>SLURM_ARRAY_TASK_ID + 9</code>.</li> </ul> <p>This would submit a 10-task array job, each of them running 10 steps simultaneously, on the 10 CPUs that each of the job array item will be allocated.</p> <p>In the end, you'll have run the same number of <code>app</code> instances, but you'll have divided the number of jobs submitted by 10, and allow you to submit the same amount of work to the scheduler, while staying under the submission limits.</p>","title":"Minimizing the number of jobs in queue"},{"location":"docs/getting-started/","text":"","title":"Getting started"},{"location":"docs/getting-started/#prerequisites","text":"<p>To start using Sherlock, you will need:</p> <ul> <li> <p>an active SUNet ID,</p>  <p>What is a SUNet ID?</p> <p>A SUNet ID is a unique 3-8 character account name that identifies you as a member of the Stanford community, with access to the Stanford University Network of computing resources and services. Not to be confused with University ID (a 8-digit number that appears on your Stanford ID Card), your SUNet ID is a permanent and visible part of your Stanford identity and often appears in your Stanford email address (eg. sunetid@stanford.edu).</p> <p>SUNet IDs are not managed by Research Computing. For more information, see https://accounts.stanford.edu/</p>   <p>SUNet ID service levels and external collaborators</p> <p>Base-level service is sufficient for Sherlock accounts. External collaborators, or users without a SUNet ID, can be sponsored by a PI a get a sponsored SUNet ID at no cost. Please see the sponsorship page for more information.</p>  </li> <li> <p>a Sherlock account,</p> </li> <li>a SSH client,</li> <li>good understanding of the concepts and terms   used throughout that documentation,</li> <li>some familiarity with Unix/Linux command-line environments, and   notions of shell scripting.</li> </ul>","title":"Prerequisites"},{"location":"docs/getting-started/#how-to-request-an-account","text":"<p>To request an account, the sponsoring Stanford faculty member should email srcc-support@stanford.edu, specifying the names and SUNet IDs of his/her research team members needing an account.</p> <p>Sherlock is open to the Stanford community as a computing resource to support departmental or sponsored research, thus a faculty member's explicit consent is required for account requests.</p>  <p>Sherlock is a resource for research</p> <p>Sherlock is a resource to help and support research, and is not suitable for course work, class assignments or general-use training sessions.</p>  <p>There is no fee associated with using Sherlock, and no limit in the amount of accounts each faculty member can request. We will periodically ensure that all accounts associated with each PI are still active, and reserve the right to close any Sherlock account whose SUNet ID is expired.</p>","title":"How to request an account"},{"location":"docs/getting-started/#ssh-clients","text":"","title":"SSH clients"},{"location":"docs/getting-started/#linux","text":"<p>Linux distributions usually come with a version of the OpenSSH client already installed. So no additional software installation is required. If not, please refer to your distribution's documentation to install it.</p>","title":"Linux"},{"location":"docs/getting-started/#macos","text":"<p>MacOS systems usually come with a version of the OpenSSH client already installed. So no additional software installation is required</p>","title":"MacOS"},{"location":"docs/getting-started/#windows","text":"<p>Microsoft Windows doesn't provide any SSH client by default. To install one, you have several options, depending on the version of Windows.</p> <ul> <li> <p>WSL recommended</p> <p>Windows 10 provides a feature called the \"Windows Subsystem for Linux\" (WSL). Please refer to the official documentation or this howto for installation instructions. Once installed, you'll be able to use the <code>ssh</code> command from a Windows terminal to connect to Sherlock.</p> </li> <li> <p>Cygwin</p> <p>The Cygwin project predates WSL and provides similar features, which among other things, allow users to install a command-line SSH client on their Windows machines.</p> </li> </ul> <p>The two options above will ensure the best compatibility with the Sherlock environment. If you'd like to explore other avenues, many other SSH client implementations are available, but have not necessarily been tested with Sherlock, so your mileage may vary.</p>","title":"Windows"},{"location":"docs/getting-started/#unixlinux-resources","text":"<p>A full tutorial on using Unix/Linux is beyond the scope of this documentation. However, there are many tutorials for beginning to use Unix/Linux on the web.</p> <p>A few tutorials we recommend are:</p> <ul> <li>Unix Tutorial for Beginners (University of Surrey, UK)</li> <li>Introduction to Unix (Imperial College, London)</li> <li>The Unix Shell (Software Carpentry)</li> </ul> <p>More specifically about HPC and Research Computing:</p> <ul> <li>Intro to HPC (HPC Carpentry)</li> <li>HPC in a day (Software Carpentry}</li> <li>Research Computing Q&amp;A (Ask.Cyberinfrastructure)</li> </ul>","title":"Unix/Linux resources"},{"location":"docs/getting-started/#text-editors","text":"<p>Multiple text editors are available on Sherlock. For beginners, we recommend the use of <code>nano</code>. And for more advanced uses, you'll also find below some resources about using <code>vim</code></p> <ul> <li>Nano guide (Gentoo wiki)</li> <li>Vim guide (Gentoo wiki)</li> </ul> <p>Note: you can also create/edit files with the Sherlock OnDemand File editor</p>","title":"Text editors"},{"location":"docs/getting-started/#shell-scripting","text":"<p>Compute jobs launched on Sherlock are most often initialized by user-written shell scripts. Beyond that, many common operations can be simplified and automated using shell scripts.</p> <p>For an introduction to shell scripting, you can refer to:</p> <ul> <li>Bash Programming - Introduction HOWTO</li> </ul>","title":"Shell scripting"},{"location":"docs/getting-started/connecting/","tags":["connection"],"text":"<p>Sherlock account required</p> <p>To be able to connect to Sherlock, you must first obtain a Sherlock account.</p>","title":"Connecting to Sherlock"},{"location":"docs/getting-started/connecting/#credentials","tags":["connection"],"text":"<p>All users must have a Stanford SUNet ID and a Sherlock account to log in to Sherlock. Your Sherlock account uses the same username/password as your SUnet ID:</p> <pre><code>Username: SUNet ID\nPassword: SUNet ID password\n</code></pre> <p>To request a Sherlock account, please see the Prerequisites page.</p>  <p>Resetting passwords</p> <p>Sherlock does not store your SUNet ID password. As a consequence, we are unable to reset your password. If you require password assistance, please see the SUNet Account page.</p>","title":"Credentials"},{"location":"docs/getting-started/connecting/#connection","tags":["connection"],"text":"<p>Access to Sherlock is provided via Secure Shell (SSH) login. Most Unix-like operating systems provide an SSH client by default that can be accessed by typing the <code>ssh</code> command in a terminal window.</p> <p>To login to Sherlock, open a terminal and type the following command, where <code>&lt;sunetid&gt;</code> should be replaced by your actual SUNet ID:</p> <pre><code>$ ssh &lt;sunetid&gt;@login.sherlock.stanford.edu\n</code></pre> <p>Upon logging in, you will be connected to one of Sherlock's load-balanced login node. You should be automatically directed to the least-loaded login node at the moment of your connection, which should give you the best possible environment to work.</p>","title":"Connection"},{"location":"docs/getting-started/connecting/#host-keys","tags":["connection"],"text":"<p>Upon your very first connection to Sherlock, you will be greeted by a warning such as :</p> <pre><code>The authenticity of host 'login.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>The same warning will be displayed if your try to connect to one of the Data Transfer Node (DTN):</p> <pre><code>The authenticity of host 'dtn.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>This warning is normal: your SSH client warns you that it is the first time it sees that new computer. To make sure you are actually connecting to the right machine, you should compare the ECDSA key fingerprint shown in the message with one of the fingerprints below:</p>    Key type Key Fingerprint     RSA <code>SHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA</code>legacy format: <code>f5:8f:01:46:d1:f9:66:5d:33:58:b4:82:d8:4a:34:41</code>   ECDSA <code>SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg</code>legacy format: <code>70:4c:76:ea:ae:b2:0f:81:4b:9c:c6:5a:52:4c:7f:64</code>    <p>If they match, you can proceed and type \u2018yes\u2019. Your SSH program will then store that key and will verify it for every subsequent SSH connection, to make sure that the server you're connecting to is indeed Sherlock.</p>","title":"Host keys"},{"location":"docs/getting-started/connecting/#host-keys-warning","tags":["connection"],"text":"<p>If you've connected to Sherlock 1.0 before, there's a good chance the Sherlock 1.0 keys were stored by your local SSH client. In that case, when connecting to Sherlock 2.0 using the <code>sherlock.stanford.edu</code> alias, you will be presented with the following message:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: POSSIBLE DNS SPOOFING DETECTED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nThe RSA host key for sherlock.stanford.edu has changed, and the key for\nthe corresponding IP address 171.66.97.101 is unknown. This could\neither mean that DNS SPOOFING is happening or the IP address for the\nhost and its host key have changed at the same time.\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle\nattack)!  It is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA.\nPlease contact your system administrator.\n</code></pre> <p>You can just check that the SHA256 key listed in that warning message correctly matches the one listed in the table above, and if that's the case, you can safely remove the <code>sherlock.stanford.edu</code> entry from your <code>~/.ssh/known_hosts</code> file with the following command on your local machine:</p> <pre><code>$ ssh-keygen -R sherlock.stanford.edu\n</code></pre> <p>and then connect again. You'll see the first-connection prompt mentioned above, and your SSH client will store the new keys for future connections.</p>","title":"Host keys warning"},{"location":"docs/getting-started/connecting/#authentication","tags":["connection"],"text":"","title":"Authentication"},{"location":"docs/getting-started/connecting/#password","tags":["connection"],"text":"<p>To ease access and increase compatibility1 with different platforms, Sherlock allows a simple password-based authentication mechanism for SSH.2.</p> <p>Upon connection, you will be asked for your SUNet ID password with the following prompt:</p> <pre><code>&lt;sunetid&gt;@login.sherlock.stanford.edu's password:\n</code></pre> <p>Enter your password, and if it's correct, you should see the following line:</p> <pre><code>Authenticated with partial success.\n</code></pre>","title":"Password"},{"location":"docs/getting-started/connecting/#second-factor-2fa","tags":["connection"],"text":"<p>Sherlock implements Stanford's Minimum Security Standards policies which mandate two-step authentication to access the cluster.</p> <p>Two-step authentication protects your personal information and credentials by combining something only you know (your password) with something only you have (your phone, tablet or token). This prevents an attacker who would steal your password to actually use it to impersonate you. For more details about two-step authentication at Stanford, please refer to the University IT two-step page.</p> <p>After successfully entering your password, you'll be prompted for your second authentication factor with a message like this:</p> <pre><code>Duo two-factor login for &lt;sunetid&gt;\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-9999\n 2. Phone call to XXX-XXX-9999\n 3. SMS passcodes to XXX-XXX-9999 (next code starts with: 9)\n\nPasscode or option (1-3):\n</code></pre>  <p>Avoiding two-factor prompt on each connection</p> <p>If you routinely open multiple sessions to Sherlock, having to confirm each one of them with a second authentication factor could rapidely become cumbersome. To work around this, the OpenSSH client allows multiplexing channels and re-using existing authenticated for opening new sessions. Please see the Advanced Connection Options page for more details.</p>  <p>If your second factor is accepted, you'll see the following message:</p> <pre><code>Success. Logging you in...\n</code></pre>","title":"Second factor (2FA)"},{"location":"docs/getting-started/connecting/#troubleshooting","tags":["connection"],"text":"","title":"Troubleshooting"},{"location":"docs/getting-started/connecting/#timeouts","tags":["connection"],"text":"<p>If you ever encounter timeout errors when connecting to Sherlock, like these:</p> <pre><code>$ ssh login.sherlock.stanford.edu\nssh: connect to host login.sherlock.stanford.edu port 22: Operation timed out\n</code></pre> <p>you can try to either:</p> <ul> <li>switch to a wired connection if you're connecting over wifi,</li> <li>connect via the Stanford VPN</li> </ul>","title":"Timeouts"},{"location":"docs/getting-started/connecting/#authentication-failures","tags":["connection"],"text":"<p>Excessive authentication failures</p> <p>Entering an invalid password multiple times will result in a (temporary) ban of your IP address.</p>  <p>To prevent brute-force password guessing attacks on Sherlock login nodes, we automatically block IP addresses that generate too many authentication failures in a given time span. This results in a temporary ban of the infringing IP address, and the impossibility for the user to connect to Sherlock from that IP address.</p> <p>When this happens, your SSH connection attempts will result in the following error:</p> <pre><code>ssh: connect to host login.sherlock.stanford.edu port 22: Connection refused\n</code></pre> <p>IP blocked by this mechanism will automatically be authorized again after a few minutes.</p>  <p>SSHFS on macOS</p> <p>SSHFS on macOS is known to try to automatically reconnect filesystem mounts after resuming from sleep or uspend, even without any valid credentials.  As a result, it will generate a lot of failed connection attempts and likely make your IP address blacklisted on login nodes.</p> <p>Make sure to unmount your SSHFS drives before putting your macOS system to sleep to avoid this situation.</p>   <p>VPN</p> <p>If your IP got blocked and you have an urgent need to connect, before the automatic blacklist expiration, we recommend trying to connect through Stanford's VPN: your computer will then use a different IP address and will not be affected by the ban on your regular IP address.</p>","title":"Authentication failures"},{"location":"docs/getting-started/connecting/#login","tags":["connection"],"text":"<p>Congratulations! You've successfully connected to Sherlock. You'll be greeted by the following message of the day:</p> <pre><code>             --*-*- Stanford Research Computing Center -*-*--\n                  ____  _               _            _\n                 / ___|| |__   ___ _ __| | ___   ___| | __\n                 \\___ \\| '_ \\ / _ \\ '__| |/ _ \\ / __| |/ /\n                  ___) | | | |  __/ |  | | (_) | (__|   &lt;\n                 |____/|_| |_|\\___|_|  |_|\\___/ \\___|_|\\_\\\n\n-----------------------------------------------------------------------------\n  This system is for authorized users only and users must comply with all\n  Stanford computing, network and research policies. All activity may be\n  recorded for security and monitoring purposes. For more information, see\n  https://doresearch.stanford.edu/policies/research-policy-handbook and\n  https://adminguide.stanford.edu/chapter-6/subchapter-2/policy-6-2-1\n-----------------------------------------------------------------------------\n  Sherlock is *NOT* approved for storing or processing HIPAA, PHI, PII nor\n  any kind of High Risk data. Users are responsible for the compliance of\n  their data.\n  See https://uit.stanford.edu/guide/riskclassifications for details.\n-----------------------------------------------------------------------------\n\n        Docs         https://www.sherlock.stanford.edu/docs\n        Support      https://www.sherlock.stanford.edu/docs/#support\n\n        Web          https://www.sherlock.stanford.edu\n        News         https://news.sherlock.stanford.edu\n        Status       https://status.sherlock.stanford.edu\n\n-----------------------------------------------------------------------------\n</code></pre> <p>Once authenticated to Sherlock, you'll see the following prompt:</p>   <p><code> [&lt;sunetid&gt;@sh03-ln01 login! ~]$ </code></p>  <p>It indicates the name of the login node you've been connected to, and a reminder that you're actually connected to a login node, not a compute node.</p>  <p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p>  <p>By contrast, the shell prompt on a compute node looks like this:</p>  <p><code> [&lt;sunetid&gt;@sh03-01n01 ~]$ </code></p>","title":"Login"},{"location":"docs/getting-started/connecting/#start-computing","tags":["connection"],"text":"<p>To start computing, there's still a extra step required, which is requesting resources to run your application. It's all described in the next section.</p>   <ol> <li> <p>On Sherlock 1.0, GSSAPI tokens (based on Kerberos tickets) were the only allowed authentication method, which could cause some interoperability with third-party SSH clients.\u00a0\u21a9</p> </li> <li> <p>For other methods of authentication, see the Advanced  Connection Options page.\u00a0\u21a9</p> </li> </ol>","title":"Start computing"},{"location":"docs/getting-started/submitting/","tags":["slurm"],"text":"","title":"Submitting jobs"},{"location":"docs/getting-started/submitting/#principle","tags":["slurm"],"text":"<p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p>","title":"Principle"},{"location":"docs/getting-started/submitting/#requesting-resources","tags":["slurm"],"text":"<p>A mandatory prerequisite for running computational tasks on Sherlock is to request computing resources. This is done via a resource scheduler, whose very purpose is to match compute resources in the cluster (CPUs, GPUs, memory, ...) with user resource requests.</p> <p>The scheduler provides three key functions:</p> <ol> <li>it allocates access to resources (compute nodes) to users for some duration    of time so they can perform work.</li> <li>it provides a framework for starting, executing, and monitoring work    (typically a parallel job such as MPI) on a set of allocated nodes.</li> <li>it arbitrates contention for resources by managing a queue of pending jobs</li> </ol>","title":"Requesting resources"},{"location":"docs/getting-started/submitting/#slurm","tags":["slurm"],"text":"<p></p> <p>Sherlock uses Slurm, an open-source resource manager and job scheduler, used by many of the world's supercomputers and computer clusters.</p> <p>Slurm supports a variety of job submission techniques. By accurately requesting the resources you need, you\u2019ll be able to get your work done.</p>  <p>Wait times in queue</p> <p>As a quick rule of thumb, it's important to keep in mind that the more resources your job requests (CPUs, GPUs, memory, nodes, and time), the longer it may have to wait in queue before it could start.</p> <p>In other words: accurately requesting resources to match your job's needs will minimize your wait times.</p>","title":"Slurm"},{"location":"docs/getting-started/submitting/#how-to-submit-a-job","tags":["slurm"],"text":"A job consists in two parts: resource requests and job steps.  <p>Resource requests describe the amount of computing resource (CPUs, GPUs, memory, expected run time, etc.) that the job will need to successfully run.</p>   <p>Job steps describe tasks that must be executed.</p>","title":"How to submit a job"},{"location":"docs/getting-started/submitting/#batch-scripts","tags":["slurm"],"text":"<p>The typical way of creating a job is to write a job submission script. A submission script is a shell script (e.g. a Bash script) whose first comments, if they are prefixed with <code>#SBATCH</code>, are interpreted by Slurm as parameters describing resource requests and submissions options1.</p> <p>The submission script itself is a job step. Other job steps are created with the <code>srun</code> command.</p> <p>For instance, the following script would request one task with one CPU for 10 minutes, along with 2 GB of memory, in the default partition:</p> submit.sh<pre><code>#!/bin/bash\n#\n#SBATCH --job-name=test\n#\n#SBATCH --time=10:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=2G\n\nsrun hostname\nsrun sleep 60\n</code></pre> <p>When started, the job would run a first job step <code>srun hostname</code>, which will launch the command <code>hostname</code> on the node on which the requested CPU was allocated. Then, a second job step will start the <code>sleep</code> command.</p> <p>You can create this job submission script on Sherlock using a text editor such as <code>nano</code> or <code>vim</code>, and save it as <code>submit.sh</code>.</p>  <p><code>#SBATCH</code> directives syntax</p>  <code>#SBATCH</code> directives must be at the top of the script  <p>Slurm will ignore all <code>#SBATCH</code> directives after the first non-comment line (that is, the first line in the script that doesn't start with a <code>#</code> character). Always put your <code>#SBATCH</code> parameters at the top of your batch script.</p>  Spaces in parameters will cause <code>#SBATCH</code> directives to be ignored  <p>Slurm will ignore all <code>#SBATCH</code> directives after the first white space. For instance directives like those: <pre><code>#SBATCH --job-name=big job\n</code></pre> <pre><code>#SBATCH --mem=16 G\n</code></pre> <pre><code>#SBATCH --partition=normal, owners\n</code></pre> will cause all following <code>#SBATCH</code> directives to be ignored and the job to be submitted with the default parameters.</p>","title":"Batch scripts"},{"location":"docs/getting-started/submitting/#job-submission","tags":["slurm"],"text":"<p>Once the submission script is written properly, you can submit it to the scheduler with the <code>sbatch</code> command. Upon success, <code>sbatch</code> will return the ID it has assigned to the job (the jobid).</p> <pre><code>$ sbatch submit.sh\nSubmitted batch job 1377\n</code></pre>","title":"Job submission"},{"location":"docs/getting-started/submitting/#check-the-job","tags":["slurm"],"text":"<p>Once submitted, the job enters the queue in the <code>PENDING</code> state. When resources become available and the job has sufficient priority, an allocation is created for it and it moves to the <code>RUNNING</code> state. If the job completes correctly, it goes to the <code>COMPLETED</code> state, otherwise, its state is set to <code>FAILED</code>.</p> <p>You'll be able to check the status of your job and follow its evolution with the <code>squeue -u $USER</code> command:</p> <pre><code>$ squeue -u $USER\n     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n      1377    normal     test   kilian  R       0:12      1 sh02-01n01\n</code></pre> <p>The scheduler will automatically create an output file that will contain the result of the commands run in the script file. That output file is names <code>slurm-&lt;jobid&gt;.out</code> by default, but can be customized via submission options. In the above example, you can list the contents of that output file with the following commands:</p> <pre><code>$ cat slurm-1377.out\nsh02-01n01\n</code></pre> <p>Congratulations, you've submitted your first batch job on Sherlock!</p>","title":"Check the job"},{"location":"docs/getting-started/submitting/#whats-next","tags":["slurm"],"text":"<p>Actually, quite a lot. Although you now know how to submit a simple batch job, there are many other options and areas to explore in the next sections:</p> <ul> <li>Data transfer</li> <li>Storage</li> <li>Running jobs</li> </ul>   <ol> <li> <p>You can get the complete list of parameters by referring to the   <code>sbatch</code> manual page (<code>man sbatch</code>).\u00a0\u21a9</p> </li> </ol>","title":"What's next?"},{"location":"docs/software/","text":"","title":"Software on Sherlock"},{"location":"docs/software/#available-software","text":"<p>A set of supported software installations is provided for use on Sherlock. This software is made available through a Software Modules system. For the complete list of available software, please refer to the Software List page.</p> <p>Licensed software can be used on Sherlock, under certain conditions. Feel free to contact us for more details or if you have questions. For more information about purchasing software licenses, you can contact the Stanford Software Licensing office.</p>","title":"Available software"},{"location":"docs/software/#installation-requests","text":"<p>Installation requests</p> <p>The SRCC team installs, for general use, a set of libraries, tools and software applications that are commonly used across many research groups. However, our staff resources are quite limited and don't allow us to build nor maintain custom software applications that may be requested by or be of use to a small number of users.</p>  <p>We strongly encourage users to build custom and field- or domain-specific software themselves, and install it in their own personal or group shared directories. That way, they can share the software installations with the rest of the users in their group, if necessary.</p> <p>Users may even maintain and publish their own local module files to dynamically configure a running environment to use the software. They could share those modules with other users to simplify the use of their own custom software installations.</p>  <p>Installing your own software</p> <p>For more information about building your own software on Sherlock, please see the Software Installation page.</p>  <p>If the software you need is not in the list of available software, and you have trouble installing it on your own, please contact us with as much details about the package as possible, and we will try to help you install it.</p> <p>If it's a widely used software that could benefit multiple users across different scientific communities, we will consider install it globally as resources permit1.</p>","title":"Installation requests"},{"location":"docs/software/#lab-provided-software","text":"<p>PI groups and Labs can share their software installations and modules with the whole Sherlock community of users, and let everyone benefit from their tuning efforts and software developments.</p> <p>Lab-provided software is supported and maintained by each lab, and contact information is usually provided in the lab category module. See the Modules page for more information about using software modules on Sherlock.</p> <p>If you're interested in sharing your software installations beyond your own group on Sherlock, please let us know, and we'll get in touch.</p>   <ol> <li> <p>Software requests, including version upgrades, are fulfilled in   the order they are received, and as time permits. We don't have any dedicated   team for software installations, and requests are handled along with other   duties, typically within two to three weeks of being received.\u00a0\u21a9</p> </li> </ol>","title":"Lab-provided software"},{"location":"docs/software/install/","text":"<p>Software installation requests</p> <p>For more information about software installation requests, please see the Software Overview page</p>  <p>If the software package or version you need is not available in the list of provided software, you may compile and install it yourself. The recommended location for user-installed software is the <code>$GROUP_HOME</code> group shared directory, which is snapshotted and replicated off-site, and can easily be shared with members of a research group.</p>  <p> Work in progress </p> <p>This page is a work in progress and is not complete yet. We are actively working on adding more content and information.</p>","title":"Installation"},{"location":"docs/software/list/","text":"<p></p>","title":"List"},{"location":"docs/software/list/#software-list","text":"<p>The full list of software centrally installed and managed on Sherlock is in the tables below.</p>  <p>Permanent work in progress</p> <p>Software installations on Sherlock are an ever ongoing process. We're continuously adding new software to the list. If you're looking for something that is not in the list, there may be other options.</p>   <p>Subscribe to updates</p> <p> Never want to miss a software update again? Stay up-to-date with new software updates by following the  Sherlock software update RSS feed.</p>","title":"Software list"},{"location":"docs/software/list/#categories","text":"<p>Software modules on Sherlock are organized in categories, by scientific field or functional class. It means that you will have to first load a category module before getting access to individual modules.  The <code>math</code> and <code>devel</code> categories are loaded by default. See the Modules page for further details and examples.</p> <p>We currently provide 467 software modules, in 7 categories, covering 80 fields of science:</p>  <ul> <li> <p><code>biology</code>      clinical science, computational biology, cryo-em, genomics, molecular biology, neurology, pathology, phylogenetics, population genetics   </p> </li> <li> <p><code>chemistry</code>      cheminformatics, computational chemistry, crystallography, electrostatics, molecular dynamics, quantum chemistry   </p> </li> <li> <p><code>devel</code>      build, compiler, data, data analytics, debug, engine, framework, language, lib, mpi, networking, parser, profiling, runtime   </p> </li> <li> <p><code>math</code>      computational geometry, deep learning, linear algebra, machine learning, numerical analysis, numerical library, optimization, scientific computing, statistics, symbolic   </p> </li> <li> <p><code>physics</code>      astronomy, CFD, climate modeling, geophysics, geoscience, lib, materials science, micromagnetics, particle, photonics, quantum information science, quantum mechanics   </p> </li> <li> <p><code>system</code>      backup, benchmark, checkpointing, cloud interface, compression, containers, database, document management, document processing, file management, file transfer, framework, hardware, job management, language, libs, media, performance, resource monitoring, scm, shell, tools   </p> </li> <li> <p><code>viz</code>      data, gis, graphs, imaging, molecular visualization, plotting, remote display   </p> </li> </ul>   <p>Licensed software</p> <p>Access to software modules marked with  in the tables below is restricted to properly licensed user groups.</p> <p>SRCC is not funded to provide commercial software on Sherlock and researchers are responsible for the costs of purchasing and renewing commercial software licenses. For more information, please feel free to contact us and see the Stanford Software Licensing page for purchasing information.</p>   <p>Additional flags and features</p> <p>Some of the modules listed below have been built to support specific architectures or parallel execution modes:</p> <ul> <li>versions marked with  support GPU acceleration</li> <li>versions marked with  support MPI parallel     execution</li> <li>versions marked with  are the default version for     the module</li> </ul>","title":"Categories"},{"location":"docs/software/list/#biology","text":"Field Module\u00a0name Version(s) URL Description     clinical science <code>simvascular</code> <code>20180704</code> Website Simvascular is a blood flow simulation and analysis toolkit. This module provides the svFSI (Fluid Solid Interaction) solver.   computational biology <code>py-biopython</code> <code>1.70_py27</code><code>1.79_py36</code><code>1.79_py39</code> Website Biopython is a set of freely available tools for biological computation written in Python.   computational biology <code>rosetta</code> <code>3.8</code> Website Rosetta is the premier software suite for modeling macromolecular structures. As a flexible, multi-purpose application, it includes tools for structure prediction, design, and remodeling of proteins and nucleic acids.   cryo-em <code>ctffind</code> <code>4.1.13</code> Website ctffind is a program for finding CTFs of electron micrographs.   cryo-em <code>eman2</code> <code>2.2</code> Website EMAN2 is a broadly based greyscale scientific image processing suite with a primary focus on processing data from transmission electron microscopes.   cryo-em <code>imod</code> <code>4.9.12</code><code>4.11.5</code> Website IMOD is a set of image processing, modeling and display programs used for tomographic reconstruction and for 3D reconstruction of EM serial sections and optical sections.   cryo-em <code>motioncor2</code> <code>1.3.1</code> Website MotionCor2 is a multi-GPU accelerated program which corrects anisotropic image motion at the single pixel level.   cryo-em <code>py-topaz</code> <code>0.2.4_py36</code> Website A pipeline for particle detection in cryo-electron microscopy images using convolutional neural networks trained from positive and unlabeled examples.   cryo-em <code>relion</code> <code>2.0.3</code><code>2.1</code> Website RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class averages in electron cryo-microscopy (cryo-EM).   genomics <code>angsd</code> <code>0.919</code><code>0.931</code> Website ANGSD is a software for analyzing next generation sequencing data.   genomics <code>augustus</code> <code>3.3.2</code> Website AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences.   genomics <code>bamtools</code> <code>2.5.1</code> Website BamTools is a project that provides both a C++ API and a command-line toolkit for reading, writing, and manipulating BAM (genome alignment) files.   genomics <code>bcftools</code> <code>1.6</code><code>1.8</code> Website BCFtools is a program for variant calling and manipulating files in the Variant Call Format (VCF) and its binary counterpart BCF.   genomics <code>bcl2fastq</code> <code>2.20</code> Website The bcl2fastq2 conversion software can be used to convert BCL files from MiniSeq, MiSeq, NextSeq, HiSeq, iSeq and NovaSeq sequening systems.   genomics <code>bedops</code> <code>2.4.40</code> Website BEDOPS is an open-source command-line toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data of arbitrary scale.   genomics <code>bedtools</code> <code>2.27.1</code><code>2.30.0</code> Website The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks.   genomics <code>bgen</code> <code>1.1.4</code> Website bgen is the reference implementation of the BGEN format, a binary file format for imputed genotype and haplotype data.   genomics <code>bowtie</code> <code>1.2.2</code> Website Bowtie is an ultrafast, memory-efficient short read aligner.   genomics <code>bowtie2</code> <code>2.3.4.1</code> Website Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.   genomics <code>bwa</code> <code>0.7.17</code> Website BWA (Burrows-Wheeler Aligner) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome.   genomics <code>canu</code> <code>1.8</code> Website A single molecule sequence assembler for genomes large and small.   genomics <code>cufflinks</code> <code>2.2.1</code> Website Cufflinks assembles transcripts, estimates their abundances, and tests for differential expression and regulation in RNA-Seq samples.   genomics <code>fastqc</code> <code>0.11.8</code> Website FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines.   genomics <code>fastx_toolkit</code> <code>0.0.14</code> Website The FASTX-Toolkit is a collection of command line tools for Short-Reads FASTA/FASTQ files preprocessing.   genomics <code>freebayes</code> <code>1.2.0</code> Website FreeBayes is a Bayesian genetic variant detector designed to find small polymorphisms.   genomics <code>gatk</code> <code>4.1.0.0</code><code>4.1.4.1</code> Website GATK (Genome Analysis Toolkit) offers a wide variety of tools with a primary focus on variant discovery and genotyping.   genomics <code>hic-pro</code> <code>2.10.0</code> Website HiC-Pro: An optimized and flexible pipeline for Hi-C data processing.   genomics <code>hisat2</code> <code>2.1.0</code> Website HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).   genomics <code>htslib</code> <code>1.6</code><code>1.8</code><code>1.10.2</code><code>1.14</code> Website C library for high-throughput sequencing data formats.   genomics <code>jellyfish</code> <code>2.2.10</code> Website A fast multi-threaded k-mer counter.   genomics <code>kallisto</code> <code>0.44.0</code> Website kallisto is a program for quantifying abundances of transcripts from RNA-Seq data using high-throughput sequencing reads.   genomics <code>metal</code> <code>20110325</code> Website The METAL software is designed to facilitate meta-analysis of large datasets (such as several whole genome scans) in a convenient, rapid and memory efficient manner.   genomics <code>mixcr</code> <code>2.1.12</code> Website MiXCR is a universal framework that processes big immunome data from raw sequences to quantitated clonotypes.   genomics <code>ncbi-blast+</code> <code>2.6.0</code><code>2.7.1</code><code>2.11.0</code> Website NCBI BLAST+ is a suite of command-line tools to run BLAST (Basic Local Alignment Search Tool), an algorithm for comparing primary biological sequence information.   genomics <code>plink</code> <code>1.07</code><code>1.90b5.3</code><code>2.0a1</code><code>2.0a2</code> Website PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.   genomics <code>py-busco</code> <code>3.0.2_py27</code> Website Assessing genome assembly and annotation completeness with Benchmarking Universal Single-Copy Orthologs (BUSCO).   genomics <code>py-bx-python</code> <code>0.8.1_py27</code> Website Tools for manipulating biological data, particularly multiple sequence alignments.   genomics <code>py-cutadapt</code> <code>1.18_py27</code><code>1.18_py36</code> Website Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.   genomics <code>py-deeptools</code> <code>3.3.1_py36</code> Website Tools to process and analyze deep sequencing data.   genomics <code>py-fithic</code> <code>1.1.3_py27</code> Website Fit-Hi-C is a tool for assigning statistical confidence estimates to chromosomal contact maps produced by genome architecture assays.   genomics <code>py-macs2</code> <code>2.1.1_py27</code> Website MACS (Model-based Analysis of ChIP-Seq) implements a novel ChIP-Seq analysis method.   genomics <code>py-mageck</code> <code>0.5.9.4_py36</code> Website Model-based Analysis of Genome-wide CRISPR-Cas9 Knockout (MAGeCK) is a computational tool to identify important genes from the recent genome-scale CRISPR-Cas9 knockout screens technology.   genomics <code>py-mapdamage</code> <code>2.2.1_py36</code> Website mapDamage2 is a computational framework which tracks and quantifies DNA damage patterns among ancient DNA sequencing reads generated by Next-Generation Sequencing platforms.   genomics <code>py-multiqc</code> <code>1.6_py27</code><code>1.6_py36</code> Website MultiQC is a reporting tool that parses summary statistics from results and log files generated by other bioinformatics tools.   genomics <code>py-obitools</code> <code>1.2.13_py27</code> Website OBITools is a set of programs designed for analyzing NGS data in a DNA metabarcoding context.   genomics <code>py-pybedtools</code> <code>0.8.0_py27</code><code>0.8.2_py36</code><code>0.9.0_py39</code> Website Pybedtools wraps and extends BEDTools and offers feature-level manipulations from within Python.   genomics <code>py-pysam</code> <code>0.14.1_py27</code><code>0.15.3_py36</code><code>0.18.0_py39</code> Website Pysam is a python module for reading, manipulating and writing genomic data sets.   genomics <code>py-scanpy</code> <code>1.8.2_py39</code> Website Scanpy is a scalable toolkit for analyzing single-cell gene expression data.   genomics <code>py-vispr</code> <code>0.4.17_py36</code> Website A visualization framework for CRISPR/Cas9 knockout screens, analyzed with MAGeCK.   genomics <code>regenie</code> <code>2.2.4</code> Website regenie is a C++ program for whole genome regression modelling of large genome-wide association studies.   genomics <code>rsem</code> <code>1.3.3</code> Website RSEM is a software package for estimating gene and isoform expression levels from RNA-Seq data.   genomics <code>salmon</code> <code>0.12.0</code> Website Highly-accurate &amp; wicked fast transcript-level quantification from RNA-seq reads using lightweight alignments.   genomics <code>samtools</code> <code>1.6</code><code>1.8</code> Website Tools (written in C using htslib) for manipulating next-generation sequencing data.   genomics <code>sentieon</code> <code>201808.01</code> Website Sentieon Genomics software is a set of software tools that perform analysis of genomic data obtained from DNA sequencing.   genomics <code>shapeit</code> <code>4.0.0</code> Website SHAPEIT4 is a fast and accurate method for estimation of haplotypes (aka phasing) for SNP array and high coverage sequencing data.   genomics <code>sra-tools</code> <code>2.11.0</code> Website The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives.   genomics <code>star</code> <code>2.5.4b</code> Website STAR: ultrafast universal RNA-seq aligner.   genomics <code>tophat</code> <code>2.1.1</code> Website TopHat is a fast splice junction mapper for RNA-Seq reads.   genomics <code>trim_galore</code> <code>0.5.0</code> Website Trim Galore! is a wrapper script to automate quality and adapter trimming as well as quality control, with some added functionality to remove biased methylation positions for RRBS sequence files.   genomics <code>trinity</code> <code>2.8.4</code><code>2.13.1</code> Website Trinity RNA-Seq de novo transcriptome assembly.   genomics <code>vcflib</code> <code>1.0.0</code> Website A C++ library for parsing and manipulating VCF files.   genomics <code>vcftools</code> <code>0.1.15</code> Website VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project.   molecular biology <code>coot</code> <code>0.9.6</code> Website unixODBC is an open-source project that implements the ODBC API.   neurology <code>afni</code> <code>17.2.07</code><code>18.2.04</code><code>21.3.00</code> Website AFNI (Analysis of Functional NeuroImages) is a set of C programs for processing, analyzing, and displaying functional MRI (FMRI) data - a technique for mapping human brain activity.   neurology <code>ants</code> <code>2.1.0</code><code>2.3.1</code> Website ANTs computes high-dimensional mappings to capture the statistics of brain structure and function.   neurology <code>bart</code> <code>0.7.00</code> Website BART is a toolbox for Computational Magnetic Resonance Imaging.   neurology <code>dcm2niix</code> <code>1.0.20171215</code><code>1.0.20211006</code> Website dcm2niix is a program esigned to convert neuroimaging data from the DICOM format to the NIfTI format.   neurology <code>freesurfer</code> <code>6.0.0</code><code>7.1.1</code> Website An open source software suite for processing and analyzing (human) brain MRI images.   neurology <code>fsl</code> <code>5.0.10</code> Website FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.   neurology <code>mricron</code> <code>20160502</code> Website MRIcron is a cross-platform NIfTI format image viewer.   neurology <code>mrtrix</code> <code>0.3.16</code><code>3.0.3</code> Website MRtrix3 provides a set of tools to perform various types of diffusion MRI analyses, from various forms of tractography through to next-generation group-level analyses.   neurology <code>py-mdt</code> <code>0.10.9_py36</code> Website The Maastricht Diffusion Toolbox, MDT, is a framework and library for parallelized (GPU and multi-core CPU) diffusion Magnetic Resonance Imaging (MRI) modeling.   neurology <code>py-nipype</code> <code>1.1.3_py27</code><code>1.1.3_py36</code> Website Nipype is a Python project that provides a uniform interface to existing neuroimaging software and facilitates interaction between these packages within a single workflow.   neurology <code>spm</code> <code>12</code> Website The SPM software package has been designed for the analysis of brain imaging data sequences. The sequences can be a series of images from different cohorts, or time-series from the same subject.   neurology <code>workbench</code> <code>1.3.1</code> Website Connectome Workbench is an open source, freely available visualization and discovery tool used to map neuroimaging data, especially data generated by the Human Connectome Project.   pathology <code>openslide</code> <code>3.4.1</code> Website OpenSlide is a C library that provides a simple interface to read whole-slide images (also known as virtual slides).   pathology <code>py-openslide-python</code> <code>1.1.1_py27</code><code>1.1.1_py36</code> Website OpenSlide Python is a Python interface to the OpenSlide library.   phylogenetics <code>py-ete</code> <code>3.0.0_py27</code> Website A Python framework for the analysis and visualization of trees.   population genetics <code>py-admixfrog</code> <code>0.6.1_py36</code> Website Admixfrog is a HMM to infer ancestry frogments (fragments) from low-coverage, contaminated data.","title":"biology"},{"location":"docs/software/list/#chemistry","text":"Field Module\u00a0name Version(s) URL Description     cheminformatics <code>py-rdkit</code> <code>2018.09.1_py27</code><code>2018.09.1_py36</code> Website RDKit is a collection of cheminformatics and machine-learning software written in C++ and Python.   computational chemistry <code>gaussian</code> <code>g16.A03</code><code>g16.B01</code> Website Gaussian is a general purpose computational chemistry software package.   computational chemistry <code>libint</code> <code>1.1.4</code><code>2.0.3</code> Website Libint computes molecular integrals.   computational chemistry <code>libxc</code> <code>3.0.0</code> Website Libxc is a library of exchange-correlation functionals for density-functional theory.   computational chemistry <code>nwchem</code> <code>6.8</code> Website NWChem is an ab initio computational chemistry software package which also includes quantum chemical and molecular dynamics functionality.   computational chemistry <code>py-ase</code> <code>3.14.1_py27</code> Website The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations.   computational chemistry <code>schrodinger</code> <code>2017-3</code><code>2018-1</code><code>2018-2</code><code>2019-2</code><code>2020-2</code><code>2021-1</code> Website Schr\u00f6dinger Suites (Small-molecule Drug Discovery Suite, Material Science Suite, Biologics Suite) provide a set of molecular modelling software.   computational chemistry <code>vasp</code> <code>5.4.1</code><code>6.6.1</code> Website The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.   crystallography <code>clipper</code> <code>2.1.20180802</code> Website Crystallographic automation and complex data manipulation libraries.   crystallography <code>mmdb2</code> <code>2.0.20</code> Website A C++ toolkit for working with macromolecular coordinate files.   crystallography <code>ssm</code> <code>1.4</code> Website A macromolecular superposition library.   crystallography <code>vesta</code> <code>3.4.4</code> Website VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies.   electrostatics <code>apbs</code> <code>1.5</code> Website APBS solves the equations of continuum electrostatics for large biomolecular assemblages.   molecular dynamics <code>gromacs</code> <code>2016.3</code><code>2018</code><code>2021.3</code> Website GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.   molecular dynamics <code>lammps</code> <code>20180316</code><code>20200303</code> Website LAMMPS is a classical molecular dynamics code that models an ensemble of particles in a liquid, solid, or gaseous state.   molecular dynamics <code>openmm</code> <code>7.1.1</code> Website A high performance toolkit for molecular simulation.   molecular dynamics <code>plumed</code> <code>2.3.2</code> Website PLUMED is an open source library for free energy calculations in molecular systems.   molecular dynamics <code>py-raspa2</code> <code>2.0.3_py27</code> Website RASPA2 is a general purpose classical simulation package that can be used for the simulation of molecules in gases, fluids, zeolites, aluminosilicates, metal-organic frameworks, carbon nanotubes and external fields.   molecular dynamics <code>qbox</code> <code>1.65.0</code> Website Qbox is a First-Principles Molecular Dynamics code.   molecular dynamics <code>quip</code> <code>20170901</code> Website The QUIP package is a collection of software tools to carry out molecular dynamics simulations.   quantum chemistry <code>cp2k</code> <code>4.1</code> Website CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.   quantum chemistry <code>ocean</code> <code>2.9.7</code> Website OCEAN is a versatile and user-friendly package for calculating core edge spectroscopy including excitonic effects.   quantum chemistry <code>orca</code> <code>4.2.1</code><code>5.0.0</code><code>5.0.3</code> Website ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry.   quantum chemistry <code>quantum-espresso</code> <code>6.2.1</code><code>6.6</code> Website Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.   quantum chemistry <code>quantum-espresso_gpu</code> <code>1.1</code> Website GPU-accelerated Quantum ESPRESSO using CUDA FORTRAN","title":"chemistry"},{"location":"docs/software/list/#devel","text":"Field Module\u00a0name Version(s) URL Description     build <code>bazel</code> <code>0.16.1</code><code>0.26.1</code><code>0.29.1</code> Website Bazel is a fast, scalable, multi-language and extensible build system.   build <code>bazelisk</code> <code>1.3.0</code><code>1.8.0</code> Website Bazelisk is a wrapper for Bazel written in Go.   build <code>cmake</code> <code>3.8.1</code><code>3.11.1</code><code>3.13.1</code><code>3.20.3</code> Website CMake is an extensible, open-source system that manages the build process in an operating system and in a compiler-independent manner.   build <code>kerl</code> <code>1.8.5</code> Website Kerl is a tool to easily build and install Erlang/OTP instances.   build <code>ninja</code> <code>1.9.0</code> Website Ninja is a small build system with a focus on speed.   build <code>py-meson</code> <code>0.51.1_py36</code> Website Meson is an open source build system meant to be both extremely fast, and, even more importantly, as user friendly as possible.   build <code>py-scons</code> <code>3.0.5_py27</code><code>3.0.5_py36</code> Website SCons is an Open Source software construction tool.   compiler <code>aocc</code> <code>2.1.0</code><code>2.2.0</code> Website AMD Optimizing C/C++ Compiler - AOCC is a highly optimized C, C++ and Fortran compiler for x86 targets especially for Zen based AMD processors.   compiler <code>gcc</code> <code>6.3.0</code><code>7.1.0</code><code>7.3.0</code><code>8.1.0</code><code>9.1.0</code><code>10.1.0</code> Website The GNU Compiler Collection includes front ends for C, C++, Fortran, Java, and Go, as well as libraries for these languages (libstdc++, libgcj,...).   compiler <code>icc</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel C++ Compiler, also known as icc or icl, is a group of C and C++ compilers from Intel   compiler <code>ifort</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel Fortran Compiler, also known as ifort, is a group of Fortran compilers from Intel   compiler <code>llvm</code> <code>3.8.1</code><code>4.0.0</code><code>5.0.0</code><code>7.0.0</code> Website The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. Clang is an LLVM native C/C++/Objective-C compiler,   compiler <code>nagfor</code> <code>npl6a70na</code> Website The NAG Fortran Compiler is a full standard implementation of the ISO Fortran 95 language with the addition of all of Fortran 2003, most of Fortran 2008 and OpenMP 3.0 and 3.1.   compiler <code>nvhpc</code> <code>21.5</code><code>21.7</code> Website NVIDIA HPC Software Development Kit (SDK) including C, C++, and Fortran compilers.   compiler <code>pgi</code> <code>19.10</code> Website PGI compilers and tools, including Open MPI (Community Edition).   compiler <code>smlnj</code> <code>110.81</code> Website Standard ML of New Jersey (abbreviated SML/NJ) is a compiler for the Standard ML '97 programming language.   data <code>h5utils</code> <code>1.12.1</code> Website h5utils is a set of utilities for visualization and conversion of scientific data in the free, portable HDF5 format.   data <code>hdf5</code> <code>1.10.6</code><code>1.10.0p1</code><code>1.10.2</code><code>1.12.0</code> Website HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.   data <code>hiredis</code> <code>0.13.3</code> Website Hiredis is a minimalistic C client library for the Redis database.   data <code>ncl</code> <code>6.4.0</code><code>6.6.2</code> Website NCL is a free interpreted language designed specifically for scientific data processing and visualization.   data <code>nco</code> <code>4.8.0</code><code>5.0.6</code> Website The NCO toolkit manipulates and analyzes data stored in netCDF-accessible formats.   data <code>netcdf</code> <code>4.4.1.1</code><code>4.8.1</code> Website NetCDF is a set of software libraries and self-describing, machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.   data <code>pnetcdf</code> <code>1.8.1</code> Website Parallel netCDF (PnetCDF) is a parallel I/O library for accessing NetCDF files in CDF-1, 2, and 5 formats.   data <code>protobuf</code> <code>3.4.0</code> Website Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data.   data <code>py-pandas</code> <code>0.23.0_py27</code><code>0.23.0_py36</code><code>1.0.3_py36</code><code>1.3.1_py39</code> Website pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.   data <code>py-protobuf</code> <code>3.4.0_py27</code><code>3.4.0_py36</code><code>3.6.1_py27</code><code>3.6.1_py36</code><code>3.15.8_py36</code> Website Python bindings for Google's Protocol Buffers data interchange format.   data <code>redis</code> <code>4.0.1</code> Website Redis is an open source, in-memory data structure store, used as a database, cache and message broker.   data analytics <code>hadoop</code> <code>3.1.0</code><code>3.3.1</code> Website The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.   data analytics <code>py-sparkhpc</code> <code>0.3_py27</code> Website Launching and controlling spark on HPC clusters   data analytics <code>spark</code> <code>2.3.0</code><code>3.2.1</code> Website Apache Spark\u2122 is a unified analytics engine for large-scale data processing.   debug <code>gdb</code> <code>8.2.1</code> Website GDB is the GNU Project debugger.   debug <code>valgrind</code> <code>3.14.0</code> Website Valgrind is an instrumentation framework for building dynamic analysis tools.   engine <code>v8</code> <code>8.4.371.22</code> Website V8 is Google\u2019s open source high-performance JavaScript and WebAssembly engine, written in C++.   framework <code>dotnet</code> <code>2.1.500</code> Website .NET is a free, cross-platform, open source developer platform for building many different types of applications.   language <code>cuda</code> <code>9.0.176</code><code>8.0.61</code><code>9.1.85</code><code>9.2.88</code><code>9.2.148</code><code>10.0.130</code><code>10.1.105</code><code>10.1.168</code><code>10.2.89</code><code>11.0.3</code><code>11.1.1</code><code>11.2.0</code><code>11.3.1</code><code>11.4.1</code><code>11.5.0</code> Website CUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers and software engineers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing.   language <code>erlang</code> <code>21.3</code> Website Erlang is a programming language used to build massively scalable soft real-time systems with requirements on high availability.   language <code>go</code> <code>1.9</code><code>1.14</code> Website Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.   language <code>guile</code> <code>2.0.11</code><code>2.2.2</code> Website GNU Guile is the preferred extension system for the GNU Project, which features an implementation of the Scheme programming language.   language <code>haskell</code> <code>8.6.5</code> Website Haskell is a statically typed, purely functional programming language with type inference and lazy evaluation.   language <code>java</code> <code>1.8.0_131</code><code>11.0.11</code> Website Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible.   language <code>julia</code> <code>1.0.0</code><code>1.1.0</code><code>1.2.0</code><code>1.3.1</code><code>1.4.0</code><code>1.5.1</code><code>1.6.2</code><code>1.7.2</code> Website Julia is a high-level, high-performance dynamic programming language for numerical computing.   language <code>lua</code> <code>5.3.4</code> Website Lua is a powerful, efficient, lightweight, embeddable scripting language. It supports procedural programming, object-oriented programming, functional programming, data-driven programming, and data description.   language <code>luarocks</code> <code>2.4.3</code> Website LuaRocks is the package manager for Lua modules.   language <code>manticore</code> <code>20180301</code> Website Manticore is a high-level parallel programming language aimed at general-purpose applications running on multi-core processors.   language <code>nodejs</code> <code>8.9.4</code><code>9.5.0</code><code>16.13.0</code> Website Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. It provides the npm package manager.   language <code>perl</code> <code>5.26.0</code> Website Perl 5 is a highly capable, feature-rich programming language with over 29 years of development.   language <code>php</code> <code>7.3.0</code> Website PHP (recursive acronym for PHP: Hypertext Preprocessor) is an open source general-purpose scripting language that is especially suited for web development.   language <code>py-cython</code> <code>0.27.3_py27</code><code>0.27.3_py36</code><code>0.29.21_py36</code><code>0.29.28_py39</code> Website Cython is an optimising static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex).   language <code>py-ipython</code> <code>5.4.1_py27</code><code>6.1.0_py36</code> Website IPython is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language.   language <code>py-jupyter</code> <code>1.0.0_py27</code><code>1.0.0_py36</code><code>1.0.0_py39</code> Website Jupyter is a browser-based interactive notebook for programming, mathematics, and data science. It supports a number of languages via plugins.   language <code>python</code> <code>2.7.13</code><code>3.6.1</code><code>3.9.0</code> Website Python is an interpreted, interactive, object-oriented programming language.   language <code>ruby</code> <code>2.4.1</code><code>2.7.1</code> Website A dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write.   language <code>rust</code> <code>1.35.0</code><code>1.56.1</code> Website A language empowering everyone to build reliable and efficient software.   language <code>scala</code> <code>2.12.6</code> Website Scala combines object-oriented and functional programming in one concise, high-level language.   lib <code>ant</code> <code>1.10.1</code> Website Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other.   lib <code>boost</code> <code>1.64.0</code><code>1.69.0</code><code>1.75.0</code><code>1.76.0</code> Website Boost is a set of libraries for the C++ programming language that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing.   lib <code>cnmem</code> <code>1.0.0</code> Website CNMeM is a simple library to help the Deep Learning frameworks manage CUDA memory.   lib <code>cub</code> <code>1.7.3</code><code>1.10.0</code> Website CUB is a flexible library of cooperative threadblock primitives and other utilities for CUDA kernel programming.   lib <code>cutlass</code> <code>0.1.0</code> Website CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) at all levels and scales within CUDA.   lib <code>dtcmp</code> <code>1.1.3</code> Website Datatype Compare (DTCMP) Library for sorting and ranking distributed data using MPI.   lib <code>eigen</code> <code>3.3.3</code> Website Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.   lib <code>libcircle</code> <code>0.3.0</code> Website libcircle is an API for distributing embarrassingly parallel workloads using self-stabilization.   lib <code>libctl</code> <code>3.2.2</code><code>4.0.1</code><code>4.5.0</code> Website libctl is a library for supporting flexible control files in scientific simulations.   lib <code>libevent</code> <code>2.1.12</code> Website The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.   lib <code>libgpuarray</code> <code>0.7.5</code> Website Library to manipulate tensors on the GPU.   lib <code>libtree</code> <code>2.0.0</code> Website libtree prints shared object dependencies as a tree.   lib <code>lwgrp</code> <code>1.0.4</code> Website The Light-weight Group Library provides methods for MPI codes to quickly create and destroy process groups.   lib <code>nccl</code> <code>1.3.4</code><code>2.0.4</code><code>2.1.15</code><code>2.2.13</code><code>2.3.7</code><code>2.4.8</code><code>2.5.6</code><code>2.8.4</code><code>2.11.4</code> Website NCCL (pronounced 'Nickel') is a stand-alone library of standard collective communication routines, such as all-gather, reduce, broadcast, etc., that have been optimized to achieve high bandwidth over PCIe.   lib <code>opencv</code> <code>3.3.0</code><code>4.5.2</code> Website OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.   lib <code>petsc</code> <code>3.10.3</code> Website PETSc is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations.   lib <code>py-h5py</code> <code>2.7.1_py27</code><code>2.8.0_py36</code><code>2.10.0_py36</code><code>3.1.0_py36</code> Website The h5py package is a Pythonic interface to the HDF5 binary data format.   lib <code>py-netcdf4</code> <code>1.3.1_py27</code><code>1.3.1_py36</code> Website netcdf4-python is a Python interface to the netCDF C library.   lib <code>py-numba</code> <code>0.35.0_py27</code><code>0.35.0_py36</code><code>0.53.1_py36</code><code>0.54.1_py39</code> Website Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python..   lib <code>py-pycuda</code> <code>2017.1.1_py27</code><code>2021.1_py36</code> Website PyCUDA lets you access Nvidia\u2018s CUDA parallel computation API from Python.   lib <code>py-schwimmbad</code> <code>0.3.1_py36</code> Website schwimmbad provides a uniform interface to parallel processing pools and enables switching easily between local development (e.g., serial processing or with multiprocessing) and deployment on a cluster or supercomputer (via, e.g., MPI or JobLib).   lib <code>py-scikit-image</code> <code>0.13.0_py27</code><code>0.14.0_py27</code><code>0.15.0_py27</code><code>0.15.0_py36</code><code>0.17.2_py36</code> Website scikit-image is a collection of algorithms for image processing.   lib <code>rabbitmq</code> <code>3.7.13</code> Website RabbitMQ is an open-source message broker.   lib <code>swig</code> <code>3.0.12</code> Website SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl.   lib <code>tbb</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel\u00ae Threading Building Blocks (Intel\u00ae TBB) is a widely used C++ library for shared-memory parallel programming and heterogeneous computing (intra-node distributed memory programming).   lib <code>trilinos</code> <code>12.12.1</code> Website Trilinos is a collection of open-source software libraries, called packages, intended to be used as building blocks for the development of scientific applications.   lib <code>zeromq</code> <code>4.2.2</code> Website ZeroMQ (also spelled \u00d8MQ, 0MQ or ZMQ) is a high-performance asynchronous messaging library, aimed at use in distributed or concurrent applications.   mpi <code>hpcx</code> <code>2.6.0</code><code>2.7.0</code><code>2.8.1</code> Website Mellanox HPC-X toolkit is a comprehensive software package that includes MPI and SHMEM/PGAS communications libraries.   mpi <code>impi</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel\u00ae MPI Library is a multi-fabric message passing library that implements the Message Passing Interface, version 3.1 (MPI-3.1) specification.   mpi <code>openmpi</code> <code>2.0.2</code><code>2.1.1</code><code>3.1.2</code><code>4.0.3</code><code>4.0.5</code><code>4.1.0</code> Website The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners.   mpi <code>py-mpi4py</code> <code>3.0.0_py27</code><code>3.0.3_py36</code> Website MPI for Python provides Python bindings for the Message Passing Interface (MPI) standard. It is implemented on top of the MPI-\u00bd/3 specification and exposes an API which grounds on the standard MPI-2 C++ bindings.   networking <code>gasnet</code> <code>1.30.0</code> Website GASNet is a language-independent, low-level networking layer that provides network-independent, high-performance communication primitives tailored for implementing parallel global address space SPMD languages and libraries.   networking <code>libfabric</code> <code>1.6.0</code><code>1.6.2</code><code>1.7.1</code><code>1.9.1</code><code>1.10.1</code><code>1.11.1</code> Website The Open Fabrics Interfaces (OFI) is a framework focused on exporting fabric communication services to applications. Libfabric is the library that defines and exports the user-space API of OFI.   networking <code>ucx</code> <code>1.3.1</code><code>1.8.1</code><code>1.9.0</code><code>1.10.0</code> Website UCX is a communication library implementing high-performance messaging for MPI/PGAS frameworks.   parser <code>antlr</code> <code>2.7.7</code> Website ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files.   parser <code>xerces-c</code> <code>3.2.1</code> Website Xerces-C++ is a validating XML parser written in a portable subset of C++.   profiling <code>amd-uprof</code> <code>3.3.462</code> Website AMD uProf is a performance analysis tool for applications.   runtime <code>starpu</code> <code>1.3.2</code> Website StarPU is a unified runtime system that offers support for heterogeneous multicore architectures","title":"devel"},{"location":"docs/software/list/#math","text":"Field Module\u00a0name Version(s) URL Description     computational geometry <code>cgal</code> <code>4.10</code> Website The Computational Geometry Algorithms Library (CGAL) is a C++ library that aims to provide easy access to efficient and reliable algorithms in computational geometry.   computational geometry <code>qhull</code> <code>2015.2</code> Website Qhull computes the convex hull, Delaunay triangulation, Voronoi diagram, halfspace intersection about a point, furthest-site Delaunay triangulation, and furthest-site Voronoi diagram.   deep learning <code>caffe2</code> <code>0.8.1</code> Website Caffe2 is a deep learning framework that provides an easy and straightforward way to experiment with deep learning and leverage community contributions of new models and algorithms.   deep learning <code>cudnn</code> <code>6.0</code><code>7.0.1</code><code>7.0.4</code><code>7.0.5</code><code>7.1.4</code><code>7.4.1.5</code><code>7.6.4</code><code>7.6.5</code><code>8.1.1.33</code><code>8.3.3.40</code> Website NVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural networks.   deep learning <code>cutensor</code> <code>1.2.0</code><code>1.5.0.3</code> Website GPU-accelerated tensor linear algebra library.   deep learning <code>py-gym</code> <code>0.21.0_py39</code> Website Gym is a toolkit for developing and comparing reinforcement learning algorithms.   deep learning <code>py-horovod</code> <code>0.12.1_py27</code><code>0.12.1_py36</code> Website Horovod is a distributed training framework for TensorFlow. The goal of Horovod is to make distributed Deep Learning fast and easy to use.   deep learning <code>py-keras</code> <code>2.1.5_py27</code><code>2.0.8_py27</code><code>2.1.5_py36</code><code>2.2.4_py27</code><code>2.2.4_py36</code><code>2.3.1_py36</code> Website Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.   deep learning <code>py-onnx</code> <code>1.0.1_py27</code><code>1.8.1_py36</code> Website ONNX is a open format to represent deep learning models.   deep learning <code>py-pytorch</code> <code>0.3.0_py27</code><code>0.2.0_py27</code><code>0.2.0_py36</code><code>0.3.0_py36</code><code>1.0.0_py27</code><code>1.0.0_py36</code><code>1.4.0_py36</code><code>1.6.0_py36</code><code>1.8.1_py39</code> Website PyTorch is a deep learning framework that puts Python first.   deep learning <code>py-tensorboardx</code> <code>1.8_py27</code> Website TensorboardX is TensorBoard\u2122 for PyTorch (and Chainer, MXNet, NumPy...)   deep learning <code>py-tensorflow</code> <code>2.0.0_py36</code><code>1.4.0_py27</code><code>1.5.0_py27</code><code>1.5.0_py36</code><code>1.6.0_py27</code><code>1.6.0_py36</code><code>1.7.0_py27</code><code>1.8.0_py27</code><code>1.9.0_py27</code><code>1.9.0_py36</code><code>1.12.0_py27</code><code>1.12.0_py36</code><code>2.1.0_py36</code><code>2.4.1_py36</code><code>2.6.2_py36</code> Website TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs.   deep learning <code>py-tensorlayer</code> <code>1.6.3_py27</code> Website TensorLayer is a Deep Learning (DL) and Reinforcement Learning (RL) library extended from Google TensorFlow.   deep learning <code>py-theano</code> <code>1.0.1_py27</code> Website Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.   deep learning <code>py-triton</code> <code>1.0.0_py39</code> Website Triton is a language and compiler for writing highly efficient custom Deep-Learning primitives.   deep learning <code>tensorrt</code> <code>3.0.1</code><code>3.0.4</code><code>4.0.1.6</code><code>5.0.2.6</code><code>6.0.1.8</code><code>7.0.0.11</code><code>7.2.3.4</code> Website NVIDIA TensorRT\u2122 is a high-performance deep learning inference optimizer and runtime that delivers low latency, high-throughput inference for deep learning applications.   deep learning <code>torch</code> <code>20180202</code> Website Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first.   linear algebra <code>armadillo</code> <code>8.200.1</code> Website Armadillo is a high quality linear algebra library (matrix maths) for the C++ language, aiming towards a good balance between speed and ease of use.   linear algebra <code>cusparselt</code> <code>0.2.0.1</code> Website NVIDIA cuSPARSELt is a high-performance CUDA library for sparse matrix-matrix multiplication.   machine learning <code>py-scikit-learn</code> <code>0.19.1_py27</code><code>0.19.1_py36</code><code>0.24.2_py36</code><code>1.0.2_py39</code> Website Scikit-learn is a free software machine learning library for the Python programming language.   numerical analysis <code>matlab</code> <code>R2017a</code><code>R2017b</code><code>R2018a</code><code>R2019a</code><code>R2020a</code> Website MATLAB is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks.   numerical analysis <code>octave</code> <code>4.2.1</code> Website GNU Octave is a high-level language primarily intended for numerical computations.   numerical library <code>arpack</code> <code>3.5.0</code><code>3.7.0</code> Website Collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.   numerical library <code>blis</code> <code>2.1</code><code>2.2.4</code> Website BLIS is a portable software framework for instantiating high-performance BLAS-like dense linear algebra libraries.   numerical library <code>fftw</code> <code>2.1.5</code><code>3.3.6</code><code>3.3.8</code> Website The Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs).   numerical library <code>glpk</code> <code>4.63</code> Website The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems.   numerical library <code>gmp</code> <code>6.1.2</code> Website GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating-point numbers.   numerical library <code>gsl</code> <code>1.16</code><code>2.3</code><code>2.7</code> Website The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting.   numerical library <code>harminv</code> <code>1.4.1</code> Website harminv is a program designed to solve the problem of harmonic inversion: given a time series consisting of a sum of sinusoids (modes), extract their frequencies and amplitudes.   numerical library <code>hypre</code> <code>2.20.0</code> Website HYPRE is a library of high performance preconditioners and solvers featuring multigrid methods for the solution of large, sparse linear systems of equations on massively parallel computers.   numerical library <code>imkl</code> <code>2017.u2</code><code>2018.u1</code><code>2018</code><code>2019</code> Website Intel Math Kernel Library (Intel MKL) is a library of optimized math routines for science, engineering, and financial applications. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math.[3] The routines in MKL are hand-optimized specifically for Intel processors   numerical library <code>libflame</code> <code>2.1</code><code>2.2.4</code> Website libflame is a portable library for dense matrix computations, providing much of the functionality present in LAPACK   numerical library <code>libxsmm</code> <code>1.8.1</code> Website LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications as well as for deep learning primitives such as small convolutions   numerical library <code>metis</code> <code>5.1.0</code> Website METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices.   numerical library <code>mpc</code> <code>1.2.1</code> Website GNU MPC is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result.   numerical library <code>mpfr</code> <code>3.1.5</code><code>4.1.0</code> Website The MPFR library is a C library for multiple-precision floating-point computations with correct rounding.   numerical library <code>mumps</code> <code>5.1.2</code> Website A parallel sparse direct solver.   numerical library <code>nagdc</code> <code>dcl6i34ngl</code> Website (Adjoint) Algorithmic Differentiation software tool for computing sensitivities of C++ codes.   numerical library <code>nagfs</code> <code>fsl6i26dcl</code> Website The NAG Library for SMP &amp; Multicore is based on, and includes, the full functionality of the NAG Fortran Library.   numerical library <code>nagmb</code> <code>MBL6I25DNL</code> Website The NAG C Library is the largest and most comprehensive collection of mathematical and statistical algorithms for C and C++.   numerical library <code>nagnl</code> <code>nll6i272nl</code> Website NAG library of numerical algorithms.   numerical library <code>openblas</code> <code>0.3.4</code><code>0.2.19</code><code>0.3.9</code><code>0.3.10</code> Website OpenBLAS is an optimized BLAS library   numerical library <code>parmetis</code> <code>4.0.3</code> Website ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices.   numerical library <code>py-cupy</code> <code>7.8.0_py36</code><code>10.2.0_py39</code> Website CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA.   numerical library <code>py-gmpy2</code> <code>2.0.8_py36</code> Website gmpy2 is a C-coded Python extension module that supports multiple-precision arithmetic.   numerical library <code>py-numpy</code> <code>1.14.3_py27</code><code>1.14.3_py36</code><code>1.17.2_py36</code><code>1.18.1_py36</code><code>1.19.2_py36</code><code>1.20.3_py39</code> Website NumPy is the fundamental package for scientific computing with Python.   numerical library <code>py-psbody-mesh</code> <code>0.4_py39</code> Website The MPI-IS Mesh Processing Library contains core functions for manipulating meshes and visualizing them.   numerical library <code>py-pyublas</code> <code>2017.1_py27</code> Website PyUblas provides a seamless glue layer between Numpy and Boost.Ublas for use with Boost.Python.   numerical library <code>py-scipy</code> <code>1.1.0_py27</code><code>1.1.0_py36</code><code>1.4.1_py36</code><code>1.6.3_py39</code> Website The SciPy library provides many user-friendly and efficient numerical routines such as routines for numerical integration and optimization.   numerical library <code>qrupdate</code> <code>1.1.2</code> Website qrupdate is a Fortran library for fast updates of QR and Cholesky decompositions.   numerical library <code>scalapack</code> <code>2.0.2</code><code>2.1</code> Website ScaLAPACK is a library of high-performance linear algebra routines for parallel distributed memory machines.   numerical library <code>scotch</code> <code>6.0.4</code> Website Software package and libraries for sequential and parallel graph partitioning, static mapping and clustering, sequential mesh and hypergraph partitioning, and sequential and parallel sparse matrix block ordering.   numerical library <code>superlu</code> <code>5.2.1</code> Website SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations.   numerical library <code>tetgen</code> <code>1.6.0</code> Website TetGen provides various features to generate good quality and adaptive tetrahedral meshes suitable for numerical methods, such as finite element or finite volume methods.   numerical library <code>xblas</code> <code>1.0.248</code> Website Extra precise basic linear algebra subroutines.   optimization <code>gurobi</code> <code>7.5.1</code><code>8.0.1_py27</code><code>8.0.1_py36</code><code>9.0.3_py36</code> Website The Gurobi Optimizer is a commercial optimization solver for mathematical programming.   optimization <code>knitro</code> <code>10.3.0</code><code>12.4.0</code> Website Artelys Knitro is an optimization solver for difficult large-scale nonlinear problems.   optimization <code>nlopt</code> <code>2.6.2</code> Website NLopt is a free/open-source library for nonlinear optimization.   optimization <code>octeract</code> <code>3.3.0</code> Website Octeract Engine is a proprietary massively parallel deterministic global optimization solver for general Mixed-Integer Nonlinear Programs (MINLP).   optimization <code>py-optuna</code> <code>2.10.0_py39</code> Website Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.   scientific computing <code>py-scipystack</code> <code>1.0_py27</code><code>1.0_py36</code> Website The SciPy Stack is a collection of open source software for scientific computing in Python. It provides the following packages: numpy, scipy, matplotlib, ipython, jupyter, pandas, sympy and nose.   statistics <code>datamash</code> <code>1.3</code> Website GNU datamash is a command-line program which performs basic numeric, textual and statistical operations on input textual data files.   statistics <code>jags</code> <code>4.3.0</code> Website Just another Gibbs sampler (JAGS) is a program for simulation from Bayesian hierarchical models using Markov chain Monte Carlo (MCMC).   statistics <code>py-rpy2</code> <code>2.8.6_py27</code><code>2.9.2_py36</code> Website rpy2 is an interface to R running embedded in a Python process.   statistics <code>R</code> <code>3.5.1</code><code>3.4.0</code><code>3.6.1</code><code>4.0.2</code><code>4.1.2</code> Website R is a free software environment for statistical computing and graphics.   statistics <code>rstudio</code> <code>1.3.1093</code> Website RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.   statistics <code>sas</code> <code>9.4</code> Website SAS is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.   statistics <code>stata</code> <code>15</code><code>14</code><code>16</code><code>17</code> Website Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics.   symbolic <code>libmatheval</code> <code>1.1.11</code> Website GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text.   symbolic <code>py-sympy</code> <code>1.1.1_py27</code><code>1.1.1_py36</code> Website SymPy is a Python library for symbolic mathematics.","title":"math"},{"location":"docs/software/list/#physics","text":"Field Module\u00a0name Version(s) URL Description     astronomy <code>cfitsio</code> <code>4.0.0</code> Website FITSIO is a library of C and Fortran subroutines for reading and writing data files in FITS (Flexible Image Transport System) data format.   astronomy <code>heasoft</code> <code>6.22.1</code><code>6.26.1</code> Website HEAsoft is a Unified Release of the FTOOLS (General and mission-specific tools to manipulate FITS files) and XANADU (High-level, multi-mission tasks for X-ray astronomical spectral, timing, and imaging data analysis) software packages.   astronomy <code>py-astropy</code> <code>4.0.1_py36</code> Website The Astropy Project is a community effort to develop a common core package for Astronomy in Python and foster an ecosystem of interoperable astronomy packages.   astronomy <code>py-lenstools</code> <code>1.0_py36</code> Website This python package collects together a suite of widely used analysis tools in Weak Gravitational Lensing.   astronomy <code>py-namaster</code> <code>1.2.2_py36</code> Website NaMaster is a C library, Python module and standalone program to compute full-sky angular cross-power spectra of masked fields with arbitrary spin and an arbitrary number of known contaminants using a pseudo-Cl (aka MASTER) approach.   CFD <code>su2</code> <code>7.0.3</code> Website SU2: An Open-Source Suite for Multiphysics Simulation and Design   climate modeling <code>cdo</code> <code>1.9.7.1</code> Website CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data.   geophysics <code>opensees</code> <code>2.5.0</code> Website OpenSees is a software framework for developing applications to simulate the performance of structural and geotechnical systems subjected to earthquakes.   geoscience <code>gdal</code> <code>2.2.1</code><code>3.4.1</code> Website GDAL is a translator library for raster and vector geospatial data formats.   geoscience <code>geos</code> <code>3.6.2</code> Website GEOS (Geometry Engine - Open Source) is a C++ port of Java Topology Suite (JTS).   geoscience <code>proj</code> <code>4.9.3</code><code>8.2.1</code> Website proj.4 is a standard UNIX filter function which converts geographic longitude and latitude coordinates into cartesian coordinates (and vice versa.   geoscience <code>py-opendrift</code> <code>1.0.3_py27</code> Website OpenDrift is a software for modeling the trajectories and fate of objects or substances drifting in the ocean, or even in the atmosphere.   geoscience <code>py-pyproj</code> <code>1.9.5.1_py27</code><code>1.9.5.1_py36</code> Website Python interface to PROJ4 library for cartographic transformations.   geoscience <code>udunits</code> <code>2.2.26</code> Website The UDUNITS package from Unidata is a C-based package for the programatic handling of units of physical quantities.   lib <code>libgdsii</code> <code>0.21</code> Website libGDSII C++ is a library and command-line utility for reading GDSII geometry files.   materials science <code>atat</code> <code>3.36</code> Website Alloy Theoretic Automated Toolkit: a software toolkit for modeling coupled configurational and vibrational disorder in alloy systems.   micromagnetics <code>oommf</code> <code>1.2b4</code> Website OOMMF is a set of portable, extensible public domain micromagnetic program and associated tools.   particle <code>openmc</code> <code>0.10.0</code> Website OpenMC is a Monte Carlo particle transport simulation code focused on neutron criticality calculations.   photonics <code>meep</code> <code>1.3</code><code>1.4.3</code> Website Meep is a free finite-difference time-domain (FDTD) simulation software package to model electromagnetic systems.   photonics <code>mpb</code> <code>1.5</code><code>1.6.2</code> Website MPB is a free software package for computing the band structures, or dispersion relations, and electromagnetic modes of periodic dielectric structures, on both serial and parallel computers.   quantum information science <code>cuquantum</code> <code>22.03.0.40</code> Website NVIDIA cuQuantum is an SDK of optimized libraries and tools for accelerating quantum computing workflows.   quantum information science <code>py-cuquantum-python</code> <code>22.3.0_py39</code> Website NVIDIA cuQuantum Python provides Python bindings and high-level object-oriented models for accessing the full functionalities of NVIDIA cuQuantum SDK from Python.   quantum mechanics <code>py-quspin</code> <code>0.3.5_py36</code> Website QuSpin is an open-source Python package for exact diagonalization and quantum dynamics of arbitrary boson, fermion and spin many-body systems.   quantum mechanics <code>py-qutip</code> <code>4.5.2_py36</code> Website QuTiP is open-source software for simulating the dynamics of closed and open quantum systems.","title":"physics"},{"location":"docs/software/list/#system","text":"Field Module\u00a0name Version(s) URL Description     backup <code>restic</code> <code>0.9.5</code><code>0.12.0</code><code>0.12.1</code> Website Fast, secure, efficient backup program.   benchmark <code>hp2p</code> <code>3.2</code> Website Heavy Peer To Peer: a MPI based benchmark for network diagnostic.   benchmark <code>mpibench</code> <code>20190729</code> Website Times MPI collectives over a series of message sizes.   benchmark <code>mprime</code> <code>29.4</code> Website mprime is used by GIMPS, a distributed computing project dedicated to finding new Mersenne prime numbers, and which is commonly used as a stability testing utility.   benchmark <code>osu-micro-benchmarks</code> <code>5.6.1</code><code>5.6.3</code><code>5.7</code> Website The OSU MicroBenchmarks carry out a variety of message passing performance tests using MPI.   checkpointing <code>dmtcp</code> <code>2.6.0</code> Website DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space -- with no modifications to user code or to the O/S.   cloud interface <code>aws-cli</code> <code>2.0.50</code> Website This package provides a unified command line interface to Amazon Web Services.   cloud interface <code>google-cloud-sdk</code> <code>240.0.0</code><code>338.0.0</code> Website Command-line interface for Google Cloud Platform products and services.   compression <code>libarchive</code> <code>3.3.2</code><code>3.4.2</code><code>3.5.2</code> Website The libarchive project develops a portable, efficient C library that can read and write streaming archives in a variety of formats.   compression <code>libzip</code> <code>1.5.1</code> Website libzip is a C library for reading, creating, and modifying zip archives.   compression <code>lz4</code> <code>1.8.0</code> Website LZ4 is lossless compression algorithm.   compression <code>lzo</code> <code>2.10</code> Website LZO is a portable lossless data compression library written in ANSI C.   compression <code>mpibzip2</code> <code>0.6</code> Website MPIBZIP2 is a parallel implementation of the bzip2 block-sorting file compressor that uses MPI and achieves significant speedup on cluster machines.   compression <code>p7zip</code> <code>16.02</code> Website p7zip is a Linux port of 7zip, a file archiver with high compression ratio.   compression <code>pbzip2</code> <code>1.1.12</code> Website PBZIP2 is a parallel implementation of the bzip2 block-sorting file compressor that uses pthreads and achieves near-linear speedup on SMP machines.   compression <code>pigz</code> <code>2.4</code> Website A parallel implementation of gzip for modern multi-processor, multi-core machines.   compression <code>szip</code> <code>2.1.1</code> Website Szip compression software, providing lossless compression of scientific data, is an implementation of the extended-Rice lossless compression algorithm.   compression <code>xz</code> <code>5.2.3</code> Website XZ Utils, the successor to LZMA Utils, is free general-purpose data compression software with a high compression ratio.   compression <code>zlib</code> <code>1.2.11</code> Website zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system.   containers <code>libnvidia-container</code> <code>1.0.0rc2</code> Website libnvidia-container is a library and a simple CLI utility to automatically configure GNU/Linux containers leveraging NVIDIA hardware.   containers <code>proot</code> <code>5.1.0</code><code>5.2.0</code> Website PRoot is a user-space implementation of chroot, mount --bind, and binfmt_misc.   database <code>bdb</code> <code>6.2.32</code> Website Berkeley DB (BDB) is a software library intended to provide a high-performance embedded database for key/value data.   database <code>mariadb</code> <code>10.2.11</code> Website MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL.   database <code>postgresql</code> <code>10.5</code> Website PostgreSQL is a powerful, open source object-relational database system with a strong focus on reliability, feature robustness, and performance.   database <code>sqlite</code> <code>3.18.0</code><code>3.37.2</code> Website SQLite is a self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine.   database <code>sqliteodbc</code> <code>0.9998</code> Website ODBC driver for SQLite   database <code>unixodbc</code> <code>2.3.9</code> Website unixODBC is an open-source project that implements the ODBC API.   document management <code>pandoc</code> <code>2.7.3</code> Website Pandoc is a universal document converter.   document processing <code>ghostscript</code> <code>9.53.2</code> Website Ghostscript is an interpreter for the PostScript language and PDF files.   document processing <code>lyx</code> <code>2.3.2</code> Website LyX is a document processor.   document processing <code>poppler</code> <code>0.47.0</code> Website Poppler is a PDF rendering library.   document processing <code>texinfo</code> <code>6.6</code> Website Texinfo is the official documentation format of the GNU project.   document processing <code>texlive</code> <code>2019</code> Website TeX Live is an easy way to get up and running with the TeX document production system.   file management <code>duc</code> <code>1.4.4</code> Website Duc is a collection of tools for indexing, inspecting and visualizing disk usage.   file management <code>exa</code> <code>0.8.0</code> Website exa is a replacement for ls written in Rust.   file management <code>fpart</code> <code>0.9.3</code> Website fpart sorts files and packs them into partitions.   file management <code>ncdu</code> <code>1.15.1</code> Website Ncdu is a disk usage analyzer with an ncurses interface.   file management <code>py-pcircle</code> <code>0.17_py27</code> Website pcircle contains a suite of file system tools developed at OLCF to take advantage of highly scalable parallel file system such as Lustre.   file management <code>rmlint</code> <code>2.8.0</code> Website rmlint finds space waste and other broken things on your filesystem and offers to remove it.   file management <code>tdu</code> <code>1.36</code> Website tdu estimates the disk space occupied by all files in a given path.   file transfer <code>aria2</code> <code>1.35.0</code> Website aria2 is a lightweight multi-protocol &amp; multi-source command-line download utility.   file transfer <code>aspera-cli</code> <code>3.9.6</code> Website The IBM Aspera Command-Line Interface (the Aspera CLI) is a collection of Aspera tools for performing high-speed, secure data transfers from the command line.   file transfer <code>gsutil</code> <code>4.31</code> Website gsutil is a Python application that lets you access Cloud Storage from the command line.   file transfer <code>lftp</code> <code>4.8.1</code> Website LFTP is a sophisticated file transfer program supporting a number of network protocols (ftp, http, sftp, fish, torrent).   file transfer <code>mpifileutils</code> <code>0.10.1</code><code>0.11</code> Website mpiFileUtils is a suite of MPI-based tools to manage large datasets, which may vary from large directory trees to large files.   file transfer <code>py-globus-cli</code> <code>1.2.0</code><code>1.9.0_py27</code><code>1.9.0_py36</code><code>3.2.0_py36</code><code>3.2.0_py39</code> Website A command line wrapper over the Globus SDK for Python.   file transfer <code>rclone</code> <code>1.39</code><code>1.43.1</code><code>1.49.5</code><code>1.55.1</code> Website Rclone is a command line program to sync files and directories to and from: Google Drive, Amazon S3, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft One Drive, Hubic, Backblaze B2, Yandex Disk, or the local filesystem.   framework <code>mono</code> <code>5.12.0.301</code><code>5.20.1.19</code> Website Mono is an open source implementation of Microsoft's .NET Framework based on the ECMA standards for C# and the Common Language Runtime.   hardware <code>hwloc</code> <code>2.7.0</code> Website The Portable Hardware Locality (hwloc) software package provides a portable abstraction of the hierarchical topology of modern architectures.   hardware <code>libpciaccess</code> <code>0.16</code> Website Generic PCI access library.   job management <code>slurm-drmaa</code> <code>1.1.2</code> Website DRMAA for Slurm Workload Manager (Slurm) is an implementation of Open Grid Forum Distributed Resource Management Application API (DRMAA) version 1 for submission and control of jobs to Slurm.   language <code>tcltk</code> <code>8.6.6</code> Website Tcl (Tool Command Language) is a dynamic programming language, suitable for web and desktop applications, networking, administration, testing. Tk is a graphical user interface toolkit.   libs <code>apr</code> <code>1.6.3</code> Website The Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system.   libs <code>apr-util</code> <code>1.6.1</code> Website The Apache Portable Runtime is a supporting library for the Apache web server. It provides a set of APIs that map to the underlying operating system.   libs <code>atk</code> <code>2.24.0</code> Website ATK is the Accessibility Toolkit. It provides a set of generic interfaces allowing accessibility technologies such as screen readers to interact with a graphical user interface.   libs <code>benchmark</code> <code>1.2.0</code> Website A microbenchmark support library   libs <code>cairo</code> <code>1.14.10</code> Website Cairo is a 2D graphics library with support for multiple output devices.   libs <code>cups</code> <code>2.2.4</code> Website CUPS is the standards-based, open source printing system.   libs <code>dbus</code> <code>1.10.22</code> Website D-Bus is a message bus system, a simple way for applications to talk to one another.   libs <code>enchant</code> <code>1.6.1</code><code>2.2.3</code> Website Enchant is a library (and command-line program) that wraps a number of different spelling libraries and programs with a consistent interface.   libs <code>fltk</code> <code>1.3.4</code> Website FLTK (pronounced 'fulltick') is a cross-platform C++ GUI toolkit.   libs <code>fontconfig</code> <code>2.12.4</code> Website Fontconfig is a library for configuring and customizing font access.   libs <code>freeglut</code> <code>3.0.0</code> Website FreeGLUT is a free-software/open-source alternative to the OpenGL Utility Toolkit (GLUT) library.   libs <code>freetype</code> <code>2.8</code><code>2.9.1</code> Website FreeType is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images).   libs <code>gc</code> <code>7.6.0</code> Website The Boehm-Demers-Weiser conservative garbage collector can be used as a garbage collecting replacement for C malloc or C++ new.   libs <code>gconf</code> <code>2.9.91</code> Website GConf is a system for storing application preferences.   libs <code>gdk-pixbuf</code> <code>2.36.8</code> Website The GdkPixbuf library provides facilities for loading images in a variety of file formats.   libs <code>gflags</code> <code>2.2.1</code> Website The gflags package contains a C++ library that implements commandline flags processing.   libs <code>giflib</code> <code>5.1.4</code> Website GIFLIB is a package of portable tools and library routines for working with GIF images.   libs <code>glib</code> <code>2.52.3</code> Website The GLib library provides core non-graphical functionality such as high level data types, Unicode manipulation, and an object and type system to C programs.   libs <code>glog</code> <code>0.3.5</code> Website C++ implementation of the Google logging module.   libs <code>gnutls</code> <code>3.5.9</code> Website GnuTLS is a secure communications library implementing the SSL, TLS and DTLS protocols and technologies around them.   libs <code>gobject-introspection</code> <code>1.52.1</code> Website GObject introspection is a middleware layer between C libraries (using GObject) and language bindings.   libs <code>googletest</code> <code>1.8.0</code> Website Google Test is Google's C++ test framework.   libs <code>gstreamer</code> <code>1.12.0</code> Website GStreamer is a library for constructing graphs of media-handling components.   libs <code>gtk+</code> <code>2.24.30</code><code>3.22.18</code> Website GTK+, or the GIMP Toolkit, is a multi-platform toolkit for creating graphical user interfaces.   libs <code>gtkglext</code> <code>1.2.0</code> Website GtkGLExt is an OpenGL extension to GTK+.   libs <code>harfbuzz</code> <code>1.4.8</code> Website HarfBuzz is an OpenType text shaping engine.   libs <code>hunspell</code> <code>1.6.2</code> Website Hunspell is a spell checker.   libs <code>hyphen</code> <code>2.8.8</code> Website Hyphen is a hyphenation library to use converted TeX hyphenation patterns.   libs <code>icu</code> <code>59.1</code> Website ICU is a set of C/C++ and Java libraries providing Unicode and Globalization support for software applications.   libs <code>json-glib</code> <code>1.4.4</code> Website JSON-GLib is a library providing serialization and deserialization support for the JavaScript Object Notation (JSON) format described by RFC 4627.   libs <code>libaio</code> <code>0.3.111</code> Website libaio provides the Linux-native API for async I/O.   libs <code>libart_lgpl</code> <code>2.3.21</code> Website Libart is a library for high-performance 2D graphics.   libs <code>libcroco</code> <code>0.6.13</code> Website Libcroco is a standalone css2 parsing and manipulation library.   libs <code>libepoxy</code> <code>1.4.1</code> Website Epoxy is a library for handling OpenGL function pointer management for you.   libs <code>libexif</code> <code>0.6.21</code> Website A library for parsing, editing, and saving EXIF data.   libs <code>libffi</code> <code>3.2.1</code> Website libffi is a portable Foreign Function Interface library.   libs <code>libgcrypt</code> <code>1.8.2</code> Website Libgcrypt is a general purpose cryptographic library originally based on code from GnuPG.   libs <code>libgd</code> <code>2.2.5</code> Website GD is an open source code library for the dynamic creation of images by programmers.   libs <code>libgdiplus</code> <code>5.6</code> Website C-based implementation of the GDI+ API   libs <code>libgnomecanvas</code> <code>2.30.3</code> Website Library for the GNOME canvas, an engine for structured graphics that offers a rich imaging model, high performance rendering, and a powerful, high-level API.   libs <code>libgpg-error</code> <code>1.27</code> Website Libgpg-error is a small library that originally defined common error values for all GnuPG components.   libs <code>libiconv</code> <code>1.16</code> Website libiconv is a conversion library for string encoding.   libs <code>libidl</code> <code>0.8.14</code> Website The libIDL package contains libraries for Interface Definition Language files. This is a specification for defining portable interfaces.   libs <code>libjpeg-turbo</code> <code>1.5.1</code> Website libjpeg-turbo is a JPEG image codec that uses SIMD instructions (MMX, SSE2, AVX2, NEON, AltiVec) to accelerate baseline JPEG compression and decompression on x86, x86-64, ARM, and PowerPC systems   libs <code>libmng</code> <code>2.0.3</code> Website THE reference library for reading, displaying, writing and examining Multiple-Image Network Graphics. MNG is the animation extension to the popular PNG image-format.   libs <code>libpng</code> <code>1.2.57</code><code>1.6.29</code> Website libpng is the official PNG reference library. It supports almost all PNG features, is extensible, and has been extensively tested for over 20 years.   libs <code>libproxy</code> <code>0.4.15</code> Website libproxy is a library that provides automatic proxy configuration management.   libs <code>libressl</code> <code>2.5.3</code><code>3.2.1</code> Website LibreSSL is a version of the TLS/crypto stack forked from OpenSSL in 2014, with goals of modernizing the codebase, improving security, and applying best practice development processes.   libs <code>librsvg</code> <code>2.36.4</code> Website Librsvg is a library to render SVG files using cairo as a rendering engine.   libs <code>libseccomp</code> <code>2.3.3</code> Website The libseccomp library provides an easy to use, platform independent, interface to the Linux Kernel's syscall filtering mechanism..   libs <code>libsodium</code> <code>1.0.18</code> Website Sodium is a modern, easy-to-use software library for encryption, decryption, signatures, password hashing and more.   libs <code>libsoup</code> <code>2.61.2</code> Website libsoup is an HTTP client/server library for GNOME.   libs <code>libtasn1</code> <code>4.13</code> Website Libtasn1 is the ASN.1 library used by GnuTLS, p11-kit and some other packages.   libs <code>libtiff</code> <code>4.0.8</code> Website libtiff provides support for the Tag Image File Format (TIFF), a widely used format for storing image data.   libs <code>libunistring</code> <code>0.9.7</code> Website Libunistring provides functions for manipulating Unicode strings and for manipulating C strings according to the Unicode standard.   libs <code>libuuid</code> <code>1.0.3</code> Website Portable uuid C library.   libs <code>libuv</code> <code>1.38.1</code> Website libuv is a multi-platform support library with a focus on asynchronous I/O.   libs <code>libwebp</code> <code>0.6.1</code> Website WebP is a modern image format that provides superior lossless and lossy compression for images on the web.   libs <code>libxkbcommon</code> <code>0.9.1</code> Website libxkbcommon is a keyboard keymap compiler and support library which processes a reduced subset of keymaps as defined by the XKB (X Keyboard Extension) specification.   libs <code>libxml2</code> <code>2.9.4</code> Website Libxml2 is a XML C parser and toolkit.   libs <code>libxslt</code> <code>1.1.32</code> Website Libxslt is the XSLT C library developed for the GNOME project. XSLT itself is a an XML language to define transformation for XML.   libs <code>mesa</code> <code>17.1.6</code> Website Mesa is an open-source implementation of the OpenGL, Vulkan and other specifications.   libs <code>ncurses</code> <code>6.0</code> Website The ncurses (new curses) library is a free software emulation of curses in System V Release 4.0 (SVr4), and more.   libs <code>nettle</code> <code>3.3</code> Website Nettle is a cryptographic library that is designed to fit easily in more or less any context.   libs <code>openjpeg</code> <code>2.3.1</code> Website OpenJPEG is an open-source JPEG 2000 codec written in C language.   libs <code>openssl</code> <code>3.0.1</code><code>3.0.2</code> Website OpenSSL is a full-featured toolkit for general-purpose cryptography and secure communication.   libs <code>orbit</code> <code>2.14.19</code> Website ORBit2 is a CORBA 2.4-compliant Object Request Broker (ORB) featuring mature C, C++ and Python bindings.   libs <code>pango</code> <code>1.40.10</code> Website Pango is a library for laying out and rendering of text, with an emphasis on internationalization.   libs <code>pangox-compat</code> <code>0.0.2</code> Website Obsolete pango functions.   libs <code>pcre</code> <code>8.40</code> Website The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5.   libs <code>pcre2</code> <code>10.35</code> Website The PCRE22 library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5.   libs <code>popt</code> <code>1.16</code> Website Library for parsing command line options.   libs <code>py-lmdb</code> <code>0.93</code> Website Universal Python binding for the LMDB 'Lightning' Database.   libs <code>py-mako</code> <code>1.0.7_py27</code><code>1.0.7_py36</code> Website Mako is a template library written in Python. It provides a familiar, non-XML syntax which compiles into Python modules for maximum performance.   libs <code>py-pygobject</code> <code>3.32.2_py36</code> Website PyGObject is a Python package which provides bindings for GObject based libraries such as GTK, GStreamer, WebKitGTK, GLib, GIO and many more.   libs <code>py-pyopengl</code> <code>3.1.5_py39</code> Website Standard OpenGL bindings for Python.   libs <code>py-pyqt5</code> <code>5.9.1_py36</code> Website PyQt5 is a comprehensive set of Python bindings for Qt v5.   libs <code>readline</code> <code>7.0</code> Website The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in.   libs <code>serf</code> <code>1.3.9</code> Website The serf library is a high performance C-based HTTP client library built upon the Apache Portable Runtime (APR) library.   libs <code>snappy</code> <code>1.1.7</code> Website A fast compressor/decompressor.   libs <code>talloc</code> <code>2.1.14</code> Website talloc is a hierarchical, reference counted memory pool system with destructors.   libs <code>utf8proc</code> <code>2.4.0</code> Website iutf8proc is a small, clean C library that provides Unicode normalization, case-folding, and other operations for data in the UTF-8 encoding.   libs <code>wxwidgets</code> <code>3.0.4</code> Website wxWidgets is a C++ library that lets developers create applications for Windows, macOS, Linux and other platforms with a single code base.   media <code>ffmpeg</code> <code>4.0</code><code>4.2.1</code><code>5.0</code> Website FFmpeg is the leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created.   media <code>libsndfile</code> <code>1.0.28</code> Website Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface.   performance <code>likwid</code> <code>4.3.2</code> Website Likwid is a simple toolsuite of command line applications for performance oriented programmers.   resource monitoring <code>nvtop</code> <code>1.1.0</code> Website Nvtop stands for NVidia TOP, a (h)top like task monitor for NVIDIA GPUs.   resource monitoring <code>remora</code> <code>1.8.5</code> Website Remora is a tool to monitor runtime resource utilization.   scm <code>gh</code> <code>1.9.1</code> Website gh is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with git and your code.   scm <code>git</code> <code>2.12.2</code> Website Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.   scm <code>git-annex</code> <code>8.20210622</code> Website git-annex allows managing files with git, without checking the file contents into git.   scm <code>git-lfs</code> <code>2.4.0</code> Website Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server.   scm <code>libgit2</code> <code>1.1.0</code> Website libgit2 is a portable, pure C implementation of the Git core methods provided as a re-entrant linkable library with a solid API   scm <code>mercurial</code> <code>4.5.3</code> Website Mercurial is a free, distributed source control management tool.   scm <code>py-dvc</code> <code>0.91.1_py36</code> Website Data Version Control or DVC is an open-source tool for data science and machine learning projects.   scm <code>subversion</code> <code>1.9.7</code><code>1.12.2</code> Website Subversion is an open source version control system.   shell <code>powershell</code> <code>7.1.5</code> Website PowerShell Core is a cross-platform automation and configuration tool/framework.   tools <code>clinfo</code> <code>2.2.18.04.06</code> Website clinfo is a simple command-line application that enumerates all possible (known) properties of the OpenCL platform and devices available on the system.   tools <code>curl</code> <code>7.54.0</code><code>7.81.0</code> Website curl is an open source command line tool and library for transferring data with URL syntax.   tools <code>depot_tools</code> <code>20200731</code> Website Tools for working with Chromium development.   tools <code>expat</code> <code>2.2.3</code> Website Expat is a stream-oriented XML parser library written in C.   tools <code>graphicsmagick</code> <code>1.3.26</code> Website GraphicsMagick is the swiss army knife of image processing.   tools <code>imagemagick</code> <code>7.0.7-2</code> Website ImageMagick is a free and open-source software suite for displaying, converting, and editing raster image and vector image files.   tools <code>leveldb</code> <code>1.20</code> Website Symas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project.   tools <code>lmdb</code> <code>0.9.21</code> Website Symas LMDB is an extraordinarily fast, memory-efficient database we developed for the Symas OpenLDAP Project.   tools <code>motif</code> <code>2.3.7</code> Website Motif is the toolkit for the Common Desktop Environment.   tools <code>parallel</code> <code>20180122</code><code>20200822</code> Website GNU parallel is a shell tool for executing jobs in parallel using one or more computers.   tools <code>qt</code> <code>5.9.1</code> Website QT is a cross-platform application framework that is used for developing application software that can be run on various software and hardware platforms.   tools <code>ripgrep</code> <code>11.0.1</code> Website ripgrep recursively searches directories for a regex pattern.   tools <code>rocksdb</code> <code>5.7.3</code> Website A library that provides an embeddable, persistent key-value store for fast storage.   tools <code>x11</code> <code>7.7</code> Website The X.Org project provides an open source implementation of the X Window System.   tools <code>xkeyboard-config</code> <code>2.21</code> Website The non-arch keyboard configuration database for X Window.","title":"system"},{"location":"docs/software/list/#viz","text":"Field Module\u00a0name Version(s) URL Description     data <code>ncview</code> <code>2.1.7</code> Website Ncview is a visual browser for netCDF format files.   gis <code>panoply</code> <code>4.10.8</code> Website Panoply plots geo-referenced and other arrays from netCDF, HDF, GRIB, and other datasets.   graphs <code>graphviz</code> <code>2.40.1</code><code>2.44.1</code> Website Graphviz is open source graph visualization software.   imaging <code>py-pillow</code> <code>5.1.0_py27</code><code>5.1.0_py36</code><code>7.0.0_py36</code><code>8.2.0_py39</code> Website Pillow is a friendly PIL (Python Imaging Library) fork.   imaging <code>py-pillow-simd</code> <code>7.0.0.post3_py36</code> Website Pillow-SIMD is an optimized version of Pillow   molecular visualization <code>pymol</code> <code>1.8.6.2</code> Website PyMOL is a Python-enhanced molecular graphics tool.   plotting <code>gnuplot</code> <code>5.2.0</code> Website Gnuplot is a portable command-line driven graphing utility for Linux, OS/2, MS Windows, OSX, VMS, and many other platforms.   plotting <code>grace</code> <code>5.1.25</code> Website Grace is a WYSIWYG tool to make two-dimensional plots of numerical data.   plotting <code>py-basemap</code> <code>1.1.0_py27</code><code>1.1.0_py36</code> Website The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python.   plotting <code>py-matplotlib</code> <code>2.2.2_py27</code><code>2.1.2_py27</code><code>2.1.2_py36</code><code>2.2.2_py36</code><code>3.1.1_py36</code><code>3.2.1_py36</code><code>3.4.2_py39</code> Website Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.   plotting <code>py-plotly</code> <code>2.4.1_py27</code> Website Plotly's Python graphing library makes interactive, publication-quality graphs online.   plotting <code>veusz</code> <code>3.3.1</code> Website Veusz is a scientific plotting and graphing program with a graphical user interface, designed to produce publication-ready 2D and 3D plots.   remote display <code>virtualgl</code> <code>2.5.2</code> Website VirtualGL is an open source toolkit that gives any Unix or Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration.","title":"viz"},{"location":"docs/software/modules/","text":"","title":"Modules"},{"location":"docs/software/modules/#environment-modules","text":"<p>Software is provided on Sherlock under the form of loadable environment modules.</p>  <p>Software is only accessible via modules</p> <p>The use of a module system means that most software is not accessible by default and has to be loaded using the <code>module</code> command. This mechanism allows us to provide multiple versions of the same software concurrently, and gives users the possibility to easily switch between software versions.</p>  <p>Sherlock uses Lmod to manage software installations. The modules system helps setting up the user's shell environment to give access to applications, and make running and compiling software easier. It also allows us to provide multiple versions of the same software, that would otherwise conflict with each other, and abstract things from the OS sometimes rigid versions and dependencies.</p> <p>When you first log into Sherlock, you'll be presented with a default, bare bone environment with minimal software available. The module system is used to manage the user environment and to activate software packages on demand. In order to use software installed on Sherlock, you must first load the corresponding software module.</p> <p>When you load a module, the system will set or modify your user environment variables to enable access to the software package provided by that module. For instance, the <code>$PATH</code> environment variable might be updated so that appropriate executables for that package can be used.</p>","title":"Environment modules"},{"location":"docs/software/modules/#module-categories","text":"<p>Modules on Sherlock are organized by scientific field, in distinct categories. This is to limit the information overload that can result when displaying the full list of available modules. Given the large diversity of the Sherlock user population,  all users are not be interested in the same kind of software, and high-energy physicists may not want to see their screens cluttered with the latest bioinformatics packages.</p>  <p>Module categories</p> <p>You will first have to load a category module before getting access to individual modules. The <code>math</code> and <code>devel</code> categories are loaded by default, and modules in those categories can be loaded directly</p>  <p>For instance, to be able to load the <code>gromacs</code> module, you'll first need to load the <code>chemistry</code> module. This can be done in a single command, by specifying first the category, then the actual application module name:</p> <pre><code>$ module load chemistry gromacs\n</code></pre> <p>The <code>math</code> and <code>devel</code> categories, which are loaded by default, provide direct access to compilers, languages, and MPI and numerical libraries.</p> <p>For a complete list of software odule categories, please refer to the list of available software</p>  <p>Searching for a module</p> <p>To know how to access a module, you can use the <code>module spider &lt;module_name&gt;</code> command. It will search through all the installed modules, even if they're masked, and display instructions to load them. See the Examples section for details.</p>","title":"Module categories"},{"location":"docs/software/modules/#module-usage","text":"<p>The most common <code>module</code> commands are outlined in the following table. <code>module</code> commands may be shortened with the <code>ml</code> alias, with slightly different semantics.</p>  <p>Module names auto-completion</p> <p>The <code>module</code> command supports auto-completion, so you can just start typing the name of a module, and press Tab to let the shell automatically complete the module name and/or version.</p>     Module\u00a0command Short\u00a0version Description     <code>module avail</code> <code>ml av</code> List\u00a0available\u00a0software1   <code>module spider gromacs</code> <code>ml spider gromacs</code> Search for particular software   <code>module keyword blas</code> <code>ml key blas</code> Search for <code>blas</code> in module names and descriptions   <code>module whatis gcc</code> <code>ml whatis gcc</code> Display information about the <code>gcc</code> module   <code>module help gcc</code> <code>ml help gcc</code> Display module specific help   <code>module load gcc</code> <code>ml gcc</code> Load a module to use the associated software   <code>module load gsl/2.3</code> <code>ml gsl/2.3</code> Load specific version of a module   <code>module unload gcc</code> <code>ml -gcc</code> Unload a module   <code>module swap gcc icc</code> <code>ml -gcc icc</code> Swap a module (unload <code>gcc</code> and replace it with <code>icc</code>)   <code>module purge</code> <code>ml purge</code> Remove all modules2   <code>module save foo</code> <code>ml save foo</code> Save the state of all loaded modules in a collection named <code>foo</code>   <code>module restore foo</code> <code>ml restore foo</code> Restore the state of saved modules from the <code>foo</code> collection    <p>Additional module sub-commands are documented in the <code>module help</code> command. For complete reference, please refer to the official Lmod documentation.</p>","title":"Module usage"},{"location":"docs/software/modules/#module-properties","text":"<p>Multiple versions</p> <p>When multiple versions of the same module exist, <code>module</code> will load the one marked as <code>Default (D)</code>. For the sake of reproducibility, we recommend always specifying the module version you want to load, as defaults may evolve over time.</p>  <p>To quickly see some of the modules characteristics, <code>module avail</code> will display colored property attributes next to the module names. The main module properties are:</p> <ul> <li><code>S</code>: Module is sticky, requires <code>--force</code> to unload or purge</li> <li><code>L</code>: Indicate currently loaded module</li> <li><code>D</code>: Default module that will be loaded when multiple versions are available</li> <li><code>r</code>: Restricted access, typically software under license.  Contact   us for details</li> <li><code>g</code>: GPU-accelerated software, will only run on GPU nodes</li> <li><code>m</code>: Software supports parallel execution using MPI</li> </ul>","title":"Module properties"},{"location":"docs/software/modules/#searching-for-modules","text":"<p>You can search through all the available modules for either:</p> <ul> <li>a module name (if you already know it), using <code>module spider</code></li> <li>any string within modules names and descriptions, using <code>module keyword</code></li> </ul> <p>For instance, if you want to know how to load the <code>gromacs</code> module, you can do:</p> <pre><code>$ module spider gromacs\n</code></pre> <p>If you don't know the module name, or want to list all the modules that contain a specific string of characters in their name or description, you can use <code>module keyword</code>. For instance, the following command will list all the modules providing a BLAS library:</p> <pre><code>$ module keyword blas\n</code></pre>","title":"Searching for modules"},{"location":"docs/software/modules/#examples","text":"","title":"Examples"},{"location":"docs/software/modules/#listing","text":"<p>To list all the modules that can be loaded, you can do:</p> <pre><code>$ ml av\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology      devel (S,L)    physics    system\n   chemistry    math  (S,L)    staging    viz\n\n  Where:\n   S:  Module is Sticky, requires --force to unload or purge\n   r:  Restricted access\n   g:  GPU support\n   L:  Module is loaded\n   m:  MPI support\n   D:  Default Module\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching\nany of the \"keys\".\n</code></pre>","title":"Listing"},{"location":"docs/software/modules/#searching","text":"<p>To search for a specific string in modules names and descriptions, you can run:</p> <pre><code>$ module keyword numpy\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria: \"numpy\"\n---------------------------------------------------------------------------\n\n  py-scipystack: py-scipystack/1.0_py27, py-scipystack/1.0_py36\n    The SciPy Stack is a collection of open source software for scientific\n    computing in Python. It provides the following packages: numpy, scipy,\n    matplotlib, ipython, jupyter, pandas, sympy and nose.\n\n---------------------------------------------------------------------------\n[...]\n$ ml key compiler\n---------------------------------------------------------------------------\n\nThe following modules match your search criteria: \"compiler\"\n---------------------------------------------------------------------------\n\n  cmake: cmake/3.8.1\n    CMake is an extensible, open-source system that manages the build\n    process in an operating system and in a compiler-independent manner.\n\n  gcc: gcc/6.3.0, gcc/7.1.0\n    The GNU Compiler Collection includes front ends for C, C++, Fortran,\n    Java, and Go, as well as libraries for these languages (libstdc++,\n    libgcj,...).\n\n  icc: icc/2017.u2\n    Intel C++ Compiler, also known as icc or icl, is a group of C and C++\n    compilers from Intel\n\n  ifort: ifort/2017.u2\n    Intel Fortran Compiler, also known as ifort, is a group of Fortran\n    compilers from Intel\n\n  llvm: llvm/4.0.0\n    The LLVM Project is a collection of modular and reusable compiler and\n    toolchain technologies. Clang is an LLVM native C/C++/Objective-C\n    compiler,\n\n---------------------------------------------------------------------------\n</code></pre> <p>To get information about a specific module, especially how to load it, the following command can be used:</p> <pre><code>$ module spider gromacs\n\n-------------------------------------------------------------------------------\n  gromacs: gromacs/2016.3\n-------------------------------------------------------------------------------\n    Description:\n      GROMACS is a versatile package to perform molecular dynamics, i.e.\n      simulate the Newtonian equations of motion for systems with hundreds to\n      millions of particles.\n\n    Properties:\n      GPU support      MPI support\n\n    You will need to load all module(s) on any one of the lines below before\n    the \"gromacs/2016.3\" module is available to load.\n\n      chemistry\n</code></pre>","title":"Searching"},{"location":"docs/software/modules/#loading","text":"<p>Loading a category module allows to get access to field-specific software:</p> <pre><code>$ ml chemistry\n$ ml av\n\n------------- chemistry -- quantum chemistry, molecular dynamics --------------\n   gromacs/2016.3 (g,m)    vasp/5.4.1 (g,r,m)\n\n-- math -- numerical libraries, statistics, deep-learning, computer science ---\n   R/3.4.0             gsl/1.16             openblas/0.2.19\n   cudnn/5.1  (g)      gsl/2.3       (D)    py-scipystack/1.0_py27 (D)\n   cudnn/6.0  (g,D)    imkl/2017.u2         py-scipystack/1.0_py36\n   fftw/3.3.6          matlab/R2017a (r)\n\n------------------ devel -- compilers, MPI, languages, libs -------------------\n   boost/1.64.0          icc/2017.u2           python/2.7.13    (D)\n   cmake/3.8.1           ifort/2017.u2         python/3.6.1\n   cuda/8.0.61    (g)    impi/2017.u2   (m)    scons/2.5.1_py27 (D)\n   eigen/3.3.3           java/1.8.0_131        scons/2.5.1_py36\n   gcc/6.3.0      (D)    julia/0.5.1           sqlite/3.18.0\n   gcc/7.1.0             llvm/4.0.0            tbb/2017.u2\n   h5utils/1.12.1        nccl/1.3.4     (g)    tcltk/8.6.6\n   hdf5/1.10.0p1         openmpi/2.0.2  (m)\n\n-------------- categories -- load to make more modules available --------------\n   biology          devel (S,L)    physics    system\n   chemistry (L)    math  (S,L)    staging    viz\n\n[...]\n</code></pre>","title":"Loading"},{"location":"docs/software/modules/#reseting-the-modules-environment","text":"<p>If you want to reset your modules environment as it was when you initially connected to Sherlock, you can use the <code>ml reset</code> command: it will remove all the modules you have loaded, and restore the original state where only the <code>math</code> and <code>devel</code> categories are accessible.</p> <p>If you want to remove all modules from your environment, including the default <code>math</code> and <code>devel</code> modules, you can use <code>ml --force purge</code>.</p>","title":"Reseting the modules environment"},{"location":"docs/software/modules/#loading-modules-in-jobs","text":"<p>In order for an application running in a Slurm job to have access to any necessary module-provided software packages, we recommend loading those modules in the job script directly. Since Slurm propagates all user environment variables by default, this is not strictly necessary, as jobs will inherit the modules loaded at submission time. But to make sure things are reproducible and avoid issues, it is preferable to explicitly load the modules in the batch scripts.</p> <p><code>module load</code> commands should be placed right after <code>#SBATCH</code> directives and before the actual executable calls. For instance:</p> <pre><code>#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n#SBATCH ...\n\nml reset\nml load gromacs/2016.3\n\nsrun gmx_mpi ...\n</code></pre>","title":"Loading modules in jobs"},{"location":"docs/software/modules/#custom-modules","text":"<p>Users are welcome and encouraged to build and install their own software on Sherlock. To that end, and to facilitate usage or sharing of their custom software installations, they can create their own module repositories.</p> <p>See the Software Installation page for more details.</p>","title":"Custom modules"},{"location":"docs/software/modules/#contributed-software","text":"<p>PI groups, labs or departments can share their software installations and modules with the whole Sherlock community of users, and let everyone benefit from their tuning efforts and software developments.</p> <p>Those modules are available in the specific <code>contribs</code> category, and organized by contributor name.</p> <p>For instance, listing the available contributed modules can be done with:</p> <pre><code>$ ml contribs\n$ ml av\n-------------------- contribs -- contributed software ----------------------\n   poldrack\n</code></pre> <p>To get information about a specific lab module:</p> <pre><code>$ ml show poldrack\n----------------------------------------------------------------------------\n   /share/software/modules/contribs/poldrack.lua:\n----------------------------------------------------------------------------\nprepend_path(\"MODULEPATH\",\"/home/groups/russpold/modules\")\nwhatis(\"Name:        poldrack\")\nwhatis(\"Version:     1.0\")\nwhatis(\"Category:    contribs\")\nwhatis(\"URL:         https://github.com/poldracklab/lmod_modules\")\nwhatis(\"Description: Software modules contributed by the Poldrack Lab.\")\n</code></pre> <p>And to list the available software modules contributed by the lab:</p> <pre><code>$ ml poldrack\n$ ml av\n\n------------------------ /home/groups/russpold/modules -------------------------\n   afni/17.3.03           freesurfer/6.0.1            gsl/2.3      (D)\n   anaconda/5.0.0-py36    fsl/5.0.9                   pigz/2.4\n   ants/2.1.0.post710     fsl/5.0.11           (D)    remora/1.8.2\n   c3d/1.1.0              git-annex/6.20171109        xft/2.3.2\n[...]\n</code></pre>   <ol> <li> <p>If a module is not listed here, it might be unavailable in the   loaded modules categories, and require loading another category module.   Search for not-listed software using the <code>module spider</code> command.\u00a0\u21a9</p> </li> <li> <p>The <code>math</code> and <code>devel</code> category modules will not be unloaded   with <code>module purge</code> as they are \"sticky\". If a user wants to unload a sticky   module, they must specify the <code>--force</code> option.\u00a0\u21a9</p> </li> </ol>","title":"Contributed software"},{"location":"docs/software/using/R/","text":"","title":"R"},{"location":"docs/software/using/R/#introduction","text":"<p>R is a programming language and software environment for statistical computing and graphics.  It is similar to the S language and environment developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies). R provides a wide variety of statistical and graphical techniques and is highly extensible.</p>","title":"Introduction"},{"location":"docs/software/using/R/#more-documentation","text":"<p>The following documentation is specifically intended for using R on Sherlock. For more complete documentation about R in general, please see the R documentation.</p>","title":"More documentation"},{"location":"docs/software/using/R/#r-on-sherlock","text":"<p>R is available on Sherlock and the corresponding module can be loaded with:</p> <pre><code>$ ml R\n</code></pre> <p>For a list of available versions, you can execute <code>ml spider R</code> at the Sherlock prompt, or refer to the Software list page.</p>","title":"R on Sherlock"},{"location":"docs/software/using/R/#using-r","text":"<p>Once your environment is configured (ie. when the <code>R</code> module is loaded), R can be started by simply typing R at the shell prompt:</p> <pre><code>$ R\n\nR version 3.5.1 (2018-07-02) -- \"Feather Spray\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n[...]\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\n</code></pre> <p>For a listing of command line options:</p> <pre><code>$ R --help\n</code></pre>","title":"Using R"},{"location":"docs/software/using/R/#running-a-r-script","text":"<p>There are several ways to launch an R script on the command line, which will have different ways of presenting the script's output:</p>    Method Output     <code>Rscript script.R</code> displayed on screen, on <code>stdout</code>   <code>R CMD BATCH script.R</code> redirected to a <code>script.Rout</code> file   <code>R --no-save &lt; script.R</code> displayed on screen, on <code>stdout</code>","title":"Running a R script"},{"location":"docs/software/using/R/#submitting-a-r-job","text":"<p>Here's an example R batch script that can be submitted via <code>sbatch</code>. It runs a simple matrix multiplication example, and demonstrate how to feed R code as a HEREDOC to R directly, so no intermediate R script is necessary:</p> R-test.sbatch   <pre><code>#!/usr/bin/bash\n#SBATCH --time=00:10:00\n#SBATCH --mem=10G\n#SBATCH --output=R-test.log\n\n# load the module\nml R\n\n# run R code\nR --no-save &lt;&lt; EOF\nset.seed (1)\nm &lt;- 4000\nn &lt;- 4000\nA &lt;- matrix (runif (m*n),m,n)\nsystem.time (B &lt;- crossprod(A))\nEOF\n</code></pre>    <p>You can save this script as <code>R-test.sbatch</code> and submit it to the scheduler with:</p> <pre><code>$ sbatch R-test.sbatch\n</code></pre> <p>Once the job is done, you should get a <code>R-test.out</code> file in the current directory, with the following contents:</p> <pre><code>R version 3.5.1 (2018-07-02) -- \"Feather Spray\"\n[...]\n&gt; set.seed (1)\n&gt; m &lt;- 4000\n&gt; n &lt;- 4000\n&gt; A &lt;- matrix (runif (m*n),m,n)\n&gt; system.time (B &lt;- crossprod(A))\n   user  system elapsed\n  2.649   0.077   2.726\n</code></pre>","title":"Submitting a R job"},{"location":"docs/software/using/R/#r-packages","text":"<p>R comes with a single package library in <code>$R_HOME/library</code>, which contains the standard and most common packages. This is usually in a system location and is not writable by end-users.</p> <p>To accommodate individual user's requirements, R provides a way for each user to install packages in the location of their choice. The default value for a directory where users can install their own R packages is <code>$HOME/R/x86_64-pc-linux-gnu-library/&lt;R_version&gt;</code> where <code>&lt;R_version&gt;</code> depends on the R version that is used.  For instance, if you have the <code>R/3.5.1</code> module loaded, the default R user library path will be <code>$HOME/R/x86_64-pc-linux-gnu-library/3.5</code>.</p> <p>This directory doesn't exist by default. The first time a user installs an R package, R will ask if she wants to use the default location and create the directory.</p>","title":"R packages"},{"location":"docs/software/using/R/#installing-packages","text":"<p>To install a R package in your personal environment, the first thing to do is load the R module:</p> <pre><code>$ ml R\n</code></pre> <p>Then start a R session, and use the <code>install.packages()</code> function at the R prompt. For instance, the following example will install the <code>doParallel</code> package, using the US mirror of the CRAN repository:</p> <pre><code>$ R\n\nR version 3.5.1 (2018-07-02) -- \"Feather Spray\"\n[...]\n\n&gt; install.packages('doParallel', repos='http://cran.us.r-project.org')\n</code></pre> <p>It should give the following warning:</p> <pre><code>Warning in install.packages(\"doParallel\", repos = \"http://cran.us.r-project.org\") :\n  'lib = \"/share/software/user/open/R/3.5.1/lib64/R/library\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel)\nWould you like to create a personal library\n\u2018~/R/x86_64-pc-linux-gnu-library/3.5\u2019\nto install packages into? (yes/No/cancel) y\n</code></pre> <p>Answering <code>y</code> twice will make R create a <code>~/R/x86_64-pc-linux-gnu-library/3.5</code> directory and instruct it to install future R packages there.</p> <p>The installation will then proceed:</p> <pre><code>trying URL 'http://cran.us.r-project.org/src/contrib/doParallel_1.0.14.tar.gz'\nContent type 'application/x-gzip' length 173607 bytes (169 KB)\n==================================================\ndownloaded 169 KB\n\n* installing *source* package \u2018doParallel\u2019 ...\n** package \u2018doParallel\u2019 successfully unpacked and MD5 sums checked\n** R\n** demo\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded\n* DONE (doParallel)\n\nThe downloaded source packages are in\n        \u2018/tmp/Rtmp0RHrMZ/downloaded_packages\u2019\n&gt;\n</code></pre> <p>and when it's done, you should be able to load the package within R with:</p> <pre><code>&gt; library(doParallel)\nLoading required package: foreach\nLoading required package: iterators\nLoading required package: parallel\n&gt;\n</code></pre>","title":"Installing packages"},{"location":"docs/software/using/R/#package-dependencies","text":"<p>Sometimes when installing R packages, other software is needed for the installation and/or compilation.  For instance, when trying to install the <code>sf</code> package, you may encounter the following error messages:</p> <pre><code>&gt; install.packages(\"sf\")\n[...]\nConfiguration failed because libudunits2.so was not found. Try installing:...\n[...]\nconfigure: error: gdal-config not found or not executable.\n</code></pre> <p>This is because <code>sf</code> needs a few dependencies, like <code>udunits</code> and <code>gdal</code> in order to compile and install successfully.  Fortunately those dependencies are already available as modules on Sherlock.</p> <p>Whenever you see \"not found\" errors, you may want to try searching the modules inventory with <code>module spider</code>:</p> <pre><code>$ module spider udunits\n\n----------------------------------------------------------------------------\n  udunits: udunits/2.2.26\n----------------------------------------------------------------------------\n    Description:\n      The UDUNITS package from Unidata is a C-based package for the\n      programatic handling of units of physical quantities.\n\n\n    You will need to load all module(s) on any one of the lines below before\n    the \"udunits/2.2.26\" module is available to load.\n\n      physics\n</code></pre> <p>So for <code>sf</code>, in order to load the dependencies, exit <code>R</code>, load the <code>udunits</code> and <code>gdal</code> modules, and try installing <code>sf</code> again:</p> <pre><code>$ ml load physics udunits gdal\n$ ml R\n$ R\n&gt; install.packages(\"sf\")\n</code></pre> <p>Sometimes, getting dependencies right is a matter of trial and error.  You may have to load R, install packages, search modules, load modules, install packages again and so forth.  Fortunately, R packages only need to be installed once, and many R package dependencies are already available as modules on Sherlock, you just need to search for them with <code>module spider</code> and load them.</p> <p>And in case you're stuck, you can of course always send us an email and we'll be happy to assist.</p>","title":"Package dependencies"},{"location":"docs/software/using/R/#alternative-installation-path","text":"<p>To install R packages in a different location, you'll need to create that directory, and instruct R to install the packages there:</p> <pre><code>$ mkdir ~/R_libs/\n$ R\n[...]\n&gt; install.packages('doParallel', repos='http://cran.us.r-project.org', lib=\"~/R_libs\")\n</code></pre> <p>The installation will proceed normally and the <code>doParallel</code> package will be installed in <code>$HOME/R_libs/</code>.</p> <p>Specifying the full destination path for each package installation could quickly become tiresome, so to avoid this, you can create a <code>.Renviron</code> file in your <code>$HOME</code> directory, and define your <code>R_libs</code> path there:</p> <pre><code>$ cat &lt;&lt; EOF &gt; $HOME/.Renviron\nR_LIBS=~/R_libs\nEOF\n</code></pre> <p>With this, whenever R is started, the <code>$HOME/R_libs/</code> directory will be added to the list of places R will look for packages, and you won't need to specify this installation path when using <code>install.packages()</code> anymore.</p>  <p>Where does R look for packages?</p> <p>To see the directories where R searches for packages and libraries, you can use the following command in R:</p> <pre><code>&gt; .libPaths()\n</code></pre>   <p>Sharing R packages</p> <p>If you'd like to share R packages within your group, you can simply define <code>$R_LIBS</code> to point to a shared directory, such as <code>$GROUP_HOME/R_libs</code> and have each user in the group use the instructions below to define it in their own environment.</p>","title":"Alternative installation path"},{"location":"docs/software/using/R/#setting-the-repository","text":"<p>When installing a package, R needs to know from which repository the package should be downloaded. If it's not specified, it will prompt for it and display a list of available CRAN mirrors.</p> <p>To avoid setting the CRAN mirror each time you run install.packages you can permanently set the mirror by creating a <code>.Rprofile</code> file in your <code>$HOME</code> directory, which R will execute each time it starts.</p> <p>For instance, adding the following contents to your <code>~/.Rprofile</code> will make sure that every <code>install.packages()</code> invocation will use the closest CRAN mirror:</p> <pre><code>## local creates a new, empty environment\n## This avoids polluting the global environment with\n## the object r\nlocal({\n  r = getOption(\"repos\")\n  r[\"CRAN\"] = \"https://cloud.r-project.org/\"\n  options(repos = r)\n})\n</code></pre> <p>Once this is set, you only need to specify the name of the package to install, and R will use the mirror you defined automatically:</p> <pre><code>&gt; install.packages(\"doParallel\")\n[...]\ntrying URL 'https://cloud.r-project.org/src/contrib/doParallel_1.0.14.tar.gz'\nContent type 'application/x-gzip' length 173607 bytes (169 KB)\n==================================================\ndownloaded 169 KB\n</code></pre>","title":"Setting the repository"},{"location":"docs/software/using/R/#installing-packages-from-github","text":"<p>R packages can be directly installed from GitHub using the <code>devtools</code> package. <code>devtools</code> needs to be installed first, with:</p> <pre><code>&gt; install.packages(\"devtools\")\n</code></pre> <p>And then, you can then install a R package directly from its GitHub repository. For instance, to install <code>dplyr</code> from url_dplyr:</p> <pre><code>&gt; library(devtools)\n&gt; install_github(\"tidyverse/dplyr\")\n</code></pre>","title":"Installing packages from GitHub"},{"location":"docs/software/using/R/#updating-packages","text":"<p>To upgrade R packages, you can use the <code>update.packages()</code> function within a R session.</p> <p>For instance, to update the <code>doParallel</code> package:</p> <pre><code>&gt; update.packages('doParallel')\n</code></pre> <p>When the package name is omitted, <code>update.pacakges()</code> will try to update all the packages that are installed. Which is the most efficient way to ensure that all the packages in your local R library are up to date.</p>  <p>Centrally installed packages can not be updated</p> <p>Note that attempting to update centrally installed packages will fail. You will have to use <code>install.packages()</code> to install your own version of the packages in your <code>$HOME</code> directory instead.</p>","title":"Updating Packages"},{"location":"docs/software/using/R/#removing-packages","text":"<p>To remove a package from your local R library, you can use the <code>remove.packages()</code> function. For instance:</p> <pre><code>&gt; remove.packages('doParallel')\n</code></pre>","title":"Removing packages"},{"location":"docs/software/using/R/#examples","text":"","title":"Examples"},{"location":"docs/software/using/R/#single-node","text":"<p>R has a couple of powerful and easy to use tools for parallelizing your R jobs. <code>doParallel</code> is one of them. If the <code>doParallel</code> package is not installed in your environment yet, you can install it in a few easy step.</p> <p>Here is a quick <code>doParallel</code> example that uses one node and 16 cores on Sherlock (more nodes or CPU cores can be requested, as needed).</p> <p>Save the two scripts below in a directory on Sherlock:</p> doParallel_test.RdoParallel_test.sbatch   <pre><code># Example doParallel script\n\nif(!require(doParallel)) install.packages(\"doParallel\")\nlibrary(doParallel)\n\n# use the environment variable SLURM_NTASKS_PER_NODE to set\n# the number of cores to use\nregisterDoParallel(cores=(Sys.getenv(\"SLURM_NTASKS_PER_NODE\")))\n\n# bootstrap iteration example\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\niterations &lt;- 10000# Number of iterations to run\n\n# parallel loop\n# note the '%dopar%' instruction\nparallel_time &lt;- system.time({\n  r &lt;- foreach(icount(iterations), .combine=cbind) %dopar% {\n    ind &lt;- sample(100, 100, replace=TRUE)\n    result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n    coefficients(result1)\n  }\n})[3]\n\n# show the number of parallel workers to be used\ngetDoParWorkers()\n\n# execute the function\nparallel_time\n</code></pre>   <pre><code>#!/bin/bash\n\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=16\n#SBATCH --output=doParallel_test.log\n\n# --ntasks-per-node will be used in doParallel_test.R to specify the number\n# of cores to use on the machine.\n\n# load modules\nml R/3.5.1\n\n# execute script\nRscript doParallel_test.R\n</code></pre>    <p>And then submit the job with:</p> <pre><code>$ sbatch doParallel_test.sbatch\n</code></pre> <p>Once the job has completed, the output file should contain something like this:</p> <pre><code>$ cat doParallel_test.out\n[1] \"16\"\nelapsed\n  3.551\n</code></pre> <p>Bonus points: observe the scalability of the <code>doParallel</code> loop by submitting the same script using a varying number of CPU cores:</p> <pre><code>$ for i in 2 4 8 16; do\n    sbatch --out=doP_${i}.out --ntasks-per-node=$i doParallel_test.sbatch\ndone\n</code></pre> <p>When the jobs are done:</p> <pre><code>$ for i in 2 4 8 16; do\n    printf \"%2i cores: %4.1fs\\n\" $i $(tail -n1 doP_$i.out)\ndone\n 2 cores: 13.6s\n 4 cores:  7.8s\n 8 cores:  4.9s\n16 cores:  3.6s\n</code></pre>","title":"Single node"},{"location":"docs/software/using/R/#multiple-nodes","text":"<p>To distribute parallel R tasks on multiple nodes, you can use the <code>Rmpi</code> package, which provides MPI bindings for R.</p> <p>To install the <code>Rmpi</code> package, a module providing MPI library must first be loaded. For instance:</p> <pre><code>$ ml openmpi R\n$ R\n&gt; install.packages(\"Rmpi\")\n</code></pre> <p>Once the package is installed, the following scripts demonstrate a very basic <code>Rmpi</code> example.</p> Rmpi-test.RRmpi-test.sbatch   <pre><code># Example Rmpi script\n\nif (!require(\"Rmpi\")) install.packages(\"Rmpi\")\nlibrary(Rmpi)\n\n# initialize an Rmpi environment\nns &lt;- mpi.universe.size() - 1\nmpi.spawn.Rslaves(nslaves=ns, needlog=TRUE)\n\n# send these commands to the slaves\nmpi.bcast.cmd( id &lt;- mpi.comm.rank() )\nmpi.bcast.cmd( ns &lt;- mpi.comm.size() )\nmpi.bcast.cmd( host &lt;- mpi.get.processor.name() )\n\n# all slaves execute this command\nmpi.remote.exec(paste(\"I am\", id, \"of\", ns, \"running on\", host))\n\n# close down the Rmpi environment\nmpi.close.Rslaves(dellog = FALSE)\nmpi.exit()\n</code></pre>   <pre><code>#!/bin/bash\n\n#SBATCH --nodes=2\n#SBATCH --ntasks=4\n#SBATCH --output=Rmpi-test.log\n\n## load modules\n# openmpi is not loaded by default with R, so it must be loaded explicitely\nml R openmpi\n\n## run script\n# we use '-np 1' since Rmpi does its own task management\nmpirun -np 1 Rscript Rmpi-test.R\n</code></pre>    <p>You can save those scripts as <code>Rmpi-test.R</code> and <code>Rmpi-test.sbatch</code> and then submit your job with:</p> <pre><code>$ sbatch Rmpi-test.sbatch\n</code></pre> <p>When the job is done, its output should look like this:</p> <pre><code>$ cat Rmpi-test.log\n        3 slaves are spawned successfully. 0 failed.\nmaster (rank 0, comm 1) of size 4 is running on: sh-06-33\nslave1 (rank 1, comm 1) of size 4 is running on: sh-06-33\nslave2 (rank 2, comm 1) of size 4 is running on: sh-06-33\nslave3 (rank 3, comm 1) of size 4 is running on: sh-06-34\n$slave1\n[1] \"I am 1 of 4 running on sh-06-33\"\n\n$slave2\n[1] \"I am 2 of 4 running on sh-06-33\"\n\n$slave3\n[1] \"I am 3 of 4 running on sh-06-34\"\n\n[1] 1\n[1] \"Detaching Rmpi. Rmpi cannot be used unless relaunching R.\"\n</code></pre>","title":"Multiple nodes"},{"location":"docs/software/using/R/#gpus","text":"<p>Here's a quick example that compares running a matrix multiplication on a CPU and on a GPU using R. It requires submitting a job to a GPU node and the <code>gpuR</code> R package.</p> gpuR-test.RgpuR-test.sbatch   <pre><code># Example gpuR script\n\nif (!require(\"gpuR\")) install.packages(\"gpuR\")\nlibrary(gpuR)\n\nprint(\"CPU times\")\nfor(i in seq(1:7)) {\n    ORDER = 64*(2^i)\n    A = matrix(rnorm(ORDER^2), nrow=ORDER)\n    B = matrix(rnorm(ORDER^2), nrow=ORDER)\n    print(paste(i, sprintf(\"%5.2f\", system.time({C = A %*% B})[3])))\n}\n\nprint(\"GPU times\")\nfor(i in seq(1:7)) {\n    ORDER = 64*(2^i)\n    A = matrix(rnorm(ORDER^2), nrow=ORDER)\n    B = matrix(rnorm(ORDER^2), nrow=ORDER)\n    gpuA = gpuMatrix(A, type=\"double\")\n    gpuB = gpuMatrix(B, type=\"double\")\n    print(paste(i, sprintf(\"%5.2f\", system.time({gpuC = gpuA %*% gpuB})[3])))\n}\n</code></pre>   <pre><code>#!/bin/bash\n\n#SBATCH --partition gpu\n#SBATCH --mem 8GB\n#SBATCH --gres gpu:1\n#SBATCH --output=gpuR-test.log\n\n## load modules\n# cuda is not loaded by default with R, so it must be loaded explicitely\nml R cuda\n\nRscript gpuR-test.R\n</code></pre>    <p>After submitting the job with <code>sbatch gpuR-test.sbatch</code>, the output file should contain something like this:</p> <pre><code>[1] \"CPU times\"\n[1] \"1  0.00\"\n[1] \"2  0.00\"\n[1] \"3  0.02\"\n[1] \"4  0.13\"\n[1] \"5  0.97\"\n[1] \"6  7.56\"\n[1] \"7 60.47\"\n\n[1] \"GPU times\"\n[1] \"1  0.10\"\n[1] \"2  0.04\"\n[1] \"3  0.02\"\n[1] \"4  0.07\"\n[1] \"5  0.39\"\n[1] \"6  2.04\"\n[1] \"7 11.59\"\n</code></pre> <p>which shows a decent speedup for running on a GPU for the largest matrix sizes.</p>","title":"GPUs"},{"location":"docs/software/using/julia/","text":"","title":"Julia"},{"location":"docs/software/using/julia/#introduction","text":"<p>Julia is a high-level general-purpose dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science, without the typical need of separate compilation to be fast, also usable for client and server web use, low-level systems programming or as a specification language. Julia aims to create an unprecedented combination of ease-of-use, power, and efficiency in a single language.</p>","title":"Introduction"},{"location":"docs/software/using/julia/#more-documentation","text":"<p>The following documentation is specifically intended for using Julia on Sherlock. For more complete documentation about Julia in general, please see the Julia documentation.</p>","title":"More documentation"},{"location":"docs/software/using/julia/#julia-on-sherlock","text":"<p>Julia is available on Sherlock and the corresponding module can be loaded with:</p> <pre><code>$ ml julia\n</code></pre> <p>For a list of available versions, you can execute <code>ml spider julia</code> at the Sherlock prompt, or refer to the Software list page.</p>","title":"Julia on Sherlock"},{"location":"docs/software/using/julia/#using-julia","text":"<p>Once your environment is configured (ie. when the <code>julia</code> module is loaded), julia can be started by simply typing julia at the shell prompt:</p> <pre><code>$ julia\n\n_\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.0.0 (2018-08-08)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt;\n</code></pre> <p>For a listing of command line options:</p> <pre><code>$ julia --help\n\njulia [switches] -- [programfile] [args...]\n -v, --version             Display version information\n -h, --help                Print this message\n\n -J, --sysimage &lt;file&gt;     Start up with the given system image file\n -H, --home &lt;dir&gt;          Set location of `julia` executable\n --startup-file={yes|no}   Load `~/.julia/config/startup.jl`\n --handle-signals={yes|no} Enable or disable Julia's default signal handlers\n --sysimage-native-code={yes|no}\n                           Use native code from system image if available\n --compiled-modules={yes|no}\n                           Enable or disable incremental precompilation of modules\n\n -e, --eval &lt;expr&gt;         Evaluate &lt;expr&gt;\n -E, --print &lt;expr&gt;        Evaluate &lt;expr&gt; and display the result\n -L, --load &lt;file&gt;         Load &lt;file&gt; immediately on all processors\n\n -p, --procs {N|auto}      Integer value N launches N additional local worker processes\n                           \"auto\" launches as many workers as the number\n                           of local CPU threads (logical cores)\n --machine-file &lt;file&gt;     Run processes on hosts listed in &lt;file&gt;\n\n -i                        Interactive mode; REPL runs and isinteractive() is true\n -q, --quiet               Quiet startup: no banner, suppress REPL warnings\n</code></pre>","title":"Using Julia"},{"location":"docs/software/using/julia/#running-a-julia-script","text":"<p>A Julia program is easy to run on the command line outside of its interactive mode.</p> <p>Here is an example where we create a simple Hello World program and launch it with Julia</p> <pre><code>$ echo 'println(\"hello world\")' &gt; helloworld.jl\n</code></pre> <p>That script can now simply be executed by calling <code>julia &lt;script_name&gt;</code>:</p> <pre><code>$ julia helloworld.jl\nhello world\n</code></pre>","title":"Running a Julia script"},{"location":"docs/software/using/julia/#submitting-a-julia-job","text":"<p>Here's an example Julia sbatch script that can be submitted via <code>sbatch</code>:</p> julia_test.sbatch   <pre><code>#!/bin/bash\n\n#SBATCH --time=00:10:00\n#SBATCH --mem=4G\n#SBATCH --output=julia_test.log\n\n# load the module\nml julia\n\n# run the Julia application\nsrun julia helloworld.jl\n</code></pre>    <p>You can save this script as <code>julia_test.sbatch</code> and submit it to the scheduler with:</p> <pre><code>$ sbatch julia_test.sbatch\n</code></pre> <p>Once the job is done, you should get a <code>julia_test.log</code> file in the current directory, with the following contents:</p> <pre><code>$ cat julia_test.log\nhello world\n</code></pre>","title":"Submitting a Julia job"},{"location":"docs/software/using/julia/#julia-packages","text":"<p>Julia provides an ever-growing list of packages that can be used to install add-on functionality to your Julia code.</p> <p>Installing packages with Julia is very simple. Julia includes a package module in its base installation that handles installing, updating, and removing packages.</p> <p>First import the <code>Pkg</code> module:</p> <pre><code>julia&gt; import Pkg\njulia&gt; Pkg.status()\n    Status `~/.julia/environments/v1.0/Project.toml`\n</code></pre>  <p>Julia packages only need to be installed once</p> <p>You only need to install Julia packages once on Sherlock. Since fielsystems are shared, packages installed on one node will immediately be available on all nodes on the cluster.</p>","title":"Julia packages"},{"location":"docs/software/using/julia/#installing-packages","text":"<p>You can first check the status of packages installed on Julia using the status function of the <code>Pkg</code> module:</p> <pre><code>julia&gt; Pkg.status()\nNo packages installed.\n</code></pre> <p>You can then add packages using the add function of the <code>Pkg</code> module:</p> <pre><code>julia&gt; Pkg.add(\"Distributions\")\nINFO: Cloning cache of Distributions from git://github.com/JuliaStats/Distributions.jl.git\nINFO: Cloning cache of NumericExtensions from git://github.com/lindahua/NumericExtensions.jl.git\nINFO: Cloning cache of Stats from git://github.com/JuliaStats/Stats.jl.git\nINFO: Installing Distributions v0.2.7\nINFO: Installing NumericExtensions v0.2.17\nINFO: Installing Stats v0.2.6\nINFO: REQUIRE updated.\n</code></pre> <p>Using the status function again, you can see that the package and its dependencies have been installed:</p> <pre><code>julia&gt; Pkg.status()\nRequired packages:\n - Distributions                 0.2.7\nAdditional packages:\n - NumericExtensions             0.2.17\n - Stats                         0.2.6\n</code></pre>","title":"Installing packages"},{"location":"docs/software/using/julia/#updating-packages","text":"<p>The update function of the <code>Pkg</code> module can update all packages installed:</p> <pre><code>julia&gt; Pkg.update()\nINFO: Updating METADATA...\nINFO: Computing changes...\nINFO: Upgrading Distributions: v0.2.8 =&gt; v0.2.10\nINFO: Upgrading Stats: v0.2.7 =&gt; v0.2.8\n</code></pre>","title":"Updating Packages"},{"location":"docs/software/using/julia/#removing-packages","text":"<p>The remove function of the <code>Pkg</code> module can remove any packages installed as well:</p> <pre><code>julia&gt; Pkg.rm(\"Distributions\")\nINFO: Removing Distributions v0.2.7\nINFO: Removing Stats v0.2.6\nINFO: Removing NumericExtensions v0.2.17\nINFO: REQUIRE updated.\n\njulia&gt; Pkg.status()\nRequired packages:\n - SHA                           0.3.2\n\njulia&gt; Pkg.rm(\"SHA\")\nINFO: Removing SHA v0.3.2\nINFO: REQUIRE updated.\n\njulia&gt; Pkg.status()\nNo packages installed.\n</code></pre> <p>Working with packages in Julia is simple!</p>","title":"Removing packages"},{"location":"docs/software/using/julia/#examples","text":"<p>Julia can natively spawn parallel workers across multiple compute nodes, without using MPI. There are two main modes of operation:</p> <ol> <li> <p>ClusterManager: in this mode, you can spawn workers from within the Julia    interpreter, and each worker will actually submit jobs to the scheduler,    executing instructions within those jobs.</p> </li> <li> <p>using the <code>--machine-file</code> option: here, you submit a SLURM job and run the    Julia interpreter in parallel mode within the job's resources.</p> </li> </ol> <p>The second mode is easier to use, and more convenient, since you have all your resources available and ready to use when the job starts. In mode 1, you'll need to wait for jobs to be dispatched and executed inside Julia.</p> <p>Here is a quick example on how to use the <code>--machine-file</code> option on Sherlock.</p> <p>Given following Julia script (<code>julia_parallel_test.jl</code>) that will print a line with the process id and the node it's executing on, in parallel:</p> <pre><code>@everywhere println(\"process: $(myid()) on host $(gethostname())\")\n</code></pre> <p>You can submit the following job:</p> julia_test.sbatch   <pre><code>#!/bin/bash\n#SBATCH --nodes 2\n#SBATCH --ntasks-per-node 4\n#SBATCH --time 5:0\n\nml julia\njulia --machine-file &lt;(srun hostname -s)  ./julia_parallel_test.jl\n</code></pre>    <p>Save as <code>julia_test.sbatch</code>, and then:</p> <pre><code>$ sbatch  julia_test.sbatch\n</code></pre> <p>It will:</p> <ol> <li>Request 2 nodes, 4 tasks per node (8 tasks total)</li> <li>load the <code>julia</code> module</li> <li>Run Julia in parallel with a machine file that is automatically generated,      listing the nodes that are assigned to your job.</li> </ol> <p>It should output something like this in your job's output file:</p> <pre><code>process: 1 on host sh-06-33.int\n      From worker 2:    process: 2 on host sh-06-33.int\n      From worker 3:    process: 3 on host sh-06-34.int\n      From worker 5:    process: 5 on host sh-06-33.int\n      From worker 4:    process: 4 on host sh-06-33.int\n      From worker 6:    process: 6 on host sh-06-33.int\n      From worker 8:    process: 8 on host sh-06-34.int\n      From worker 9:    process: 9 on host sh-06-34.int\n      From worker 7:    process: 7 on host sh-06-34.int\n</code></pre>","title":"Examples"},{"location":"docs/software/using/mariadb/","text":"","title":"MariaDB"},{"location":"docs/software/using/mariadb/#introduction","text":"<p>MariaDB is a community-developed fork of the MySQL relational database management system. It is completely compatible with MySQL and could be use as a drop-in replacement in the vast majority of cases.</p>","title":"Introduction"},{"location":"docs/software/using/mariadb/#more-documentation","text":"<p>The following documentation specifically intended for using MariaDB on Sherlock. For more complete documentation about MariaDB in general, please see the MariaDB documentation.</p>","title":"More documentation"},{"location":"docs/software/using/mariadb/#mariadb-on-sherlock","text":"<p>We don't provide any centralized database service on Sherlock, but we provide a centralized installation of MariaDB, and each user is welcome to start their own instance of the database server to fit their jobs' needs.</p> <p>The overall process to run an instance of MariaDB on Sherlock would look like this:</p> <ol> <li>configure and initialize your environment so you can start a database    instance under your user account,</li> <li>start the database server,</li> <li>run SQL queries from the same node (via a local socket), or from other nodes    and/or jobs (via the network).</li> </ol>","title":"MariaDB on Sherlock"},{"location":"docs/software/using/mariadb/#single-node-access","text":"<p>In that example, the database server and client will run within the same job, on the same compute node.</p>","title":"Single-node access"},{"location":"docs/software/using/mariadb/#preparation","text":"<p>You first need to let MariaDB know where to store its database, where to log things, and how to allow connections from clients. The commands below only need to be executed once.</p> <p>For this, you'll need to create a <code>.my.cnf</code> file in your home directory. Assuming you'll want to store your database files in a <code>db/</code> directory in your <code>$SCRATCH</code> folder, you can run the following commands:</p> <pre><code>$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n\n$ cat &lt;&lt; EOF &gt; ~/.my.cnf\n[mysqld]\ndatadir=$DB_DIR\nsocket=$DB_DIR/mariadb.sock\nuser=$USER\nsymbolic-links=0\nskip-networking\n\n[mysqld_safe]\nlog-error=$DB_DIR/mariadbd.log\npid-file=$DB_DIR/mariadbd.pid\n\n[mysql]\nsocket=$DB_DIR/mariadb.sock\nEOF\n</code></pre>  <p><code>.my.cnf</code> doesn't support environment variables</p> <p>Please note that if you edit your <code>~/.my.cnf</code> file directly in a file editor, without using the HEREDOC syntax above, environment variables such as <code>$DB_DIR</code>, <code>$HOME</code> or <code>$USER</code> won't work: you will need to specify absolute paths explicitely, such as <code>/scratch/users/kilian/db/mariadbd.log</code>.</p> <p>If you use the HEREDOC syntax, you can verify that the resulting <code>.my.cnf</code> file does actually contain full paths, and not environment variables anymore.</p>  <p>Once you have the <code>.my.cnf</code> file in place, you need to initialize your database with some internal data that MariaDB needs. In the same terminal, run the following commands:</p> <pre><code>$ ml system mariadb\n$ $MARIADB_DIR/scripts/mysql_install_db --basedir=$MARIADB_DIR  --datadir=$DB_DIR\n</code></pre>","title":"Preparation"},{"location":"docs/software/using/mariadb/#start-the-server","text":"<p>You can now start the MariaDB server. For this, first get an allocation on a compute node, note the hostname of the compute node your job has been allocated, load the <code>mariadb</code> module, and then run the <code>mysqld_safe</code> process:</p> <pre><code>$ srun --pty bash\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ ml system mariadb\n$ mysqld_safe\n180705 18:14:27 mysqld_safe Logging to '/home/users/kilian/db/mysqld.log'.\n180705 18:14:28 mysqld_safe Starting mysqld daemon with databases from /home/users/kilian/db/\n</code></pre> <p>The <code>mysqld_safe</code> will be blocking, meaning it will not give the prompt back for as long as the MariaDB server runs.</p> <p>If it does return on its own, it probably means that something went wrong, and you'll find more information about the issue in the <code>$DB_DIR/mysqld.log</code> file you defined in <code>~/.my.cnf</code>.</p>","title":"Start the server"},{"location":"docs/software/using/mariadb/#run-queries","text":"<p>You're now ready to run queries against that MariaDB instance, from the same node your job is running on.</p> <p>From another terminal on Sherlock, connect to your job's compute node (here, it's <code>sh-01-01</code>, as shown above), load the <code>mariadb</code> module, and then run the <code>mysql</code> command: it will open the MariaDB shell, ready to run your SQL queries:</p> <pre><code>$ ssh sh-01-01\n$ ml system mariadb\n$ mysql\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 8\nServer version: 10.2.11-MariaDB Source distribution\n\nCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt;\n</code></pre> <p>Once you're done with your MariaDB instance, you can just terminate your job, and all the processes will be terminated automatically.</p>","title":"Run queries"},{"location":"docs/software/using/mariadb/#multi-node-access","text":"<p>In case you need to run a more persistent instance of MariaDB, you can for instance submit a dedicated job to run the server, make it accessible over the network, and run queries from other jobs and/or nodes.</p>","title":"Multi-node access"},{"location":"docs/software/using/mariadb/#enable-network-access","text":"<p>The preparation steps are pretty similar to the single-node case, except the MariaDB server instance will be accessed over the network rather than through a local socket.</p>  <p>Network access must be secured</p> <p>When running an networked instance of MariaDB, please keep in mind that any user on Sherlock will be able to connect to the TCP ports that <code>mysqld</code> runs on, and that proper configuration must be done to prevent unauthrozied access.</p>  <p>Like in the single-node case, you need to create a <code>~/.my.cnf</code> file, but without the <code>skip-networking</code> directive.</p> <pre><code>$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n\n$ cat &lt;&lt; EOF &gt; ~/.my.cnf\n[mysqld]\ndatadir=$DB_DIR\nsocket=$DB_DIR/mariadb.sock\nuser=$USER\nsymbolic-links=0\n\n[mysqld_safe]\nlog-error=$DB_DIR/mariadbd.log\npid-file=$DB_DIR/mariadbd.pid\n\n[mysql]\nsocket=$DB_DIR/mariadb.sock\nEOF\n</code></pre> <p>And then initiate the database:</p> <pre><code>$ ml system mariadb\n$ $MARIADB_DIR/scripts/mysql_install_db --basedir=$MARIADB_DIR  --datadir=$DB_DIR\n</code></pre>","title":"Enable network access"},{"location":"docs/software/using/mariadb/#secure-access","text":"<p>We will now set a password for the MariaDB <code>root</code> user to a random string, just for the purpose of preventing unauthorized access, since we won't need it for anything.</p> <p>We will actually create a MariaDB user with all privileges on the databases, that will be able to connect to this instance from any node. This user will need a real password, though. So please make sure to replace the <code>my-secure-password</code> string below by the actual password of your choice.</p>  <p>Choose a proper password</p> <p>This password will only be used to access this specific instance of MariaDB. Note that anybody knowing that password will be allowed to connect to your MariaDB instances and modify data in the tables.</p> <ul> <li>do NOT literally use <code>my-secure-password</code></li> <li>do NOT use your SUNet ID password</li> </ul>  <p>Once you've chosen your password, you can start the <code>mysqld</code> process on a compute node, like before:</p> <pre><code>$ srun --pty bash\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ ml system mariadb\n$ mysqld_safe\n</code></pre> <p>And then, from another terminal, run the following commands to secure access to your MariaDB database.</p> <pre><code>$ ssh sh-01-01\n$ mysql -u root &lt;&lt; EOF\nUPDATE mysql.user SET Password=PASSWORD(RAND()) WHERE User='root';\nDELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');\nDELETE FROM mysql.user WHERE User='';\nDELETE FROM mysql.db WHERE Db='test' OR Db='test_%';\nGRANT ALL PRIVILEGES ON *.* TO '$USER'@'%' IDENTIFIED BY 'my-secure-password' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\nEOF\n</code></pre> <p>Once you've done that, you're ready to terminate that interactive job, and start a dedicated MariaDB server job.</p>","title":"Secure access"},{"location":"docs/software/using/mariadb/#start-mariadb-in-a-job","text":"<p>You can use the following <code>mariadb.sbatch</code> job as a template:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=mariadb\n#SBATCH --time=8:0:0\n#SBATCH --dependency=singleton\n\nml system mariadb\nmysqld_safe\n</code></pre> <p>and submit it with:</p> <pre><code>$ sbatch mariadb.sbatch\n</code></pre>  <p>Concurrent instances will lead to data corruption</p> <p>An important thing to keep in mind is that having multiple instances of a MariaDB server running at the same time, using the same database files, will certainly lead to catastrophic situations and the corruption of those files.</p> <p>To prevent this from happening, the <code>--dependency=singleton</code> job submission option will make sure that only one instance of that job (based on its name and user) will run at any given time.</p>","title":"Start MariaDB in a job"},{"location":"docs/software/using/mariadb/#connect-to-the-running-instance","text":"<p>Now, from any node on Sherlock, whether from a login node, an interactive job, or a batch job, using the <code>mysql</code> CLI or any application binding in any language, you should be able to connect to your running MariaDB instance,</p> <p>First, identify the node your job is running on with <code>squeue</code>:</p> <pre><code>$ squeue -u $USER -n mariadb\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          21383445    normal  mariadb   kilian  R       0:07      1 sh-01-02\n</code></pre> <p>and then, point your MariaDB client to that node:</p> <pre><code>$ ml system mariadb\n$ mysql -h sh-01-02 -p\nEnter password:\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 15\nServer version: 10.2.11-MariaDB Source distribution\n\nCopyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt;\n</code></pre> <p>That's it! You can now run SQL queries from anywhere on Sherlock to your own MariaDB instance.</p>","title":"Connect to the running instance"},{"location":"docs/software/using/mariadb/#persistent-db-instances","text":"<p>SQL data is persistent</p> <p>All the data you import in your SQL databases will be persistent across jobs. Meaning that you can run a PostgreSQL server job for the day, import data in its database, stop the job, and resubmit the same PostgreSQL server job the next day: all your data will still be there as long as the location you've chosen for your database (the <code>$DB_DIR</code> defined in the Preparation steps) is on a persistent storage location.</p>  <p>If you need database access for more than the maximum runtime of a job, you can use the instructions provided to define self-resubmitting recurring jobs and submit long-running database instances.</p>","title":"Persistent DB instances"},{"location":"docs/software/using/matlab/","text":"","title":"Matlab"},{"location":"docs/software/using/matlab/#introduction","text":"<p>MATLAB is a numerical computing environment and proprietary programming language developed by MathWorks.</p>","title":"Introduction"},{"location":"docs/software/using/matlab/#more-documentation","text":"<p>The following documentation is specifically intended for using Matlab on Sherlock. For more complete documentation about Matlab in general, please see the official MATLAB documentation.</p>","title":"More documentation"},{"location":"docs/software/using/matlab/#matlab-on-sherlock","text":"","title":"MATLAB on Sherlock"},{"location":"docs/software/using/matlab/#licensing","text":"<p>MATLAB is a commercial software suite, which is now available to no cost for all Stanford Faculty, students, and staff.</p>  <p>Note: a number of free, open-source alternatives exist and can be used in many situations: Octave, R, Julia, or Python are all available on Sherlock, and can often replace MATLAB with good results.</p>","title":"Licensing"},{"location":"docs/software/using/matlab/#using-matlab","text":"<p>The MATLAB module can be loaded with:</p> <pre><code>$ ml load matlab\n</code></pre> <p>This will load the current default version. For a list of available versions run <code>ml spider matlab</code> at the Sherlock prompt, or refer to the Software list page.</p>  <p>MATLAB can't run on login nodes</p> <p>Running MATLAB directly on login nodes is not supported and will produce the following message: <pre><code>-----------------------------------------------------------------------\nWARNING: running MATLAB directly on login nodes is not supported.  Please\nmake sure you request an interactive session on a compute node with \"sdev\"\nfor instance) before launching MATLAB interactively.\n-----------------------------------------------------------------------\n</code></pre> You will need to submit a job or request an interactive session on a compute node before you can start MATLAB.</p>  <p>Once you are on a compute node and your environment is configured (ie. when the <code>matlab</code> module is loaded), MATLAB can be started by simply typing <code>matlab</code> at the shell prompt.</p> <pre><code>$ sdev\n$ ml load matlab\n$ matlab\nMATLAB is selecting SOFTWARE OPENGL rendering.\n                          &lt; M A T L A B (R) &gt;\n                Copyright 1984-2019 The MathWorks, Inc.\n                R2019a (9.6.0.1072779) 64-bit (glnxa64)\n                             March 8, 2019\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\n&gt;&gt;\n</code></pre> <p>For a listing of command line options:</p> <pre><code>$ matlab -help\n</code></pre>","title":"Using MATLAB"},{"location":"docs/software/using/matlab/#running-a-matlab-script","text":"<p>There are several ways to launch a MATLAB script on the command line, as documented in the MATLAB documentation:</p>    Method Output     <code>matlab -nodesktop &lt; script.m</code> MATLAB will run the code from <code>script.m</code> and display output on <code>stdout</code>   <code>matlab -nodisplay</code> Start MATLAB in CLI mode, without its graphical desktop environment   <code>matlab -nojvm</code> do not start the JVM1","title":"Running a MATLAB script"},{"location":"docs/software/using/matlab/#matlab-gui","text":"<p>It's often best to use your laptop or desktop to develop, debug MATLAB and visualize the output. If you do need to use the MATLAB GUI on a large cluster like Sherlock, you will need to enable X11 forwarding in your SSH client.</p> <p>For instance:</p> <pre><code>$ ssh -X &lt;YourSUNetID&gt;@login.sherlock.stanford.edu\n</code></pre> <p>And then, once on Sherlock:</p> <pre><code>$ sdev\n$ ml load matlab\n$ matlab\n</code></pre> <p>For more info on X11 forwarding, you can refer to this UIT page.</p>","title":"MATLAB GUI"},{"location":"docs/software/using/matlab/#examples","text":"","title":"Examples"},{"location":"docs/software/using/matlab/#simple-matlab-job","text":"<p>Here is an example MATLAB batch script that can submitted with <code>sbatch</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=matlab_test\n#SBATCH --output=matlab_test.\"%j\".out\n#SBATCH --error=matlab_test.\"%j\".err\n#SBATCH --partition=normal\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=8G\n#SBATCH --mail-type=ALL\n\nmodule load matlab\nmatlab -nodisplay &lt; example.m\n</code></pre> <p>This simple job, named <code>matlab_test</code> will run a MATLAB script named <code>example.m</code> in the <code>normal</code> partition, for a duration of 10 minutes, and use 1 CPU and 8GB of RAM.  It will send you an email (to whatever email you used wen you signed up for Sherlock) when it begins, ends or fails.</p> <p>Additionally, to aid in debugging, it will log any errors and output to the files <code>matlab_test.JOBID.{out,err}</code> with the jobid appended to the filename (<code>%j</code>).</p> <p>To create the script, open a text editor on Sherlock, copy the contents of the script, and save it as <code>matlab_test.sbatch</code></p> <p>Then, submit the job with the <code>sbatch</code> command:</p> <pre><code>$ sbatch matlab_test.sbatch\nSubmitted batch job 59942277\n</code></pre> <p>You can check the status of the job with the <code>squeue</code> command, and check the contents of the <code>matlab_test.JOBID.{out,err}</code> files to see the results.</p>","title":"Simple MATLAB job"},{"location":"docs/software/using/matlab/#parallel-loop","text":"<p>You can run your MATLAB code across multiple CPUs on Sherlock using <code>parfor</code> loops, to take advantage of the multiple CPU cores that each node features. You can submit a job requesting as many CPUs as there are on a node in a single job.  The key is to grab the SLURM environment variable <code>$SLURM_CPUS_PER_TASK</code> and create the worker pool in your MATLAB code with:</p> <pre><code>parpool('local', str2num(getenv('SLURM_CPUS_PER_TASK')))\n</code></pre> <p>Here is an example of a <code>sbatch</code> submission script that requests 16 CPUs on a node, and runs a simple MATLAB script using <code>parfor</code>.</p> <p>Save the two scripts below as <code>parfor.sbatch</code> and <code>parfor.m</code>:</p> parfor.sbatchparfor.m   <pre><code>#!/bin/bash\n#SBATCH -J pfor_matlab\n#SBATCH -o pfor\".%j\".out\n#SBATCH -e pfor\".%j\".err\n#SBATCH -t 20:00\n#SBATCH -p normal\n#SBATCH -c 16\n#SBATCH --mail-type=ALL\n\nmodule load matlab\nmatlab -nosplash -nodesktop -r parfor\n</code></pre>   <pre><code>%============================================================================\n% Parallel Monte Carlo calculation of PI\n%============================================================================\nparpool('local', str2num(getenv('SLURM_CPUS_PER_TASK')))\nR = 1;\ndarts = 1e7;\ncount = 0;\ntic\nparfor i = 1:darts\n   % Compute the X and Y coordinates of where the dart hit the...............\n   % square using Uniform distribution.......................................\n   x = R*rand(1);\n   y = R*rand(1);\n   if x^2 + y^2 &lt;= R^2\n      % Increment the count of darts that fell inside of the.................\n      % circle...............................................................\n     count = count + 1; % Count is a reduction variable.\n   end\nend\n% Compute pi.................................................................\nmyPI = 4*count/darts;\nT = toc;\nfprintf('The computed value of pi is %8.7f.n',myPI);\nfprintf('The parallel Monte-Carlo method is executed in %8.2f seconds.n', T);\ndelete(gcp);\nexit;\n</code></pre>    <p>You can now submit the job with the following command:</p> <pre><code>sbatch parfor.sbatch\n</code></pre> <p>If you run <code>htop</code> or <code>pstree -u $USER</code> on the compute node that is running your job, you will see all 16 cores allocated to your MATLAB code.</p> <p>You can also try that same job with different numbers of CPUs, and see how well it scales.</p>   <ol> <li> <p>MATLAB uses the Java\u00ae Virtual Machine (JVM\u2122) software to run the   desktop and to display graphics. The <code>-nojvm</code> option enables you to start   MATLAB without the JVM. Using this option minimizes memory usage and improves   initial start-up speed, but restricts functionality.\u00a0\u21a9</p> </li> </ol>","title":"Parallel loop"},{"location":"docs/software/using/perl/","text":"","title":"Perl"},{"location":"docs/software/using/perl/#introduction","text":"<p>Perl is a high-level, general-purpose, interpreted, dynamic programming language. Originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier, it has since undergone many changes and revisions.</p> <p>Perl provides a framework allowing users to easily extend the language by installing new modules in their local environment. The Comprehensive Perl Archive Network (CPAN1) is an archive of over 25,000 distributions of software written in Perl, as well as documentation for it. It is searchable at http://metacpan.org or http://search.cpan.org and mirrored in over 270 locations around the world.</p>","title":"Introduction"},{"location":"docs/software/using/perl/#more-documentation","text":"<p>The following documentation specifically intended for using Perl on Sherlock. For more complete documentation about Perl in general, please see the Perl documentation.</p>","title":"More documentation"},{"location":"docs/software/using/perl/#perl-modules-on-sherlock","text":"<p>To install Perl modules from CPAN, we recommend using the (provided) <code>App::cpanminus</code> module and <code>local::lib</code> modules:</p> <ul> <li><code>App::cpanminus</code> is a popular alternative CPAN client that can be used to   manage Perl distributions. It has many great features, including uninstalling   modules.</li> <li><code>local::lib</code> allows users to install Perl modules in the directory of their   choice (typically their home directory) without administrative privileges.</li> </ul> <p>Both are already installed on Sherlock, and are automatically enabled and configured when you load the <code>perl</code> module. You don't need to add anything in your <code>~/.bashrc</code> file, the Sherlock <code>perl</code> module will automatically create everything that is required so you can directly run a command to install Perl modules locally.</p>","title":"Perl modules on Sherlock"},{"location":"docs/software/using/perl/#installation","text":"<p>Perl modules installation is only necessary once</p> <p>You only need to install Perl modules once on Sherlock. Since fielsystems are shared, modules installed on one node will immediately be available on all nodes on the cluster.</p>  <p>As an example, to install the <code>DateTime::TimeZone</code> module, you can do the following:</p> <pre><code>$ ml perl\n$ cpanm DateTime::TimeZone\n</code></pre>","title":"Installation"},{"location":"docs/software/using/perl/#usage","text":"<p>Once installed, you can use the Perl modules directly, no specific options or syntax is required.</p> <p>For instance, to check that the <code>DateTime::TimeZone</code> module is correctly installed:</p> <pre><code>$ perl -MDateTime::TimeZone -e 'print $DateTime::TimeZone::VERSION . \"\\n\"';\n2.13\n</code></pre>","title":"Usage"},{"location":"docs/software/using/perl/#uninstallation","text":"<p>To uninstall a Perl module:</p> <pre><code>$ cpanm -U DateTime::TimeZone\n</code></pre>   <ol> <li> <p>CPAN can denote either the archive network itself, or the Perl program   that acts as an interface to the network and as an automated software   installer (somewhat like a package manager). Most software on CPAN is free   and open source.\u00a0\u21a9</p> </li> </ol>","title":"Uninstallation"},{"location":"docs/software/using/postgresql/","text":"","title":"PostgreSQL"},{"location":"docs/software/using/postgresql/#introduction","text":"<p>PostgreSQL is a powerful, open source object-relational database system with a strong focus on reliability, feature robustness, and performance.</p>","title":"Introduction"},{"location":"docs/software/using/postgresql/#more-documentation","text":"<p>The following documentation specifically intended for using PostgreSQL on Sherlock. For more complete documentation about PostgreSQL in general, please see the PostgreSQL documentation.</p>","title":"More documentation"},{"location":"docs/software/using/postgresql/#postgresql-on-sherlock","text":"<p>We don't provide any centralized database service on Sherlock, but we provide a centralized installation of PostgreSQL, and each user is welcome to start their own instance of the database server to fit their jobs' needs.</p> <p>The overall process to run an instance of PostgreSQL on Sherlock would look like this:</p> <ol> <li>configure and initialize your environment so you can start a database    instance under your user account,</li> <li>start the database server,</li> <li>run SQL queries from the same node (via a local socket), or from other nodes    and/or jobs (via the network).</li> </ol>","title":"PostgreSQL on Sherlock"},{"location":"docs/software/using/postgresql/#single-node-access","text":"<p>In that example, the database server and client will run within the same job, on the same compute node.</p>","title":"Single-node access"},{"location":"docs/software/using/postgresql/#preparation","text":"<p>You first need to let PostgreSQL know where to store its database. The commands below only need to be executed once.</p> <p>Assuming you'll want to store your database files in a <code>db/</code> directory in your <code>$SCRATCH</code> folder, you can run the following commands:</p> <pre><code>$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n</code></pre> <p>Once you have your <code>$DB_DIR</code> in place, you need to initialize your database with some internal data that PostgreSQL needs. In the same terminal, run the following commands:</p> <pre><code>$ ml system postgresql\n$ initdb $DB_DIR\n</code></pre>","title":"Preparation"},{"location":"docs/software/using/postgresql/#start-the-server","text":"<p>You can now start the PostgreSQL server. For this, first get an allocation on a compute node, note the hostname of the compute node your job has been allocated, load the <code>postgresql</code> module, and then run the <code>postgresql</code> server:</p> <pre><code>$ srun --pty bash\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ ml system postgresql\n$ export DB_DIR=$SCRATCH/db\n$ postgres -D $DB_DIR\n[...]\n2018-10-09 17:42:08.094 PDT [3841] LOG:  database system is ready to accept connections\n</code></pre> <p>The <code>postgres</code> process will be blocking, meaning it will not give the prompt back for as long as the PostgreSQL server runs.</p>","title":"Start the server"},{"location":"docs/software/using/postgresql/#run-queries","text":"<p>You're now ready to run queries against that PostgreSQL instance, from the same node your job is running on.</p> <p>From another terminal on Sherlock, connect to your job's compute node (here, it's <code>sh-01-01</code>, as shown above), load the <code>postgresql</code> module, and then run the <code>createdb</code> command: it will create a database that you can use as a testbed:</p> <pre><code>$ ssh sh-01-01\n$ ml system postgresql\n$ createdb test_db\n</code></pre> <p>Once this is done, from the same shell, you can run the <code>psql</code> command, which will open the PostgreSQL shell, ready to run your SQL queries:</p> <pre><code>$ psql test_db\npsql (10.5)\nType \"help\" for help.\n\ntest_db=#\n</code></pre> <p>Once you're done with your PostgreSQL instance, you can just terminate your job, and all the processes will be terminated automatically.</p>","title":"Run queries"},{"location":"docs/software/using/postgresql/#multi-node-access","text":"<p>In case you need to run a more persistent instance of PostgreSQL, you can for instance submit a dedicated job to run the server, make it accessible over the network, and run queries from other jobs and/or nodes.</p>","title":"Multi-node access"},{"location":"docs/software/using/postgresql/#enable-network-access","text":"<p>The preparation steps are pretty similar to the single-node case, except the PostgreSQL server instance will be accessed over the network rather than through a local socket.</p>  <p>Network access must be secured</p> <p>When running an networked instance of PostgreSQL, please keep in mind that any user on Sherlock could potentially be able to connect to the TCP ports that <code>postgres</code> runs on, and that proper configuration must be done to prevent unauthrozied access.</p>  <p>Like in the single-node case, you need to start the <code>postgres</code> server process, but with the <code>-i</code> option to enable network connections, and define user access in your <code>$DB_DIR/pg_hba.conf</code> file (see below).</p>","title":"Enable network access"},{"location":"docs/software/using/postgresql/#secure-access","text":"<p>To allow network connections to the database server, a password will need to be defined for the PostgreSQL user.  That will allow this user to connect to the PostgreSQL instance from any node.  Please make sure to replace the <code>my-secure-password</code> string below by the actual password of your choice.</p>  <p>Choose a proper password</p> <p>This password will only be used to access this specific instance of PostgreSQL. Note that anybody knowing that password will be allowed to connect to your PostgreSQL instances and modify data in the tables.</p> <ul> <li>do NOT use <code>my-secure-password</code></li> <li>do NOT use your SUNet ID password</li> </ul>  <p>Once you've chosen your password, you can now start the PostgreSQL server on a compute, as described in the previous section, initialize the database, and set the user password:</p> <pre><code>$ srun --pty bash\n\n$ echo $SLURM_JOB_NODELIST\nsh-01-01\n$ export DB_DIR=$SCRATCH/db\n$ mkdir $DB_DIR\n\n$ ml system postgresql\n$ initdb $DB_DIR\n$ createdb test_db\n\n$ psql -c \"ALTER USER $USER PASSWORD 'my-secure-password';\" test_db\n</code></pre> <p>Then, we need to edit the <code>$DB_DIR/ph_hba.conf</code> file to allow network access for user <code>$USER</code>:</p> <pre><code>$ cat &lt;&lt; EOF &gt; $DB_DIR/pg_hba.conf\nlocal   all             all                                     trust\nhost    all             all             127.0.0.1/32            trust\nhost    all             all             ::1/128                 trust\nhost    all             $USER           samenet                 md5\nEOF\n</code></pre> <p>Once you've done that, you're ready to terminate that interactive job, and start a dedicated PostgreSQL server job.</p> <pre><code>$ pg_ctl stop -D $DB_DIR\n$ logout\n</code></pre>","title":"Secure access"},{"location":"docs/software/using/postgresql/#start-postgresql-in-a-job","text":"<p>You can use the following <code>postgresql.sbatch</code> job as a template:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=postgresql\n#SBATCH --time=8:0:0\n#SBATCH --dependency=singleton\n\nexport DB_DIR=$SCRATCH/db\n\nml system postgresql\n\npostgres -i -D $DB_DIR\n</code></pre> <p>and submit it with:</p> <pre><code>$ sbatch postgresql.sbatch\n</code></pre>  <p>Concurrent instances will lead to data corruption</p> <p>An important thing to keep in mind is that having multiple instances of a PostgreSQL server running at the same time, using the same database files, will certainly lead to catastrophic situations and the corruption of those files.</p> <p>To prevent this from happening, the <code>--dependency=singleton</code> job submission option will make sure that only one instance of that job (based on its name and user) will run at any given time.</p>","title":"Start PostgreSQL in a job"},{"location":"docs/software/using/postgresql/#connect-to-the-running-instance","text":"<p>Now, from any node on Sherlock, whether from a login node, an interactive job, or a batch job, using the <code>mysql</code> CLI or any application binding in any language, you should be able to connect to your running PostgreSQL instance,</p> <p>First, identify the node your job is running on with <code>squeue</code>:</p> <pre><code>$ squeue -u $USER -n postgresql\n             JOBID PARTITION       NAME     USER ST       TIME  NODES NODELIST(REASON)\n          21383445    normal postgresql   kilian  R       0:07      1 sh-01-02\n</code></pre> <p>and then, point your PostgreSQL client to that node:</p> <pre><code>$ ml system postgresql\n$ mpsql -h sh-06-34  test_db\nPassword:\npsql (10.5)\nType \"help\" for help.\n\ntest_db=#\n</code></pre> <p>That's it! You can now run SQL queries from anywhere on Sherlock to your own PostgreSQL instance.</p>","title":"Connect to the running instance"},{"location":"docs/software/using/postgresql/#persistent-db-instances","text":"<p>SQL data is persistent</p> <p>All the data you import in your SQL databases will be persistent across jobs. Meaning that you can run a PostgreSQL server job for the day, import data in its database, stop the job, and resubmit the same PostgreSQL server job the next day: all your data will still be there as long as the location you've chosen for your database (the <code>$DB_DIR</code> defined in the Preparation steps) is on a persistent storage location.</p>  <p>If you need database access for more than the maximum runtime of a job, you can use the instructions provided to define self-resubmitting recurring jobs and submit long-running database instances.</p>","title":"Persistent DB instances"},{"location":"docs/software/using/python/","text":"","title":"Python"},{"location":"docs/software/using/python/#introduction","text":"<p>Python is an interpreted high-level programming language for general-purpose programming. Its design philosophy emphasizes code readability, notably using significant whitespace. It provides constructs that enable clear programming on both small and large scales, which makes it both easy to learn and very well-suited for rapid prototyping.</p>","title":"Introduction"},{"location":"docs/software/using/python/#more-documentation","text":"<p>The following documentation is specifically intended for using Python on Sherlock. For more complete documentation about Python in general, please see the Python documentation.</p>","title":"More documentation"},{"location":"docs/software/using/python/#python-on-sherlock","text":"<p>Sherlock features multiple versions of Python (currently <code>2.7</code> and <code>3.6</code>).</p> <p>Some applications only work with legacy features of version 2.x, while more recent code will require specific version 3.x features.  Modules on Sherlock may only be available in a single flavor (as denoted by their suffix: <code>_py27</code> or <code>_py36</code>, because the application only supports one or the other.</p> <p>You can load either version on Sherlock by doing the following commands:</p> <pre><code>$ ml python/2.7.13\n</code></pre> <p>or</p> <pre><code>$ ml python/3.6.1\n</code></pre>  <p>The Python3 interpreter is <code>python3</code></p> <p>The Python3 executable is named <code>python3</code>, not <code>python</code>. So, once you have the \"python/3.6.1\" module loaded on Sherlock, you will need to use <code>python3</code> to invoke the proper interpreter. <code>python</code> will still refer to the default, older system-level Python installation, and may result in errors when trying to run Python3 code.</p> <p>This is an upstream decision detailled in PEP-394, not something specific to Sherlock.</p>","title":"Python on Sherlock"},{"location":"docs/software/using/python/#using-python","text":"<p>Once your environment is configured (ie. when the Python module is loaded), Python can be started by simply typing <code>python</code> at the shell prompt:</p> <pre><code>$ python\nPython 2.7.13 (default, Apr 27 2017, 14:19:21)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre>","title":"Using Python"},{"location":"docs/software/using/python/#python-packages","text":"<p>The capabilities of Python can be extended with packages developed by third parties. In general, to simplify operations, it is left up to individual users and groups to install these third-party packages in their own directories. However, Sherlock provides tools to help you install the third-party packages that you need.</p> <p>Among many others, the following common Python packages are provided on Sherlock:</p> <ul> <li>NumPy</li> <li>SciPy</li> <li>TensorFlow</li> </ul> <p>Python modules on Sherlock generally follow the naming scheme below:</p> <pre><code>py-&lt;package_name&gt;/version_py&lt;python_version&gt;\n</code></pre> <p>For instance, NumPy modules are:</p> <ul> <li><code>py-numpy/1.14.3_py27</code></li> <li><code>py-numpy/1.14.3_py36</code></li> </ul> <p>You can list all available module versions for a package with <code>ml spider &lt;package_name&gt;</code>. For instance:</p> <pre><code>$ ml spider tensorflow\n-------------------------------------------------------------------------------\n  py-tensorflow:\n-------------------------------------------------------------------------------\n    Description:\n      TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs.\n\n     Versions:\n        py-tensorflow/1.6.0_py27\n        py-tensorflow/1.6.0_py36\n        py-tensorflow/1.7.0_py27\n        py-tensorflow/1.9.0_py27\n        py-tensorflow/1.9.0_py36\n</code></pre>  <p>Dependencies are handled automatically</p> <p>When you decide to use NumPy on Sherlock, you just need to load the <code>py-numpy</code> module of your choice, and the correct Python interpreter will be loaded automatically. No need to load a <code>python</code> module explicitly.</p>","title":"Python packages"},{"location":"docs/software/using/python/#installing-packages","text":"<p>If you need to use a Python package that is not already provided as a module on Sherlock, you can use the <code>pip</code> command. This command takes care of compiling and installing most of Python packages and their dependencies. All of <code>pip</code>'s commands and options are explained in detail in the Pip user guide.</p> <p>A comprehensive index of Python packages can be found at PyPI.</p> <p>To install Python packages with <code>pip</code>, you'll need to use the <code>--user</code> option. This will make sure that those packages are installed in a user-writable location (by default, your <code>$HOME</code> directory). Since your <code>$HOME</code> directory is shared across nodes on Sherlock, you'll only need to install your Python packages once, and they'll be ready to be used on every single node in the cluster.</p> <p>For example:</p> <pre><code>$ pip install --user &lt;package_name&gt;\n</code></pre> <p>For Python 3, use <code>pip3</code>:</p> <pre><code>$ pip3 install --user &lt;package_name&gt;\n</code></pre> <p>Python packages will be installed in <code> $HOME/.local/lib/python&lt;&lt;version&gt;/site-packages</code>, meaning that packages for Python 2.x and Python 3.x will be kept separate. This both means that they won't interfere with each other, but also that if you need to use a package with both Python 2.x and 3.x, you'll need to install it twice, once for each Python version.</p>","title":"Installing packages"},{"location":"docs/software/using/python/#list-installed-packages","text":"<p>You can easily see the list of the Python packages installed in your environment, and their location, with <code>pip list</code>:</p> <pre><code>$ pip list -v\nPackage    Version Location                                                            Installer\n---------- ------- ------------------------------------------------------------------- ---------\npip        18.1    /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nsetuptools 28.8.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nurllib3    1.24    /home/users/kilian/.local/lib/python2.7/site-packages               pip\nvirtualenv 15.1.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\n</code></pre>","title":"List installed packages"},{"location":"docs/software/using/python/#alternative-installation-path","text":"<p>Python paths</p> <p>While theoretically possible, installing Python packages in alternate locations can be tricky, so we recommend trying to stick to the <code>pip install --user</code> way as often as possible. But in case you absolutely need it, we provide some guidelines below.</p>  <p>One common case of needing to install Python packages in alternate locations is to share those packages with a group of users. Here's an example that will show how to install the <code>urllib3</code> Python package in a group-shared location and let users from the group use it without having to install it themselves.</p> <p>First, you need to create a directory to store those packages. We'll put it in <code>$GROUP_HOME</code>:</p> <pre><code>$ mkdir -p $GROUP_HOME/python/\n</code></pre> <p>Then, we load the Python module we need, and we instruct <code>pip</code> to install its packages in the directory we just created:</p> <pre><code>$ ml python/2.7.13\n$ PYTHONUSERBASE=$GROUP_HOME/python pip install --user urllib3\n</code></pre> <p>We still use the <code>--user</code> option, but with <code>PYTHONUSERBASE</code> pointing to a different directory, <code>pip</code> will install packages there.</p> <p>Now, to be able to use that Python module, since it's not been installed in a default directory, you (and all the members of the group who will want to use that module) need to set their <code>PYTHONPATH</code> to include our new shared directory1:</p> <pre><code>$ export PYTHONPATH=$GROUP_HOME/python/lib/python2.7/site-packages:$PYTHONPATH\n</code></pre> <p>And now, the module should be visible:</p> <pre><code>$ pip list -v\nPackage    Version Location                                                            Installer\n---------- ------- ------------------------------------------------------------------- ---------\npip        18.1    /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nsetuptools 28.8.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\nurllib3    1.24    /home/groups/ruthm/python/lib/python2.7/site-packages               pip\nvirtualenv 15.1.0  /share/software/user/open/python/2.7.13/lib/python2.7/site-packages pip\n</code></pre>  <p><code>$PYTHONPATH</code> depends on the Python version</p> <p>The <code>$PYTHONPATH</code> environment variable is dependent on the Python version you're using, so for Python 3.6, it should include <code>$GROUP_HOME/python/lib/python3.6/site-packages</code></p>   <p><code>$PATH</code> may also need to be updated</p> <p>Some Python package sometimes also install executable scripts. To make them easily accessible in your environment, you may also want to modify your <code>$PATH</code> to include their installation directory.</p> <p>For instance, if you installed Python packages in <code>$GROUP_HOME/python</code>: <pre><code>$ export PATH=$GROUP_HOME/python/bin:$PATH\n</code></pre></p>","title":"Alternative installation path"},{"location":"docs/software/using/python/#installing-from-github","text":"<p><code>pip</code> also supports installing packages from a variety of sources, including GitHub repositories.</p> <p>For instance, to install HTTPie, you can do:</p> <pre><code>$ pip install --user git+git://github.com/jkbr/httpie.git\n</code></pre>","title":"Installing from GitHub"},{"location":"docs/software/using/python/#installing-from-a-requirements-file","text":"<p><code>pip</code> allows installing a list of packages listed in a file, which can be pretty convenient to install several dependencies at once.</p> <p>In order to do this, create a text file called <code>requirements.txt</code> and place each package you would like to install on its own line:</p> requirements.txt   <pre><code>numpy\nscikit-learn\nkeras\ntensorflow\n</code></pre>    <p>You can now install your modules like so:</p> <pre><code>$ ml python\n$ pip install--user -r requirements.txt\n</code></pre>","title":"Installing from a requirements file"},{"location":"docs/software/using/python/#upgrading-packages","text":"<p><code>pip</code> can update already installed packages with the following command:</p> <pre><code>$ pip install --user --upgrade &lt;package_name&gt;\n</code></pre> <p>Upgrading packages also works with <code>requirements.txt</code> files:</p> <pre><code>$ pip install --user --upgrade -r requirements.txt\n</code></pre>","title":"Upgrading packages"},{"location":"docs/software/using/python/#uninstalling-packages","text":"<p>To uninstall a Python package, you can use the <code>pip uninstall</code> command (note that it doesn't take any <code>--user</code> option):</p> <pre><code>$ pip uninstall &lt;package_name&gt;\n$ pip uninstall -r requirements.txt\n</code></pre>   <ol> <li> <p>This line can also be added to a user's <code>~/.profile</code> file, for a   more permanent setting.\u00a0\u21a9</p> </li> </ol>","title":"Uninstalling packages"},{"location":"docs/software/using/rclone/","text":"","title":"Rclone"},{"location":"docs/software/using/rclone/#introduction","text":"<p>If you need to sync files between cloud storage to Sherlock, rclone is a command line program that can help. You can easily use it to transfer files from a cloud storage provider to Sherlock, or vice versa.  The following tutorial is provided by a member of the Stanford community, and walks through transferring files from Box to Sherlock on a Mac.</p>","title":"Introduction"},{"location":"docs/software/using/rclone/#setup","text":"","title":"Setup"},{"location":"docs/software/using/rclone/#connection","text":"<p>If you haven't done so already, open up a terminal and shell to sherlock:</p> <pre><code>$ ssh &lt;sunetid&gt;@login.sherlock.stanford.edu\n</code></pre> <p>You then will want to go to a location that has enough room to save the files. Since <code>$HOME</code> has a smaller limit (and you can lock yourself out if it fills up) let's go to the <code>$SCRATCH</code> space:</p> <pre><code>$ cd $SCRATCH\n</code></pre> <p>Since we don't want to run anything computationally intensive on a login node, let's request an interactive session. You can either ask for one with a particular time and partition on the fly:</p> <pre><code>$ srun --partition normal --time 1:00:00 --pty bash\n</code></pre> <p>or ask for a development node for 1 hour:</p> <pre><code>$ sdev\n</code></pre>","title":"Connection"},{"location":"docs/software/using/rclone/#module","text":"<p>Rclone is readily available on Sherlock, but the corresponding module needs to be explicitely loaded to be made available in your environment:</p> <pre><code>$ ml load system rclone\n</code></pre>","title":"Module"},{"location":"docs/software/using/rclone/#configuration","text":"<p>We can then configure it as follows:</p> <pre><code>$ rclone config\n</code></pre> <p>You'll notice that it's going to store a configuration file in your <code>$HOME</code> directory:</p> <pre><code>2019/07/09 13:03:56 NOTICE: Config file \"/home/users/vsochat/.config/rclone/rclone.conf\" not found - using defaults\n</code></pre>","title":"Configuration"},{"location":"docs/software/using/rclone/#remotes","text":"<p>It will first tell you that there are no \"remotes\" (cloud endpoints that you connect to) found, and you can press \"n\" to make a new one:</p> <pre><code>No remotes found - make a new one\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\n</code></pre> <p>Next, it asks for a meaningful name. It's recommended to use some combination to remind your future self that the endpoint is intended to be from Sherlock, and to your cloud provider. For example, I might do:</p> <pre><code>$ name&gt; VanessaSherlockToBox\n</code></pre> <p>The next choice is the cloud endpoint itself. This is where you would select the cloud provider that has the files that you want to connect to. There are many to choose from! You would want to select the number that corresponds with your choice. For example, I'd choose 5 or type \"box\" to select Box:</p> <pre><code>Type of storage to configure.\nEnter a string value. Press Enter for the default (\"\").\nChoose a number from below, or type in your own value\n 1 / Alias for a existing remote\n   \\ \"alias\"\n 2 / Amazon Drive\n   \\ \"amazon cloud drive\"\n 3 / Amazon S3 Compliant Storage Providers (AWS, Ceph, Dreamhost, IBM COS, Minio)\n   \\ \"s3\"\n 4 / Backblaze B2\n   \\ \"b2\"\n 5 / Box\n   \\ \"box\"\n 6 / Cache a remote\n   \\ \"cache\"\n 7 / Dropbox\n   \\ \"dropbox\"\n 8 / Encrypt/Decrypt a remote\n   \\ \"crypt\"\n 9 / FTP Connection\n   \\ \"ftp\"\n10 / Google Cloud Storage (this is not Google Drive)\n   \\ \"google cloud storage\"\n11 / Google Drive\n   \\ \"drive\"\n12 / Hubic\n   \\ \"hubic\"\n13 / JottaCloud\n   \\ \"jottacloud\"\n14 / Local Disk\n   \\ \"local\"\n15 / Mega\n   \\ \"mega\"\n16 / Microsoft Azure Blob Storage\n   \\ \"azureblob\"\n17 / Microsoft OneDrive\n   \\ \"onedrive\"\n18 / OpenDrive\n   \\ \"opendrive\"\n19 / Openstack Swift (Rackspace Cloud Files, Memset Memstore, OVH)\n   \\ \"swift\"\n20 / Pcloud\n   \\ \"pcloud\"\n21 / QingCloud Object Storage\n   \\ \"qingstor\"\n22 / SSH/SFTP Connection\n   \\ \"sftp\"\n23 / Webdav\n   \\ \"webdav\"\n24 / Yandex Disk\n   \\ \"yandex\"\n25 / http Connection\n   \\ \"http\"\nStorage&gt; 5\n</code></pre> <p>For client id and client secret, we will leave it blank (press ENTER for each) to designate that we want to enter it manually when we run it, as opposed to saving our credentials somewhere.</p> <pre><code>Box App Client Id.\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nclient_id&gt;\nBox App Client Secret\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nclient_secret&gt;\n</code></pre> <p>Finally, it will ask you if you want to edit the Advanced config. You can say no (n).</p> <pre><code>Edit advanced config? (y/n)\ny) Yes\nn) No\ny/n&gt; n\n</code></pre> <p>And finally, since you are working on a remote and headless machine (Sherlock), you should say no to the next answer.</p> <pre><code>Remote config\nUse auto config?\n * Say Y if not sure\n * Say N if you are working on a remote or headless machine\ny) Yes\nn) No\ny/n&gt; n\n</code></pre>","title":"Remotes"},{"location":"docs/software/using/rclone/#authentication","text":"<p>The next part is important, because we need to open a separate terminal (one where we have a web browser available) to enter result asked for here.</p> <p>If you have a Mac, you can select Shell -&gt; New Window -&gt; New Window with Profile. If you have another flavor of Linux (or Windows) then you will need to install rclone locally and then issue this command:</p> <pre><code>$ rclone authorize \"box\"\n</code></pre> <p>A website will open for you to log in with your cloud provider (e.g., Box), and after login, it will tell you to return to your terminal:</p> <pre><code>Success!\n\n\nAll done. Please go back to rclone.\n</code></pre> <p>Back in the (second terminal, not the one on Sherlock) you will see a message (that you might have previously missed) about the browser opening, waiting for a code, and then you will get the code (replaced below with xxxxxxxx):</p> <pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth\nLog in and authorize rclone for access\nWaiting for code...\nGot code\nPaste the following into your remote machine ---&gt;\n{\"access_token\":\"xxxxxxxxxxxxxx\",\"token_type\":\"bearer\",\"refresh_token\":\"xxxxxxxxxx\",\"expiry\":\"2019-xx-xxxxxxxx\"}\n(---End paste\n</code></pre> <p>You need to copy the entire thing between the two brackets \"{}\" back into the first terminal running on Sherlock, which will be showing this:</p> <pre><code>For this to work, you will need rclone available on a machine that has a web browser available.\nExecute the following on your machine:\n rclone authorize \"box\"\nThen paste the result below:\nresult&gt;\n</code></pre> <p>After you paste, it will then ask you if it looks ok, and you can type Y for yes.</p> <pre><code>[VanessaSherlockToBox]\ntype = box\ntoken = {xxxxxxxxxxxxxxxxxxx}\n--------------------\ny) Yes this is OK\ne) Edit this remote\nd) Delete this remote\n</code></pre> <p>And close up with a listing of your current remotes. You can quit Q after this, because next we will test our setup.</p> <pre><code>Current remotes:\n\nName                 Type\n====                 ====\nVanessaSherlockToBox box\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n</code></pre>","title":"Authentication"},{"location":"docs/software/using/rclone/#testing","text":"<p>Did it work? Let's test listing files for our remote to see (filenames below are made up).</p> <pre><code>$ rclone lsd VanessaSherlockToBox: --max-depth 1\n          -1 2018-08-09 09:52:01        -1 pancakes\n          -1 2019-03-13 23:33:03        -1 miracles\n          -1 2019-03-06 09:42:39        -1 alaska\n          -1 2018-02-06 02:37:40        -1 share\n</code></pre> <p>Next, let's copy a file to Sherlock.</p> <pre><code># rclone copy &lt;remote&gt;:&lt;cloud storage path&gt;  &lt;sherlock path&gt;\n$ rclone copy VanessaSherlockToBox:pancakes  /scratch/users/vsochat/pancakes\n</code></pre> <p>There you go! If you want to interactively browse files, you can use the File Manager on the Sherlock OnDemand interface.</p>","title":"Testing"},{"location":"docs/software/using/singularity/","text":"<p>Singularity is a tool for running containers on HPC systems, similar to Docker.</p>","title":"Singularity"},{"location":"docs/software/using/singularity/#introduction","text":"<p>Containers are a solution to the problem of how to get software to run reliably when moved from one computing environment to another. They also resolve installation problems by packaging all the dependencies of an application within a self-sustainable image, a.k.a a container.</p>  <p>What's a container?</p> <p>Put simply, a container consists of an entire runtime environment: an application, plus all its dependencies, libraries and other binaries, and configuration files needed to run it, bundled into one package. By containerizing the application platform and its dependencies, differences in OS distributions and underlying infrastructure are abstracted away.</p>","title":"Introduction"},{"location":"docs/software/using/singularity/#why-not-docker","text":"<p>Docker has long been the reference and the most popular container framework in DevOps and Enterprise IT environments, so why not use Docker on Sherlock? Well, for a variety of technical reasons, mostly related to security.</p> <p>Docker has never been designed nor developed to run in multi-tenants environments, and even less on HPC clusters. Specifically:</p> <ul> <li>Docker requires a daemon running as <code>root</code> on all of the compute nodes, which   has serious security implications,</li> <li>all authenticated actions (such as <code>login</code>, <code>push</code> ...) are also executed as   <code>root</code>, meaning that multiple users can't use those functions on the same   node,</li> <li>Docker uses cgroups to isolate containers, as does the Slurm scheduler, which   uses cgroups to allocate resources to jobs and enforce limits. Those uses are   unfortunately conflicting.</li> <li>but most importantly, allowing users to run Docker containers will give   them <code>root</code> privileges inside that container, which will in turn let them   access any of the clusters' filesystems as <code>root</code>. This opens the door to   user impersonation, inappropriate file tampering or stealing, and is   obviously not something that can be allowed on a shared resource.</li> </ul> <p>That last point is certainly the single most important reason why we won't use Docker on Sherlock.</p>","title":"Why not Docker?"},{"location":"docs/software/using/singularity/#why-singularity","text":"<p>Singularity is Docker for HPC systems</p> <p>Singularity allows running Docker containers natively, and is a perfect replacement for Docker on HPC systems such as Sherlock. That means that existing Docker container can be directly imported and natively run with SIngularity.</p>  <p>Despite Docker's shortcomings on HPC systems, the appeal of containers for scientific computing is undeniable, which is why we provide Singularity on Sherlock. Singularity is an alternative container framework, especially designed to run scientific applications on HPC clusters.</p> <p>Singularity provides the same functionalities as Docker, without any of the drawbacks listed above. Using a completely different implementation, it doesn't require any privilege to run containers, and allow direct interaction with existing Docker containers.</p> <p>The main motivation to use Singularity over Docker is the fact that it's been developed with HPC systems in mind, to solve those specific problems:</p> <ul> <li>security: a user in the container is the same user as the one running the   container, so no privilege escalation possible,</li> <li>ease of deployment: no daemon running as root on each node, a container is     simply an executable,</li> <li>no need to mount filesystems or do bind mappings to access devices,</li> <li>ability to run MPI jobs based on containers,</li> <li>and more...</li> </ul>","title":"Why Singularity?"},{"location":"docs/software/using/singularity/#more-documentation","text":"<p>The following documentation specifically intended for using Singularity on Sherlock. For more complete documentation about building and running containers with Singularity, please see the Singularity documentation.</p>","title":"More documentation"},{"location":"docs/software/using/singularity/#singularity-on-sherlock","text":"<p>As announced during the SC'18 Supercomputing Conference, Singularity is an integral part of the Sherlock cluster, and Singularity commands can be executed natively on any login or compute node, without the need to load any additional module.</p>","title":"Singularity on Sherlock"},{"location":"docs/software/using/singularity/#importing-containers","text":"<p>Pre-built containers can be obtained from a variety of sources. For instance:</p> <ul> <li>DockerHub contains containers for various software   packages, which can be directly used with   Singularity,</li> <li>SingularityHub is a registry for scientific   linux containers,</li> <li>the NVIDIA GPU Cloud registry for   GPU-optimized containers,</li> <li>many individual projects contain specific instructions for installation via   Docker and/or Singularity, and may provide pre-built images in other   locations.</li> </ul> <p>To illustrate how Singularity can import and run Docker containers, here's an example how to install and run the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but Singularity makes it very easy.</p>  <p>Interactive or batch usage</p> <p>This example shows how to use Singularity interactively, but Singularity containers can be run in batch jobs as well.</p>  <p>The first step is to request an interactive shell, and to load the singularity module. Singularity images can be pulled directly from the compute nodes, and Singularity uses multiple CPU cores when assembling the image, so requesting multiple cores in your job can make the pull operation faster:</p> <pre><code>$ srun -c 4 --pty bash\n</code></pre> <p>We recommend storing Singularity images in <code>$GROUP_HOME</code>, as container images can take significant space in your <code>$HOME</code> directory.</p> <pre><code>$ mkdir -p $GROUP_HOME/$USER/simg\n$ cd $GROUP_HOME/$USER/simg\n</code></pre> <p>Then, the OpenFOAM container could be pulled directly from DockerHub by Singularity. This can take a moment to complete:</p> <pre><code>$ singularity pull docker://openfoam/openfoam6-paraview54\nDocker image path: index.docker.io/openfoam/openfoam6-paraview54:latest\nCache folder set to /scratch/users/kilian/.singularity/docker\nImporting: base Singularity environment\nExploding layer: sha256:1be7f2b886e89a58e59c4e685fcc5905a26ddef3201f290b96f1eff7d778e122.tar.gz\n[...]\nBuilding Singularity image...\nSingularity container built: ./openfoam6-paraview54.simg\nCleaning up...\nDone. Container is at: ./openfoam6-paraview54.simg\n</code></pre>","title":"Importing containers"},{"location":"docs/software/using/singularity/#running-containers","text":"<p>Once the image is downloaded, you are ready to run OpenFOAM from the container. The <code>singularity shell</code> command can be used to start the container, and run a shell within that image:</p> <p>By default on Sherlock, all the filesystems that are available on the compute node will also be available in the container. If you want to start your shell in a specific directory, you can use the <code>--pwd /path/</code> option. For instance, we'll create a <code>/tmp/openfoam_test/</code> directory to store our tests results (that will be wiped out at the end of the job), and start the container shell there:</p> <pre><code>$ mkdir /tmp/openfoam_test\n$ singularity shell --pwd /tmp/openfoam_test openfoam6-paraview54.simg\nSingularity: Invoking an interactive shell within container...\nSingularity openfoam6-paraview54.simg:/tmp/openfoam_test&gt;\n</code></pre> <p>You're now in the container, as denoted by the shell prompt (<code>Singularity[...].simg:[path]&gt;</code>), which is different from the prompt displayed on the compute node (which usually looks like <code>[login]@[compute_node] [path]$</code>.</p> <p>OpenFOAM provides a convenience script that can be sourced to make OpenFOAM commands directly accessible and set a few useful environment variables:</p> <pre><code>&gt; source /opt/openfoam6/etc/bashrc\n</code></pre> <p>Now, we can run a simple example using OpenFOAM:</p> <pre><code>&gt; cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily .\n&gt; cd pitzDaily\n&gt; blockMesh\n[...]\nEnd\n\n&gt; simpleFoam\n/*---------------------------------------------------------------------------*\\\n  =========                 |\n  \\\\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox\n   \\\\    /   O peration     | Website:  https://openfoam.org\n    \\\\  /    A nd           | Version:  6\n     \\\\/     M anipulation  |\n\\*---------------------------------------------------------------------------*/\nBuild  : 6-1a0c91b3baa8\nExec   : simpleFoam\nDate   : Oct 05 2018\nTime   : 23:37:30\nHost   : \"sh01-06n33.int\"\nPID    : 14670\nI/O    : uncollated\nCase   : /tmp/openfoam_test/pitzDaily\nnProcs : 1\nsigFpe : Enabling floating point exception trapping (FOAM_SIGFPE).\nfileModificationChecking : Monitoring run-time modified files using timeStampMaster (fileModificationSkew 10)\nallowSystemOperations : Allowing user-supplied system call operations\n\n// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //\nCreate time\n[...]\nSIMPLE solution converged in 288 iterations\n\nstreamLine streamlines write:\n    seeded 10 particles\n    Tracks:10\n    Total samples:11980\n    Writing data to \"/tmp/openfoam_test/pitzDaily/postProcessing/sets/streamlines/288\"\nEnd\n\n&gt;\n</code></pre> <p>When the simulation is done, you can exit the container with:</p> <pre><code>&gt; exit\n</code></pre> <p>Because the container can see all the compute node's filesystems, the simulation output will be available in <code>/tmp/openfoam_test</code> after you exit the container:</p> <pre><code>$ ls /tmp/openfoam_test/pitzDaily/postProcessing/\nsets\n</code></pre>","title":"Running containers"},{"location":"docs/software/using/singularity/#gpu-enabled-containers","text":"<p>Sherlock also supports the use of container images provided by NVIDIA in the NVIDIA GPU Cloud (NGC). This registry provides GPU-accelerated containers for the most popular HPC and deep-learning scientific applications.</p>  <p>GPU support</p> <p>Containers provided on NGC are only supported on Pascal and Volta architectures (TITAN Xp, Tesla P40, P100 or V100). For GPUs from the previous generations (GTX TITAN Black/X, Tesla K20/K80), things may or may not work.</p> <p>We recommend making sure to select a supported GPU generation by adding the following directive to your batch script when submitting a job to run GPU-enabled containers from NGC: <pre><code>#SBATCH -C \"GPU_GEN:PSC|GPU_GEN:VLT\"\n</code></pre></p>","title":"GPU-enabled containers"},{"location":"docs/software/using/singularity/#pulling-ngc-images","text":"<p>As before, we start by requesting an interactive shell with multiple CPU cores, loading the Singularity module and moving the directory where we'll save those images:</p> <pre><code>$ srun -c 4 --pty bash\n$ cd $GROUP_HOME/simg\n</code></pre>  <p>A GPU is not required for pulling GPU-enabled containers</p> <p>GPU-enabled containers can be pulled on any node, including nodes without a GPU. But their execution requires a GPU and thus, they need to be executed within a GPU job. See the GPU job section for more information.</p>  <p>To be able to pull an image from NGC, authentication credentials must be set. Users need to register and create an NGC API key, complete details could be found in the NGC Getting Started Guide.</p> <p>You can then set the following environment variable to allow Singularity to authenticate with NGC:</p> <pre><code>$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken'\n$ export SINGULARITY_DOCKER_PASSWORD=&lt;NVIDIA NGC API key&gt;\n</code></pre>  <p>Note</p> <p>The <code>SINGULARITY_DOCKER_USERNAME</code> environment variable must be set to the literal <code>$oauthtoken</code> string, for every user. It should not be replaced by anything else. Only the API key is specific to each user.</p>  <p>Once credentials are set in the environment, container images can be pulled from the NGC registry normally.</p> <p>The general form of the Singularity command used to pull NGC containers is: <code>$ singularity pull docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;</code></p> <p>For example to pull the NAMD NGC container tagged with version <code>2.12-171025</code> the corresponding command would be:</p> <pre><code>$ singularity pull docker://nvcr.io/hpc/namd:2.12-171025\n</code></pre> <p>After this command has finished, we'll have a Singularity image file in the current directory, named <code>namd-2.12-171025.simg</code>.</p>","title":"Pulling NGC images"},{"location":"docs/software/using/singularity/#running-ngc-containers","text":"<p>Instructions about running NGC containers are provided on the NGC website, under each application:</p> <p></p> <p>Each application comes with specific running instructions, so we recommend to follow the container's particular guidelines before running it with Singularity.</p>  <p>Containers that lack Singularity documentation have not been tested with Singularity.</p>  <p>Since all NGC containers are optimized for GPU acceleration, they will always be executed with the <code>--nv</code> Singularity option, to enable GPU support within the container.</p> <p>We also need to submit a job requesting a GPU to run GPU-enabled containers.  For instance:</p> <pre><code>$ srun -p gpu -c 4 --gres gpu:1 --pty bash\n</code></pre> <p>This will start an interactive shell on a GPU node, with 4 CPU cores and 1 GPU.</p> <p>The NAMD container that was pulled just before can now be started with the following commands. We start by creating a temporary directory to hold the execution results, and start the container using this as the current directory:</p> <pre><code>$ mkdir /tmp/namd_test\n$ singularity shell --nv --pwd /tmp/namd_test $GROUP_HOME/simg/namd-2.12-171025.simg\nSingularity: Invoking an interactive shell within container...\n\nSingularity namd-2.12-171025.simg:/tmp/namd_test&gt;\n</code></pre> <p>From there, we can run a NAMD test to verify that everything is working as expected.</p> <pre><code>&gt; cp -r /workspace/examples .\n&gt; /opt/namd/namd-multicore +p4 +idlepoll examples/apoa1/apoa1.namd\nCharm++: standalone mode (not using charmrun)\nCharm++&gt; Running in Multicore mode:  4 threads\nCharm++&gt; Using recursive bisection (scheme 3) for topology aware partitions\nConverse/Charm++ Commit ID: v6.8.2\n[...]\nInfo: Built with CUDA version 9000\nDid not find +devices i,j,k,... argument, using all\nPe 1 physical rank 1 will use CUDA device of pe 2\nPe 3 physical rank 3 will use CUDA device of pe 2\nPe 0 physical rank 0 will use CUDA device of pe 2\nPe 2 physical rank 2 binding to CUDA device 0 on sh02-14n13.int: 'TITAN Xp'  Mem: 12196MB  Rev: 6.1\nInfo: NAMD 2.12 for Linux-x86_64-multicore-CUDA\n[...]\nInfo: SIMULATION PARAMETERS:\nInfo: TIMESTEP               1\n[...]\nENERGY:    2000     20247.5090     20325.4554      5719.0088       183.9328        -340639.3103     25366.3986         0.0000         0.0000     46364.9951        -222432.0107       168.6631   -268797.0057   -222054.5175       168.8733          -1129.9509     -1799.6459    921491.4634     -2007.8380     -2007.4145\n\nWRITING EXTENDED SYSTEM TO OUTPUT FILE AT STEP 2000\nWRITING COORDINATES TO OUTPUT FILE AT STEP 2000\nThe last position output (seq=-2) takes 0.001 seconds, 559.844 MB of memory in use\nWRITING VELOCITIES TO OUTPUT FILE AT STEP 2000\nThe last velocity output (seq=-2) takes 0.001 seconds, 559.844 MB of memory in use\n====================================================\n\nWallClock: 17.593451  CPUTime: 17.497925  Memory: 559.843750 MB\n[Partition 0][Node 0] End of program\n</code></pre> <p>The simulation should take a few seconds to run. You can verify that it correctly executed on a GPU in the output above. When it's done, you can exit the container with:</p> <pre><code>&gt; exit\n</code></pre> <p>Because the container can see all the compute node's filesystems, the simulation output will be available in <code>/tmp/named_test</code> after you exit the container:</p> <pre><code>$ cd /tmp/namd_test/examples/apoa1/\n$ ls apoa1-out*\napoa1-out.coor  apoa1-out.vel  apoa1-out.xsc\n</code></pre>","title":"Running NGC containers"},{"location":"docs/software/using/singularity/#building-your-own-containers","text":"<p>Building Singularity containers requires <code>root</code> privileges, and as such, cannot be done on Sherlock directly.</p> <p>If you need to modify existing containers or build your own from scratch, The recommended workflow is to prepare and build your containers on your local Linux machine (it could either be a workstation, a laptop or a virtual machine), transfer the resulting container image to Sherlock, and run it there.</p> <p>For complete details about how to build Singularity containers, please refer to the Singularity documentation.</p>   <ol> <li> <p>For more information about using modules on Sherlock,   please see the software modules documentation.\u00a0\u21a9</p> </li> </ol>","title":"Building your own containers"},{"location":"docs/software/using/spark/","text":"","title":"Spark"},{"location":"docs/software/using/spark/#introduction","text":"<p>Apache Spark\u2122 is a general engine for large-scale data processing.  This document gives a quick introduction how to get a first test program in Spark running on Sherlock.</p>","title":"Introduction"},{"location":"docs/software/using/spark/#more-documentation","text":"<p>The following documentation specifically intended for using Spark on Sherlock. For more complete documentation about Spark in general, please see the Apache Spark documentation.</p>","title":"More documentation"},{"location":"docs/software/using/spark/#spark-on-sherlock","text":"<p>Running Apache Spark on Sherlock is a bit different from using a traditional Spark/Hadoop cluster in that it requires some level of integration with the scheduler.  In a sense, the computing resources (memory and CPU) need to be allocated twice. First, sufficient resources for the Spark application need to be allocated via Slurm ; and secondly, <code>spark-submit</code> resource allocation flags need to be properly specified.</p> <p>In order to use Spark, three steps have to be kept in mind when submitting a job to the queuing system:</p> <ol> <li>a new Spark cluster has to be started on the allocated nodes</li> <li>once the Spark cluster is up and running, Spark jobs have to be submitted to    the cluster</li> <li>after all Spark jobs have finished running, the cluster has to be shut down</li> </ol> <p>The following scripts show how to implement these three steps, and use the Pi Monte-Carlo calculation as an example.</p>","title":"Spark on Sherlock"},{"location":"docs/software/using/spark/#single-node-job","text":"<p>In this example, all the Spark processes run on the same compute node, which makes for a fairly simply sbatch script. The following example will start a 8-core job on a single node, and run a Spark task within that allocation:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=spark_singlenode\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=8\n#SBATCH --time=10\n\nmodule load spark\n\n# This syntax tells spark to use all cpu cores on the node.\nexport MASTER=\"local[*]\"\n\n# This is a Scala example\nrun-example SparkPi 1000\n\n# This is a Python example.\nspark-submit --master $MASTER $SPARK_HOME/examples/src/main/python/pi.py 1000\n</code></pre>","title":"Single-node job"},{"location":"docs/software/using/spark/#multi-node-job","text":"<p>To start a Spark cluster and run a task on multiple nodes, more preliminary steps are necessary. Here's an example script that will span 2 nodes, start 2 Spark workers on each node, and allow each worker to use 8 cores:</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --mem-per-cpu=4G\n#SBATCH --cpus-per-task=8\n#SBATCH --ntasks-per-node=2\n#SBATCH --output=sparkjob-%j.out\n\n## --------------------------------------\n## 0. Preparation\n## --------------------------------------\n\n# load the Spark module\nmodule load spark\n\n# identify the Spark cluster with the Slurm jobid\nexport SPARK_IDENT_STRING=$SLURM_JOBID\n\n# prepare directories\nexport SPARK_WORKER_DIR=${SPARK_WORKER_DIR:-$HOME/.spark/worker}\nexport SPARK_LOG_DIR=${SPARK_LOG_DIR:-$HOME/.spark/logs}\nexport SPARK_LOCAL_DIRS=${SPARK_LOCAL_DIRS:-/tmp/spark}\nmkdir -p $SPARK_LOG_DIR $SPARK_WORKER_DIR\n\n## --------------------------------------\n## 1. Start the Spark cluster master\n## --------------------------------------\n\nstart-master.sh\nsleep 1\nMASTER_URL=$(grep -Po '(?=spark://).*' \\\n             $SPARK_LOG_DIR/spark-${SPARK_IDENT_STRING}-org.*master*.out)\n\n## --------------------------------------\n## 2. Start the Spark cluster workers\n## --------------------------------------\n\n# get the resource details from the Slurm job\nexport SPARK_WORKER_CORES=${SLURM_CPUS_PER_TASK:-1}\nexport SPARK_MEM=$(( ${SLURM_MEM_PER_CPU:-4096} * ${SLURM_CPUS_PER_TASK:-1} ))M\nexport SPARK_DAEMON_MEMORY=$SPARK_MEM\nexport SPARK_WORKER_MEMORY=$SPARK_MEM\nexport SPARK_EXECUTOR_MEMORY=$SPARK_MEM\n\n# start the workers on each node allocated to the tjob\nexport SPARK_NO_DAEMONIZE=1\nsrun  --output=$SPARK_LOG_DIR/spark-%j-workers.out --label \\\n      start-slave.sh ${MASTER_URL} &amp;\n\n## --------------------------------------\n## 3. Submit a task to the Spark cluster\n## --------------------------------------\n\nspark-submit --master ${MASTER_URL} \\\n             --total-executor-cores $((SLURM_NTASKS * SLURM_CPUS_PER_TASK)) \\\n             $SPARK_HOME/examples/src/main/python/pi.py 10000\n\n## --------------------------------------\n## 4. Clean up\n## --------------------------------------\n\n# stop the workers\nscancel ${SLURM_JOBID}.0\n\n# stop the master\nstop-master.sh\n</code></pre>","title":"Multi-node job"},{"location":"docs/storage/","text":"<p>Sherlock provides access to several file systems, each with distinct storage characteristics. Each user and PI group get access to a set of pre-defined directories in these file systems to store their data.</p>  <p>Sherlock is a compute cluster, not a storage system</p> <p>Sherlock's storage resources are limited and are shared among many users. They are meant to store data and code associated with projects for which you are using Sherlock's computational resources. This space is for work actively being computed on with Sherlock, and should not be used as a target for backups from other systems.</p> <p>If you're looking for a long-term storage solution for research data, SRCC offers the Oak storage system, which is specifically intended for this usage.</p>  <p>Those file systems are shared with other users, and are subject to quota limits and for some of them, purge policies (time-residency limits).</p>","title":"Storage on Sherlock"},{"location":"docs/storage/#filesystem-overview","text":"","title":"Filesystem overview"},{"location":"docs/storage/#features-and-purpose","text":"Name Type Backups / Snapshots Performance Purpose Cost     <code>$HOME</code>, <code>$GROUP_HOME</code> NFS  /  low small, important files (source code, executables, configuration files...) free   <code>$SCRATCH</code>, <code>$GROUP_SCRATCH</code> Lustre  /  high bandwidth large, temporary files (checkpoints, raw application output...) free   <code>$L_SCRATCH</code> local SSD  /  low latency, high IOPS job specific output requiring high IOPS free   <code>$OAK</code> Lustre option /  moderate long term storage of research data volume-based1","title":"Features and purpose"},{"location":"docs/storage/#access-scope","text":"Name Scope Access sharing level     <code>$HOME</code> cluster user   <code>$GROUP_HOME</code> cluster group   <code>$SCRATCH</code> cluster user   <code>$GROUP_SCRATCH</code> cluster group   <code>$L_SCRATCH</code> compute node user   <code>$OAK</code> cluster (optional, purchase required) group    <p>Group storage locations are typically shared between all the members of the same PI group. User locations are only accessible by the user.</p>","title":"Access scope"},{"location":"docs/storage/#quotas-and-limits","text":"<p>Info</p> <p>Quotas are applied on both volume (the amount of data stored in bytes) and inode: an inode (index node) is a data structure in a Unix-style file system that describes a file-system object such as a file or a directory. In practice, each filesystem entry (file, directory, link) counts as an inode.</p>     Name Quota type Volume quota Inode quota Retention     <code>$HOME</code> directory 15 GB n/a    <code>$GROUP_HOME</code> directory 1 TB n/a    <code>$SCRATCH</code> directory 100 TB 20 million time limited   <code>$GROUP_SCRATCH</code> directory 100 TB 20 million time limited   <code>$L_SCRATCH</code> n/a n/a n/a job lifetime   <code>$OAK</code> group amount purchased function of the volume purchased     <p>Quota types:</p> <ul> <li>directory: based on files location and account for all the files   that are in a given directory.</li> <li>group: based on files ownership and account for all the files that   belong to a given group.</li> </ul> <p>Retention types:</p> <ul> <li>: files are kept as long as the user account exists on Sherlock.</li> <li>time limited: files are kept for a fixed length of time after they've   been last modified. Once the limit is reached, files expire and are   automatically deleted.</li> <li>job lifetime: files are only kept for the duration of the job and are   automatically purged when the job ends.</li> </ul>","title":"Quotas and limits"},{"location":"docs/storage/#checking-quotas","text":"<p>To check your quota usage on the different filesystems you have access to, you can use the <code>sh_quota</code> command:</p> <pre><code>$ sh_quota\n+---------------------------------------------------------------------------+\n| Disk usage for user kilian (group: ruthm)                                 |\n+---------------------------------------------------------------------------+\n|   Filesystem |  volume /   limit                  | inodes /  limit       |\n+---------------------------------------------------------------------------+\n          HOME |   9.4GB /  15.0GB [||||||     62%] |      - /      - (  -%)\n    GROUP_HOME | 562.6GB /   1.0TB [|||||      56%] |      - /      - (  -%)\n       SCRATCH |  65.0GB / 100.0TB [            0%] | 143.8K /  20.0M (  0%)\n GROUP_SCRATCH | 172.2GB / 100.0TB [            0%] |  53.4K /  20.0M (  0%)\n           OAK |  30.8TB / 240.0TB [|          12%] |   6.6M /  36.0M ( 18%)\n+---------------------------------------------------------------------------+\n</code></pre> <p>Several options are provided to allow listing quotas for a specific filesystem only, or in the context of a different group (for users who are members of several PI groups). Please see the <code>sh_quota</code> usage information for details:</p> <pre><code>$ sh_quota -h\nsh_quota: display user and group quota information for all accessible filesystems.\n\nUsage: sh_quota [OPTIONS]\n    Optional arguments:\n        -f FILESYSTEM   only display quota information for FILESYSTEM.\n                        For instance: \"-f $HOME\"\n        -g GROUP        for users with multiple group memberships, display\n                        group quotas in the context of that group\n        -n              don't display headers\n        -j              JSON output (implies -n)\n</code></pre>","title":"Checking quotas"},{"location":"docs/storage/#examples","text":"<p>For instance, to only display your quota usage on <code>$HOME</code>:</p> <pre><code>$ sh_quota -f HOME\n</code></pre> <p>If you belong to multiple groups, you can display the group quotas for your secondary groups with:</p> <pre><code>$ sh_quota -g &lt;group_name&gt;\n</code></pre> <p>And finally, for great output control, an option to display quota usage in JSON is provided via the <code>-j</code> option:</p> <pre><code>$ sh_quota -f SCRATCH -j\n{\n  \"SCRATCH\": {\n    \"quotas\": {\n      \"type\": \"user\",\n      \"blocks\": {\n        \"usage\": \"47476660\",\n        \"limit\": \"21474836480\"\n      },\n      \"inodes\": {\n        \"usage\": \"97794\",\n        \"limit\": \"20000000\"\n      }\n    }\n  }\n}\n</code></pre>","title":"Examples"},{"location":"docs/storage/#where-should-i-store-my-files","text":"<p>Not all filesystems are equivalent</p> <p>Choosing the appropriate storage location for your files is an essential step towards making your utilization of the cluster the most efficient possible. It will make your own experience much smoother, yield better performance for your jobs and simulations, and contribute to make Sherlock a useful and well-functioning resource for everyone.</p>  <p>Here is where we recommend storing different types of files and data on Sherlock:</p> <ul> <li>personal scripts, configuration files and software installations \u2192 <code>$HOME</code></li> <li>group-shared scripts, software installations and medium-sized datasets \u2192   <code>$GROUP_HOME</code></li> <li>temporary output of jobs, large checkpoint files \u2192 <code>$SCRATCH</code></li> <li>curated output of job campaigns, large group-shared datasets, archives \u2192 <code>$OAK</code></li> </ul>","title":"Where should I store my files?"},{"location":"docs/storage/#accessing-filesystems","text":"","title":"Accessing filesystems"},{"location":"docs/storage/#on-sherlock","text":"<p>Filesystem environment variables</p> <p>To facilitate access and data management, user and group storage location on Sherlock are identified by a set of environment variables, such as <code>$HOME</code> or <code>$SCRATCH</code>.</p>  <p>We strongly recommend using those variables in your scripts rather than explicit paths, to facilitate transition to new systems for instance. By using those environment variables, you'll be sure that your scripts will continue to work even if the underlying filesystem paths change.</p> <p>To see the contents of these variables, you can use the <code>echo</code> command. For instance, to see the absolute path of your $SCRATCH directory:</p> <pre><code>$ echo $SCRATCH\n/scratch/users/kilian\n</code></pre> <p>Or for instance, to move to your group-shared home directory:</p> <pre><code>$ cd $GROUP_HOME\n</code></pre>","title":"On Sherlock"},{"location":"docs/storage/#from-other-systems","text":"<p>External filesystems cannot be mounted on Sherlock</p> <p>For a variety of security, manageability and technical considerations, we can't mount external filesystems nor data storage systems on Sherlock. The recommended approach is to make Sherlock's data available on external systems.</p>  <p>You can mount any of your Sherlock directories on any external system you have access to by using SSHFS. For more details, please refer to the Data Transfer page.</p>   <ol> <li> <p>For more information about Oak, its characteristics and cost model,        please see the Oak Service Description page.\u00a0\u21a9</p> </li> </ol>","title":"From other systems"},{"location":"docs/storage/data-protection/","text":"<p>Data protection is mostly a task for the user</p> <p>Except for <code>$HOME</code> and <code>$GROUP_HOME</code>, data on Sherlock is not backed up, nor archived. It's up to each user and group to make sure they maintain multiple copies of their data if needed.</p>","title":"Data protection"},{"location":"docs/storage/data-protection/#snapshots","text":"<p>File system snapshots represent the state of the file system at a particular point in time. They allow accessing the file system contents as it was a different times in the past, and get back data that may have been deleted or modified since the snapshot was taken.</p>  <p>Important</p> <p>Snapshots are only available on <code>$HOME</code> and <code>$GROUP_HOME</code>.</p>","title":"Snapshots"},{"location":"docs/storage/data-protection/#accessing-snapshots","text":"<p>Snapshots taken in <code>$HOME</code> and <code>$GROUP_HOME</code> are accessible in a <code>.snapshot</code> directory at any level of the hierarchy. Those <code>.snapshot</code> directories don't appear when listing directory contents with <code>ls</code>, but they can be listed explicitly or accessed with <code>cd</code>:</p> <pre><code>$ cd $HOME\n$ ls -ald .snapshot/users*\n[...]\ndrwx------ 118 sunetid group  6680 Jul 21 11:16 .snapshot/users.daily.20170721\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.daily.20170722\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.daily.20170723\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/users.daily.20170724\ndrwx------ 118 sunetid group  6702 Jul 24 10:57 .snapshot/users.daily.latest\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-16:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-17:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-18:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-19:00\ndrwx------ 118 sunetid group  6702 Jul 21 16:19 .snapshot/users.hourly.20170722-20:00\n[...]\n$ cd .snapshot/users.daily.latest\n</code></pre> <p>For instance:</p> <ul> <li>the <code>$HOME/.snapshot/users.daily.latest</code> directory is the latest daily   snapshot available, and stores the contents of the $HOME directory as they   were when the last daily snapshot was taken,</li> <li>the <code>$HOME/foo/.snapshot/users.hourly.20170722-18:00</code> can be used to retrieve   the contents of the <code>$HOME/foo</code> directory as it was at 6pm on July 22th,   2017.</li> </ul>","title":"Accessing snapshots"},{"location":"docs/storage/data-protection/#restoring-from-a-snapshot","text":"<p>If you deleted a file or modified it and want to restore an earlier version, you can simply copy the file from its saved version in the appropriate snapshot.</p> <p>Examples:</p> <ul> <li> <p>to restore the last known version of <code>$HOME/foo/bar</code>:</p> <pre><code>$ cp $HOME/foo/.snapshot/users.hourly.latest/bar $HOME/foo/bar\n</code></pre> <p>or</p> <pre><code>$ cp $HOME/.snapshot/foo/users.hourly.latest/bar $HOME/foo/bar\n</code></pre> <p>(both commands are equivalent)</p> </li> <li> <p>to restore your <code>~/.bashrc</code> file from 2 days ago:</p> <pre><code>$ SNAP_DATE=$(date +%Y%m%d -d \"2 days ago\")\n$ cp $HOME/.snapshot/users.daily.${SNAP_DATE}/.bashrc $HOME/.bashrc\n</code></pre> </li> </ul>","title":"Restoring from a snapshot"},{"location":"docs/storage/data-protection/#snapshot-policy","text":"<p>The current1 policy is to take snapshots on an hourly, daily and weekly basis.  Older snapshots automatically expire after their retention period. The snapshot policy applies to both <code>$HOME</code> and <code>$GROUP_HOME</code> storage spaces.</p>    Snapshot frequency Retention period Number of snapshots     hourly 2 days 48   daily 1 week 7   weekly 1 month 4    <p>The shortest interval between snapshots is an hour. That means that if you create a file and then delete it within the hour, it won't appear in snapshots, and you won't be able to restore it.</p> <p>If a file exists for more than an hour, and is then deleted, it will be present in the hourly snapshots for the next 48 hours, and you'll be able to retrieve it during that period. Similarly, if a file exists for more than a day, it could be restored for up to 7 days.</p> <p>Snapshots don't count towards your quota.</p> <p>Snapshots, as well as the entire filesystem, are replicated to an off-site system, to ensure that data could be retrieved even in case of a catastrophic failure of the whole system or datacenter-level disaster.</p>","title":"Snapshot policy"},{"location":"docs/storage/data-protection/#backups","text":"<p>Although the SRCC doesn't offer any backup service per se, we do provide all the tools required to transfer data in and out of Sherlock.</p> <p>Suggested options to backup your data include:</p> <ul> <li>Oak, SRCC's long-term research data storage service   (Recommended)</li> <li>University IT Storage options and backup   services</li> <li>Cloud storage providers (see the Data transfer page for   information about the tools we provide to transfer files to/from the cloud)</li> </ul>   <ol> <li> <p>The snapshot policy is subject to change and may be adjusted as        the storage system usage conditions evolve.\u00a0\u21a9</p> </li> </ol>","title":"Backups"},{"location":"docs/storage/data-sharing/","text":"<p>The following sections present and detail options to share data across users and groups on Sherlock.</p>","title":"Data sharing"},{"location":"docs/storage/data-sharing/#sharing-data-locally-on-sherlock","text":"","title":"Sharing data locally on Sherlock"},{"location":"docs/storage/data-sharing/#traditional-unix-permissions","text":"<p>Standard Unix file permissions are supported on Sherlock and provide read, write and execute permissions for the three distinct access classes.</p> <p>The access classes are defined as follows:</p> <ul> <li>Files and directories are owned by a user. The owner determines the file's   user class. Distinct permissions apply to the owner.</li> <li>Files and directories are assigned a group, which define the file's group   class. Distinct permissions apply to members of the file's group. The owner   may be a member of the file's group.</li> <li>Users who are not the owner, nor a member of the group, comprise a file's   others class. Distinct permissions apply to others.</li> </ul> <p>The following permissions apply to each class:</p> <ul> <li>The <code>read</code> permission grants the ability to read a file. When set for a   directory, this permission grants the ability to read the names of files in   the directory, but not to find out any further information about them such as   contents, file type, size, ownership, permissions.</li> <li>The <code>write</code> permission grants the ability to modify a file. When set for a   directory, this permission grants the ability to modify entries in the   directory. This includes creating files, deleting files, and renaming files.</li> <li>The <code>execute</code> permission grants the ability to execute a file. This   permission must be set for executable programs, including shell scripts, in   order to allow the operating system to run them. When set for a directory,   this permission grants the ability to access file contents and   meta-information if its name is known, but not list files inside the   directory, unless read is set also.</li> </ul>  <p>Shared directories traversal</p> <p>If you need to give access to one of your files to another user, they will at least need execute permission on each directory within the path to that file.</p>  <p>The effective permissions are determined based on the first class the user falls within in the order of user, group then others. For example, the user who is the owner of the file will have the permissions given to the user class regardless of the permissions assigned to the group class or others class.</p> <p>While traditional Unix permissions are sufficient in most cases to share files with all the users within the same group, they are not enough to share files with a specific subset of users, or with users from other groups. Access Control Lists (ACLs) can be used for that purpose.</p> <p>There are two type of ACLs supported on Sherlock depending on the underlying filesystem:</p>    Type Filesystems     NFSv4 ACLs <code>$HOME</code> and <code>$GROUP_HOME</code>   POSIX ACLs <code>$SCRATCH</code>, <code>$GROUP_SCRATCH</code>, <code>$L_SCRATCH</code> and <code>$OAK</code>","title":"Traditional Unix permissions"},{"location":"docs/storage/data-sharing/#posix-acls","text":"<p>POSIX ACLs allows you to grant or deny access to files and directories for different users (or groups), independently of the file owner or group.</p> <p>Two types of POSIX ACLs can be defined:</p> <ul> <li>Access ACLs: grant permission for a specific file or directory.</li> <li>Default ACLs: allow to set a default set of ACLs that will be applied to   any file or directory without any already defined ACL. Can only be set on   directories.</li> </ul> <p>ACLs are set with the <code>setfacl</code> command, and displayed with <code>getfacl</code>. For more details and examples, please refer to this documentation.</p> <p>In the example below, we allow two users to access a restricted directory located at <code>$GROUP_SCRATCH/restricted-dir/</code>:</p> <pre><code>$ cd $GROUP_SCRATCH\n\n### Create new directory\n$ mkdir restricted-dir\n\n### Remove 'group' and 'other' access\n$ chmod g-rwx,o-rwx restricted-dir\n\n### Give user bob read and traversal permissions to the directory\n$ setfacl -m u:bob:rX restricted-dir\n\n### Use default ACLs (-d) to give user bob read access to all new\n### files and sub-directories that will be created in \"restricted-dir\"\n$ setfacl -d -m u:bob:rX restricted-dir\n\n### Give user alice read, write and traversal permissions for the directory\n$ setfacl -m u:alice:rwX restricted-dir\n\n### Use default ACLs (-d) to give user alice read and write access to all\n### new files and sub-directories\n$ setfacl -d -m u:alice:rwX restricted-dir\n\n### Show ACLs\n$ getfacl restricted-dir\n# file: restricted-dir/\n# owner: joe\n# group: grp\n# flags: -s-\nuser::rwx\nuser:bob:r-x\ngroup::---\nmask::r-x\nother::---\ndefault:user::rwx\ndefault:user:alice:rwx\ndefault:user:bob:r-x\ndefault:group::---\ndefault:mask::rwx\ndefault:other::---\n</code></pre>  <p>Default permissions on <code>$GROUP_SCRATCH</code></p> <p>By default, the Unix permissions on the root directory <code>$GROUP_SCRATCH</code> don't allow read nor traversal access for others (ie. any user not part of your PI group). If you need to share files with users outside of your own group, please contact us so we can set the appropriate permissions on your folder.</p>  <p>For <code>$SCRATCH</code>, you're the owner of the directory and so you can change the permissions yourself.</p>","title":"POSIX ACLs"},{"location":"docs/storage/data-sharing/#nfsv4-acls","text":"<p><code>$HOME</code> and <code>$GROUP_HOME</code> also allow setting ACLs, albeit with different syntax and semantics than POSIX ACLs. The principle is very similar, though.</p> <p>An ACL in NFSv4 is a list of rules setting permissions on files or directories. A permission rule, or Access Control Entry (ACE), is of the form <code>type:flags:principle:permissions</code>.</p> <p>Commonly used entries for these fields are:</p> <ul> <li>type: <code>A</code> (allow) or <code>D</code> (deny)</li> <li>flags: <code>g</code> (group), <code>d</code> (directory-inherit), <code>f</code> (file-inherit), <code>n</code>   (no-propagate-inherit), or <code>i</code> (inherit-only)</li> <li>principle:  a named user (<code>user@sherlock</code>), a group, or one of three   special principles: <code>OWNER@</code>, <code>GROUP@</code>, and <code>EVERYONE@</code>.</li> <li>permissions: there are 14 permission characters, as well as the shortcuts   <code>R</code>, <code>W</code>, and <code>X</code>. Here is a list of possible permissions that can be   included in the permissions field (options are Case Sensitive) <li><code>r</code> read-data (files) / list-directory (directories)</li> <li><code>w</code> write-data (files) / create-file (directories)</li> <li><code>x</code> execute (files) / change-directory (directories)</li> <li><code>a</code> append-data (files) / create-subdirectory (directories)</li> <li><code>t</code> read-attributes: read the attributes of the file/directory.</li> <li><code>T</code> write-attributes: write the attributes of the file/directory.</li> <li><code>n</code> read-named-attributes: read the named attributes of the       file/directory.</li> <li><code>N</code> write-named-attributes: write the named attributes of the       file/directory.</li> <li><code>c</code> read-ACL: read the file/directory NFSv4 ACL.</li> <li><code>C</code> write-ACL: write the file/directory NFSv4 ACL.</li> <li><code>o</code> write-owner: change ownership of the file/directory.</li> <li><code>y</code> synchronize: allow clients to use synchronous I/O with the server.</li> <li><code>d</code> delete: delete the file/directory. Some servers will allow a delete       to occur if either this permission is set in the file/directory or if the       delete-child permission is set in its parent direcory.</li> <li><code>D</code> delete-child: remove a file or subdirectory from within the given       directory (directories only)</li>  <p></p> <p>A comprehensive listing of allowable field strings is given in the manual page nfs4_acl(5)</p> <p>To see what permissions are set on a particular file, use the <code>nfs4_getfacl</code> command. For example, newly created <code>file1</code> may have default permissions listed by <code>ls -l</code> as <code>-rw-r\u2014r\u2014</code>. Listing the permissions with <code>nfs4_getfacl</code> would display the following:</p> <pre><code>$ nfs4_getfacl file1\nA::OWNER@:rwatTnNcCoy\nA:g:GROUP@:rtncy\nA::EVERYONE@:rtncy\n</code></pre> <p>To set permissions on a file, use the <code>nfs4_setfacl</code> command. For convenience, NFSv4 provides the shortcuts <code>R</code>, <code>W</code> and <code>X</code> for setting read, write, and execute permissions. For example, to add write permissions for the current group on <code>file1</code>, use <code>nfs4_setfacl</code> with the <code>-a</code> switch:</p> <pre><code>$ nfs4_setfacl -a A::GROUP@:W file1\n</code></pre> <p>This command switched the <code>GROUP@</code> permission field from <code>rtncy</code> to <code>rwatTnNcCoy</code>.  However, be aware that NFSv4 file permission shortcuts have a different meanings than the traditional Unix <code>r</code>, <code>w</code>, and <code>x</code>. For example issuing <code>chmod g+w file1</code> will set <code>GROUP@</code> to <code>rwatncy</code>.</p> <p>Although the shortcut permissions can be handy, often rules need to be more customized. Use <code>nfs4_setfacl -e file1</code>  to open the ACL for <code>file1</code> in a text editor.</p> <p>Access Control Entries allow more fine grained control over file and directory permissions than does the <code>chmod</code> command. For example, if user <code>joe</code> wants to give read and write permissions to <code>jack</code> for her directory <code>private</code>, she would issue:</p> <pre><code>$ nfs4_setfacl -R -a A::jack@sherlock:RW private/\n</code></pre> <p>The <code>-R</code> switch recursively applies the rule to the files and directories within <code>private/</code> as well.</p> <p>To allow <code>jack</code> to create files and subdirectories within <code>private/</code> with the permissions as granted above, inheritance rules need to be applied.</p> <pre><code>$ nfs4_setfacl -R -a A:fdi:jack@sherlock:RW private/\n</code></pre> <p>By default, each permission is in the Deny state and an ACE is required to explicitly allow a permission. However, be aware that a server may silently override a users ACE, usually to a less permissive setting.</p> <p>For complete documentation and examples on using NFSv4 ACLs, please see the manual page at nfs4_acl(5).</p>  <p>Default permissions on <code>$GROUP_HOME</code></p> <p>By default, the Unix permissions on the root directory <code>$GROUP_HOME</code> don't allow read nor traversal access for others (ie. any user not part of your PI group). If you need to share files with users outside of your own group, please contact us so we can set the appropriate permissions on your folder.</p>  <p>For <code>$HOME</code>, you're the owner of the directory and so you can change the permissions yourself.</p>","title":"NFSv4 ACLs"},{"location":"docs/storage/data-sharing/#sharing-data-outside-of-sherlock","text":"<p>If you'd like to share data stored on Sherlock with external collaborators, there are two possiblities:</p> <ol> <li> <p>sponsor a SUNet ID1 for these      collaborators, and contact us us to create a account for      them on Sherlock.  This will grant them access to your resources on      Sherlock (compute as well as storage) and give them access to your group      shared files, like any other user in your group.</p> </li> <li> <p>if you don't want to grant full access to your Sherlock resources to your      external collaborators, you can use the Globus data      sharing feature. This won't require your      collaborators to get Stanford accounts, and will allow easy sharing of      the datasets of your choice.</p>  <p>Globus Sharing is only available through the Oak endpoint</p> <p>Globus Sharing is only available on <code>$OAK</code>, using the Oak Globus Endpoint 2 (<code>srcc#oak</code>).</p>  <p>For complete details about sharing data wih Globus, please see the Globus  documentation at https://docs.globus.org/how-to/share-files/</p> </li> </ol>   <ol> <li> <p>a base-level SUNet ID (free) is sufficient to get an                 account on Sherlock. For more details about SUNet ID levels                 and associated services, please see the Stanford UIT SUNet                 IDs page.\u00a0\u21a9</p> </li> <li> <p>SUNet ID required\u00a0\u21a9</p> </li> </ol>","title":"Sharing data outside of Sherlock"},{"location":"docs/storage/data-transfer/","tags":["connection"],"text":"","title":"Data transfer"},{"location":"docs/storage/data-transfer/#transfer-protocols","tags":["connection"],"text":"<p>A number of methods allow transferring data in/out of Sherlock. For most cases, we recommend using SSH-based file transfer commands, such as <code>scp</code>, <code>sftp</code>, or <code>rsync</code>.  They will provide the best performance for data transfers from and to campus.</p>  <p>For large transfers, using DTNs is recommended</p> <p>Most casual data transfers could be done through the login nodes, by pointing your transfer tool to <code>login.sherlock.stanford.edu</code>. But because of resource limits on the login nodes, larger transfer may not work as expected.</p> <p>For transferring large amounts of data, Sherlock features a specific Data Transfer Node, with dedicated bandwidth, as well as a managed Globus endpoint, that can be used for scheduled, unattended data transfers.</p>  <p>We also provide tools on Sherlock to transfer data to various Cloud providers, such as AWS, Google Drive, Dropbox, Box, etc.</p>","title":"Transfer protocols"},{"location":"docs/storage/data-transfer/#prerequisites","tags":["connection"],"text":"<p>Most of the commands detailed below require a terminal and an SSH client1 on your local machine to launch commands.</p> <p>You'll need to start a terminal and type the given example commands at the prompt, omitting the initial <code>$</code> character (it just indicates a command prompt, and then should not be typed in).</p>","title":"Prerequisites"},{"location":"docs/storage/data-transfer/#host-keys","tags":["connection"],"text":"<p>Upon your very first connection to Sherlock, you will be greeted by a warning such as :</p> <pre><code>The authenticity of host 'login.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>The same warning will be displayed if your try to connect to one of the Data Transfer Node (DTN):</p> <pre><code>The authenticity of host 'dtn.sherlock.stanford.edu' can't be established.\nECDSA key fingerprint is SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>This warning is normal: your SSH client warns you that it is the first time it sees that new computer. To make sure you are actually connecting to the right machine, you should compare the ECDSA key fingerprint shown in the message with one of the fingerprints below:</p>    Key type Key Fingerprint     RSA <code>SHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA</code>legacy format: <code>f5:8f:01:46:d1:f9:66:5d:33:58:b4:82:d8:4a:34:41</code>   ECDSA <code>SHA256:eB0bODKdaCWtPgv0pYozsdC5ckfcBFVOxeMwrNKdkmg</code>legacy format: <code>70:4c:76:ea:ae:b2:0f:81:4b:9c:c6:5a:52:4c:7f:64</code>    <p>If they match, you can proceed and type \u2018yes\u2019. Your SSH program will then store that key and will verify it for every subsequent SSH connection, to make sure that the server you're connecting to is indeed Sherlock.</p>","title":"Host keys"},{"location":"docs/storage/data-transfer/#host-keys-warning","tags":["connection"],"text":"<p>If you've connected to Sherlock 1.0 before, there's a good chance the Sherlock 1.0 keys were stored by your local SSH client. In that case, when connecting to Sherlock 2.0 using the <code>sherlock.stanford.edu</code> alias, you will be presented with the following message:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: POSSIBLE DNS SPOOFING DETECTED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nThe RSA host key for sherlock.stanford.edu has changed, and the key for\nthe corresponding IP address 171.66.97.101 is unknown. This could\neither mean that DNS SPOOFING is happening or the IP address for the\nhost and its host key have changed at the same time.\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle\nattack)!  It is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:T1q1Tbq8k5XBD5PIxvlCfTxNMi1ORWwKNRPeZPXUfJA.\nPlease contact your system administrator.\n</code></pre> <p>You can just check that the SHA256 key listed in that warning message correctly matches the one listed in the table above, and if that's the case, you can safely remove the <code>sherlock.stanford.edu</code> entry from your <code>~/.ssh/known_hosts</code> file with the following command on your local machine:</p> <pre><code>$ ssh-keygen -R sherlock.stanford.edu\n</code></pre> <p>and then connect again. You'll see the first-connection prompt mentioned above, and your SSH client will store the new keys for future connections.</p>","title":"Host keys warning"},{"location":"docs/storage/data-transfer/#ssh-based-protocols","tags":["connection"],"text":"<p>User name</p> <p>In all the examples below, you'll need to replace <code>&lt;sunetid&gt;</code> by your actual SUNet ID. If you happen to use the same login name on your local machine, you can omit it.</p>","title":"SSH-based protocols"},{"location":"docs/storage/data-transfer/#scp-secure-copy","tags":["connection"],"text":"<p>The easiest command to use to transfer files to/from Sherlock is <code>scp</code>. It works like the <code>cp</code> command, except it can work over the network to copy files from one computer to another, using the secure SSH protocol.</p> <p>The general syntax to copy a file to a remote server is: <pre><code>$ scp &lt;source_file_path&gt; &lt;username&gt;@&lt;remote_host&gt;:&lt;destination_path&gt;'\n</code></pre></p> <p>For instance, the following command will copy the file named <code>foo</code> from your local machine to your home directory on Sherlock: <pre><code>$ scp foo &lt;sunetid&gt;@login.sherlock.stanford.edu:\n</code></pre> Note the <code>:</code> character, that separates the hostname from the destination path. Here, the destination path is empty, which will instruct scp to copy the file in your home directory.</p> <p>You can copy <code>foo</code> under a different name, or to another directory, with the following commands: <pre><code>$ scp foo &lt;sunetid&gt;@login.sherlock.stanford.edu:bar\n$ scp foo &lt;sunetid&gt;@login.sherlock.stanford.edu:~/subdir/baz\n</code></pre></p> <p>To copy back files from Sherlock to your local machine, you just need to reverse the order of the arguments: <pre><code>$ scp &lt;sunetid&gt;@login.sherlock.stanford.edu:foo local_foo\n</code></pre></p> <p>And finally, <code>scp</code> also support recursive copying of directories, with the <code>-r</code> option: <pre><code>$ scp -r dir/ &lt;sunetid&gt;@login.sherlock.stanford.edu:dir/\n</code></pre> This will copy the <code>dir/</code> directory and all of its contents in your home directory on Sherlock.</p>","title":"SCP (Secure Copy)"},{"location":"docs/storage/data-transfer/#sftp-secure-file-transfer-protocol","tags":["connection"],"text":"<p>SFTP clients are interactive file transfer programs, similar to FTP, which perform all operations over an encrypted transport.</p> <p>A variety of graphical SFTP clients are available for different OSes:</p> <ul> <li>WinSCP </li> <li>SecureFX ,</li> <li>Fetch2 </li> <li>CyberDuck </li> </ul> <p>When setting up your connection to Sherlock in the above programs, use the following information:</p> <pre><code>Hostname: login.sherlock.stanford.edu\nPort:     22\nUsername: SUNet ID\nPassword: SUNet ID password\n</code></pre> <p>OpenSSH also provides a command-line SFTP client, originally named <code>sftp</code>.</p> <p>To log in to Sherlock: <pre><code>$ sftp &lt;sunetid&gt;@login.sherlock.stanford.edu\nConnected to login.sherlock.stanford.edu.\nsftp&gt;\n</code></pre> For more information about using the command-line SFTP client, you can refer to this tutorial for more details and examples.</p>","title":"SFTP (Secure File Transfer Protocol)"},{"location":"docs/storage/data-transfer/#rsync","tags":["connection"],"text":"<p>If you have complex hierarchies of files to transfer, or if you need to synchronize a set of files and directories between your local machine and Sherlock, <code>rsync</code> will be the best tool for the job. It will efficiently transfer and synchronize files across systems, by checking the timestamp and size of files. Which means that it won't re-transfer files that have not changed since the last transfer, and will complete faster.</p> <p>For instance, to transfer the whole <code>~/data/</code> folder tree from your local machine to your home directory on Sherlock, you can use the following command: <pre><code>$ rsync -a ~/data/ &lt;sunetid&gt;@login.sherlock.stanford.edu:data/\n</code></pre> Note the slash (<code>/</code>) at the end of the directories name,  which is important to instruct <code>rsync</code> to synchronize the whole directories.</p> <p>To get more information about the transfer rate and follow its progress, you can use additional options: <pre><code>$ rsync -avP ~/data/ &lt;sunetid&gt;@login.sherlock.stanford.edu:data/\nsending incremental file list\n./\nfile1\n      1,755,049 100%    2.01MB/s    0:00:00 (xfr#2, to-chk=226/240)\nfile2\n      2,543,699 100%    2.48MB/s    0:00:00 (xfr#3, to-chk=225/240)\nfile3\n     34,930,688  19%   72.62MB/s    0:00:08\n\n[...]\n</code></pre> For more information about using the <code>rsync</code>, you can refer to this tutorial for more details and examples.</p>","title":"rsync"},{"location":"docs/storage/data-transfer/#sshfs","tags":["connection"],"text":"<p>Sometimes, moving files in and out of the cluster, and maintaining two copies of each of the files you work on, both on your local machine and on Sherlock, may be painful. Fortunately, Sherlock offers the ability to mount any of its filesystems to your local machine, using a secure and encrypted connection.</p> <p>With SSHFS, a FUSE-based filesystem implementation used to mount remote SSH-accessible filesystems, you can access your files on Sherlock as if they were locally stored on your own computer.</p> <p>This comes particularly handy when you need to access those files from an application that is not available on Sherlock, but that you already use or can install on your local machine. Like a data processing program that you have licensed for your own computer but can't use on Sherlock, a specific text editor that only runs on MacOS, or any data-intensive 3D rendering software that wouldn't work comfortably enough over a forwarded X11 connection.</p> <p>SSHFS is available for Linux , MacOS , and Windows .</p>  <p>SSHFS on macOS</p> <p>SSHFS on macOS is known to try to automatically reconnect filesystem mounts after resuming from sleep or suspend, even without any valid credentials.  As a result, it will generate a lot of failed connection attempts and likely make your IP address blacklisted on login nodes.</p> <p>Make sure to unmount your SSHFS drives before putting your macOS system to sleep to avoid this situation.</p>  <p>For instance, on a Linux machine with SSHFS installed, you could mount your Sherlock home directory with the following commands:</p> <pre><code>$ mkdir ~/sherlock_home\n$ sshfs &lt;sunetid&gt;@login.sherlock.stanford.edu:./ ~/sherlock_home\n</code></pre> <p>And to unmount it: <pre><code>$ umount ~/sherlock_home\n</code></pre></p> <p>For more information about using SSHFS on your local machine, you can refer to this tutorial for more details and examples.</p>","title":"SSHFS"},{"location":"docs/storage/data-transfer/#globus","tags":["connection"],"text":"<p>Globus improves SSH-based file transfer protocols by providing the following features:</p> <ul> <li>automates large data transfers,</li> <li>handles transient errors, and can resume failed transfers,</li> <li>simplifies the implementation of high-performance transfers between computing   centers.</li> </ul> <p>Globus is a Software as a Service (SaaS) system that provides end-users with a browser interface to initiate data transfers between endpoints. Globus allows users to \"drag and drop\" files from one endpoint to another. Endpoints are terminals for data; they can be laptops or supercomputers, and anything in between. The Globus web service negotiates, monitors, and optimizes transfers through firewalls and across network address translation (NAT). Under certain circumstances, with high performance hardware transfer rates exceeding 1 GB/s are possible. For more information about Globus, please see the Globus documentation.</p>","title":"Globus"},{"location":"docs/storage/data-transfer/#authentication","tags":["connection"],"text":"<p>To use Globus, you will first need to authenticate at Globus.org. You can either sign up for a Globus account, or use your SUNet ID account for authentication to Globus (which will be required to authenticate to the Sherlock endpoint).</p> <p>To use your SUNet ID, choose \"Stanford University\" from the drop down menu at the Login page and follow the instructions from there.</p>","title":"Authentication"},{"location":"docs/storage/data-transfer/#transfer","tags":["connection"],"text":"<p>Endpoint name</p> <p>The Globus endpoint name for Sherlock is <code>SRCC Sherlock</code>.</p>   <p>Oak endpoint</p> <p>The Sherlock endpoint only provides access to Sherlock-specific file systems (<code>$HOME</code>, <code>$GROUP_HOME</code>, <code>$SCRATCH</code> and <code>$GROUP_SCRATCH</code>). Oak features its own Globus endpoint: <code>SRCC Oak</code>.</p>  <p>You can use Globus to transfer data between your local workstation (e.g., your laptop or desktop) and Sherlock. In this workflow, you configure your local workstation as a Globus endpoint by installing the Globus Connect software.</p> <ol> <li>Log in to Globus.org</li> <li>Use the Manage Endpoints interface to \"add Globus    Connect Personal\" as an endpoint (you'll need to install Globus Connect    Personal on your local machine)</li> <li>Transfer Files, using your new workstation endpoint    for one side of the transfer, and the Sherlock endpoint (<code>SRCC Sherlock</code>) on    the other side.</li> </ol> <p>You can also transfer data between two remote endpoints, by choosing another endpoint you have access to instead of your local machine.</p>","title":"Transfer"},{"location":"docs/storage/data-transfer/#cli-and-api","tags":["connection"],"text":"<p>Globus also provides a command-line interface (CLI) and application programming interface (API) as an alternative to its web interface. Please see the Globus CLI documentation and Globus API documentation for more details.</p>","title":"CLI and API"},{"location":"docs/storage/data-transfer/#data-transfer-nodes-dtns","tags":["connection"],"text":"<p>No shell</p> <p>The DTNs don't provide any interactive shell, so connecting via SSH directly won't work. It will only accept <code>scp</code>, <code>sftp</code>, <code>rsync</code> of <code>bbcp</code> connections.</p>  <p>A pool of dedicated Data Transfer Nodes is available on Sherlock, to provide exclusive resources for large-scale data transfers.</p> <p>The main benefit of using it is that transfer tasks can't be disrupted by other users interactive tasks or filesystem access and I/O-related workloads on the login nodes.</p> <p>By using the Sherlock DTNs, you'll make sure that your data flows will go through a computer whose sole purpose is to move data around.</p> <p>It supports:</p> <ul> <li>SSH-based protocols (such as the ones described   above)</li> <li>BBCP</li> <li>Globus</li> </ul> <p>To transfer files via the DTNs, simply use <code>dtn.sherlock.stanford.edu</code> as a remote server hostname. For instance:</p> <pre><code>$ scp foo &lt;sunetid&gt;@dtn.sherlock.stanford.edu:~/foo\n</code></pre>  <p>$HOME on DTNs</p> <p>One important difference to keep in mind when transferring files through the Sherlock DTNs is that the default destination path for files, unless specified, is the user <code>$SCRATCH</code> directory, not <code>$HOME</code>.</p> <p>That means that the following command: <pre><code>$ scp foo &lt;sunetid&gt;@dtn.sherlock.stanford.edu:\n</code></pre> will create the <code>foo</code> file in  <code>$SCRATCH/foo</code>, and not in <code>$HOME/foo</code>.</p> <p>You can transfer file to your <code>$HOME</code> directory via the DTNs by specifying the full path as the destination: <pre><code>$ scp foo &lt;sunetid&gt;@dtn.sherlock.stanford.edu:$HOME/foo\n</code></pre></p>","title":"Data Transfer Nodes (DTNs)"},{"location":"docs/storage/data-transfer/#cloud-storage","tags":["connection"],"text":"<p>If you need to backup some of your Sherlock files to cloud-based storage services, we also provide a set of utilities that can help.</p>","title":"Cloud storage"},{"location":"docs/storage/data-transfer/#google-drive","tags":["connection"],"text":"<p>Google Drive storage for Stanford users</p> <p>Google Drive is free for educational institutions. Meaning you can get free and unlimited storage on Google Drive using your @stanford.edu account. See the University IT Google Drive page for more details.</p>  <p>We provide the <code>rclone</code> tool on Sherlock to interact with Google Drive. You'll just need to load the <code>rclone</code> module to be able to use it to move your files from/to Google Drive:</p> <pre><code>$ module load system rclone\n$ rclone --help\n</code></pre>","title":"Google Drive"},{"location":"docs/storage/data-transfer/#aws","tags":["connection"],"text":"<p>You can also access AWS storage from the Sherlock command line with the AWS Command Line Interface:</p> <pre><code>$ module load system aws-cli\n$ aws help\n</code></pre>","title":"AWS"},{"location":"docs/storage/data-transfer/#other-services","tags":["connection"],"text":"<p>If you need to access other cloud storage services, you can use rclone: it can be used to sync files and directories to and from Google Drive, Amazon S3, Box, Dropbox, Google Cloud Storage, Amazon Drive, Microsoft OneDrive and many more.</p> <pre><code>$ ml load system rclone\n$ rclone -h\n</code></pre> <p>For more details about how to use <code>rclone</code>, please see the official documentation.</p>   <ol> <li> <p>For more details, see the SSH clients page.\u00a0\u21a9</p> </li> <li> <p>Fetch is a commercial program, and is available as part of   the Essential Stanford Software bundle.\u00a0\u21a9</p> </li> </ol>","title":"Other services"},{"location":"docs/storage/filesystems/","text":"<p>The following sections describe the characteristics and best uses of each of the Sherlock's filesystems.</p>","title":"Filesystems"},{"location":"docs/storage/filesystems/#home","text":"<p>Summary</p> <p><code>$HOME</code> is your home directory. It's the best place to keep your code and important data as it provides snapshots and off-site replication. It is not meant to host data that will be actively read and written to by compute jobs.</p>     Characteristics      Type high speed, distributed NFS file system   Quota 15 GB for the whole <code>$HOME</code> directory   Snapshots yes (cf. Snapshots) for more info)   Backups off-site replication   Purge policy not purged   Scope all login and compute nodes","title":"<code>$HOME</code>"},{"location":"docs/storage/filesystems/#recommended-usage","text":"<p><code>$HOME</code> is best suited for personal configuration files, scripts, small reference files or datasets, source code and individual software installation</p> <p>When you log in, the system automatically sets the current working directory to <code>$HOME</code>: it's the location you'll end up when connecting to Sherlock. You can store your source code and build your executables there.</p> <p>We strongly recommend using <code>$HOME</code> to reference your home directory in scripts, rather than its explicit path.</p>","title":"Recommended usage"},{"location":"docs/storage/filesystems/#checking-quota-usage","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$HOME</code></p> <pre><code>$ sh_quota -f HOME\n</code></pre> <p>See the Checking Quotas section for more details.</p>","title":"Checking quota usage"},{"location":"docs/storage/filesystems/#group_home","text":"<p>Summary</p> <p><code>$GROUP_HOME</code> is your group home directory. It's the best place to keep your group's shared code, software installations and important data as it provides snapshots and off-site replication. It is not meant to host data that will be actively read and written to by compute jobs.</p>  <p><code>$HOME</code> and <code>$GROUP_HOME</code> are based on the same physical file system.</p>    Characteristics      Type high speed, distributed NFS file system   Quota 1 TB for the whole <code>$GROUP_HOME</code> directory   Snapshots yes (cf. Snapshots) for more info)   Backups off-site replication   Purge policy not purged   Scope all login and compute nodes","title":"<code>$GROUP_HOME</code>"},{"location":"docs/storage/filesystems/#recommended-usage_1","text":"<p><code>$GROUP_HOME</code> is best suited for group shared source code, common software installations, shared data sets and scripts.</p> <p>We strongly recommend using <code>$GROUP_HOME</code> to reference your group home directory in scripts, rather than its explicit path.</p>","title":"Recommended usage"},{"location":"docs/storage/filesystems/#checking-quota-usage_1","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$GROUP_HOME</code></p> <pre><code>$ sh_quota -f GROUP_HOME\n</code></pre> <p>See the Checking Quotas section for more details.</p>","title":"Checking quota usage"},{"location":"docs/storage/filesystems/#scratch","text":"<p>Summary</p> <p><code>$SCRATCH</code> is your personal scratch space. It's the best place to store temporary files, such as raw job output, intermediate files, unprocessed results, and so on.</p>   <p>Purge policy</p> <p>Files are automatically purged from <code>$SCRATCH</code> after an inactivity period:</p> <ul> <li>files that are not modified after 90 days are automatically   deleted,</li> <li>contents need to change for a file to be considered modified. The <code>touch</code>   command does not modify file contents and thus does not extend a file's   lifetime on the filesystem.</li> </ul> <p><code>$SCRATCH</code> is not meant to store permanent data, and should only be used for data associated with currently running jobs. It's not a target for backups, archived data, etc. See the Expiration Policy section for details.</p>     Characteristics      Type Parallel, high-performance Lustre file system   Quota 100 TB / 20,000,000 inodes2   Snapshots NO   Backups NO   Purge policy data not modified in the last 90 days are automatically purged   Scope all login and compute nodes","title":"<code>$SCRATCH</code>"},{"location":"docs/storage/filesystems/#recommended-usage_2","text":"<p><code>$SCRATCH</code> is best suited for large files, such as raw job output, intermediate job files, unprocessed simulation results, and so on.  This is the recommended location to run jobs from, and to store files that will be read or written to during job execution.</p> <p>Old files are automatically purged on <code>$SCRATCH</code> so users should avoid storing long-term data there.</p> <p>Each compute node has a low latency, high-bandwidth Infiniband link to <code>$SCRATCH</code>. The aggregate bandwidth of the filesystem is about 75GB/s. So any job with high data performance requirements will take advantage from using <code>$SCRATCH</code> for I/O.</p> <p>We strongly recommend using <code>$SCRATCH</code> to reference your scratch directory in scripts, rather than its explicit path.</p>","title":"Recommended usage"},{"location":"docs/storage/filesystems/#checking-quota-usage_2","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$SCRATCH</code></p> <pre><code>$ sh_quota -f SCRATCH\n</code></pre> <p>See the Checking Quotas section for more details.</p>","title":"Checking quota usage"},{"location":"docs/storage/filesystems/#expiration-policy","text":"<p>Inactive files are automatically purged</p> <p>Files that are not modified in the last 90 days will be automatically deleted from the filesystem.</p>  <p>To manage available space and maintain optimal performance for all jobs, all files on <code>$SCRATCH</code> are subject to automatic purges. Meaning that after a period of inactivity, files that are not used anymore will be automatically deleted from the filesystem.</p> <p>File activity is defined based on the last time a file's contents (the actual data in the file) have been modified. Meaning that files whose contents have not been modified in the previous 90 days will be automatically deleted.</p> <p>Each time a file's contents are modified, the expiration countdown is reset, and the file gets another 90-day of lifetime.</p>  <p>Metadata changes don't qualify as an update</p> <p>Modifying a file's contents is the only way to reset the expiration countdown and extend the file's lifetime on the filesystem.</p> <p>Metadata modifications such as: reading the file, renaming it, moving it to a different directory, changing its permissions or its ownership, \"touching\" it to update its last modification or access times, won't have any effect on the purge countdown.</p>  <p>Purges are based on an internal filesystem property that reflects the last date a file's data has been modified, and which is unfortunately not readily accessible by users.</p> <p>Please note that tools like <code>ls</code> will only display the date of the last metadata1 modification for a file, which is not necessarily relevant to determine a file's eligibility for deletion. For instance, using the <code>touch</code> command on a file to update its last modification date will only update the metadata, not the data, and as such, will not reset the purge countdown timer.</p> <p>Filesystem purges are a continuous process: they don't run at particular times, but are carried out in a permanent background fashion. Files are not necessarily deleted right away when they become eligible for deletion.  For instance, if you create a file on February 1st and don't ever modify it afterwards, it will be automatically become eligible for deletion on May 1st, and can be deleted anytime after this date.</p> <p>Empty directories that would remain after all their files have been purged are are not automatically deleted, because user workflows may rely on and require specific directory trees to be present. And there's no good way to distinguish between a directory created empty intentionally, and a directory emptied by automatic purges.</p>","title":"Expiration policy"},{"location":"docs/storage/filesystems/#group_scratch","text":"<p><code>$SCRATCH</code> and <code>$GROUP_SCRATCH</code> are based on the same physical file system.</p>  <p>Summary</p> <p><code>$GROUP_SCRATCH</code> is your group shared scratch space. It's the best place to store temporary files, such as raw job output, intermediate files, or unprocessed results that need to be shared among users within a group.</p>   <p><code>$GROUP_SCRATCH</code> is NOT a backup target</p> <p><code>$GROUP_SCRATCH</code> is not meant to store permanent data, and should only be used for data associated with currently running jobs. It's not a target for backups, archived data, etc.</p>     Characteristics      Type parallel, high-performance Lustre file system   Quota 100 TB / 20,000,000 inodes2   Snapshots NO   Backups NO   Purge policy data not accessed in the last 90 days are automatically purged   Scope all login and compute nodes","title":"<code>$GROUP_SCRATCH</code>"},{"location":"docs/storage/filesystems/#recommended-usage_3","text":"<p><code>$GROUP_SCRATCH</code> is best suited for large files, such as raw job output, intermediate job files, unprocessed simulation results, and so on.  This is the recommended location to run jobs from, and to store files that will be read or written to during job execution.</p> <p>Old files are automatically purged on <code>$GROUP_SCRATCH</code> so users should avoid storing long-term data there.</p> <p>We strongly recommend using <code>$GROUP_SCRATCH</code> to reference your group scratch directory in scripts, rather than its explicit path.</p>","title":"Recommended usage"},{"location":"docs/storage/filesystems/#checking-quota-usage_3","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$GROUP_SCRATCH</code></p> <pre><code>$ sh_quota -f GROUP_SCRATCH\n</code></pre> <p>See the Checking Quotas section for more details.</p>","title":"Checking quota usage"},{"location":"docs/storage/filesystems/#expiration-policy_1","text":"<p>As <code>$SCRATCH</code> and <code>$GROUP_SCRATCH</code> are on the same filesystem, the same expiration policy applies to both. Please see the <code>$SCRATCH</code> section above for more details.</p>","title":"Expiration policy"},{"location":"docs/storage/filesystems/#l_scratch","text":"<p>Summary</p> <p><code>$L_SCRATCH</code> is local to each compute node, and could be used to store temporary files for jobs with high IOPS requirements. Files stored in <code>$L_SCRATCH</code> are purged at the end of the job.</p>     Characteristics      Type local filesystem, specific to each node, based on SSD   Quota n/a (usable space limited by the size of the physical storage devices, typically around 150 GB)   Snapshots NO   Backups NO   Purge policy data immediately purged at the end of the job   Scope locally on each node, not shared across nodes","title":"<code>$L_SCRATCH</code>"},{"location":"docs/storage/filesystems/#recommended-usage_4","text":"<p><code>$L_SCRATCH</code> is best suited for small temporary files and applications which require low latency and high IOPS levels, typically intermediate job files, checkpoints, dumps of temporary states, etc.</p> <p>Files stored in <code>$L_SCRATCH</code> are local to each node and can't be accessed from other nodes, nor from login nodes.</p> <p>Please note that an additional, job-specific environment variable, <code>$L_SCRATCH_JOB</code>, will be set to a subdirectory of <code>$L_SCRATCH</code> for each job. So, if you have two jobs running on the same compute node, <code>$L_SCRATCH</code> will be the same and accessible from both jobs, while <code>$L_SCRATCH_JOB</code> will be different for each job.</p> <p>For instance, if you have jobs <code>98423</code> and <code>98672</code> running on this same nodes, the variables will be set as follows:</p>    Job id <code>$L_SCRATCH</code> <code>L_SCRATCH_JOB</code>     <code>98423</code> <code>/lscratch/kilian</code> <code>/lscratch/kilian/98423</code>   <code>98672</code> <code>/lscratch/kilian</code> <code>/lscratch/kilian/98672</code>    <p>We strongly recommend using <code>$L_SCRATCH</code> to reference your local scratch directory in scripts, rather than its full path.</p>","title":"Recommended usage"},{"location":"docs/storage/filesystems/#expiration-policy_2","text":"<p>All files stored in <code>$L_SCRATCH_JOB</code> are automatically purged at the end of the job, whether the job was successful or not. If you need to conserve files that were generated in <code>$L_SCRATCH_JOB</code> after the job ends, don't forget to add a command at the end of your batch script to copy them to one of the more persistent storage locations, such as <code>$HOME</code> or <code>$SCRATCH</code>.</p> <p>Data stored in <code>$L_SCRATCH</code> will be purged at the end of a job, only if no other job from the same user is still running on the node. Which means that data stored in <code>$L_SCRATCH</code> (but in not <code>$L_SCRATCH_JOB</code>) will persist on the node until the last job from the user terminates.</p>","title":"Expiration policy"},{"location":"docs/storage/filesystems/#oak","text":"<p>Summary</p> <p><code>$OAK</code> is SRCC's research data storage offering. It provides an affordable, longer-term storage option for labs and researchers, and is ideally suited to host large datasets, or curated, post-processed results from job campaigns, as well as final results used for publication.</p>   <p>Order <code>$OAK</code></p> <p>Oak storage can be easily ordered online using the Oak Storage Service page.</p> <p><code>$OAK</code> is opt-in and is available as an option on Sherlock. Meaning that only members of groups which have purchased storage on Oak can access this filesystem.</p> <p>For complete details and characteristics, including pricing, please refer to the Oak Storage Service page.</p>     Characteristics      Type parallel, capacitive Lustre filesystem   Quota amount purchased (in 10 TB increments)   Snapshots NO   Backups optional cloud backup available please contact us for details   Purge policy not purged   Scope all login and compute nodes also available through gateways outside of Sherlock","title":"<code>$OAK</code>"},{"location":"docs/storage/filesystems/#recommended-usage_5","text":"<p><code>$OAK</code> is ideally suited for large shared datasets, archival data and curated, post-processed results   from job campaigns, as well as final results used for publication.</p> <p>Although jobs can directly read and write to <code>$OAK</code> during execution, it is recommended to first stage files from <code>$OAK</code> to <code>$SCRATCH</code> at the beginning of a series of jobs, and save the desired results back from <code>$SCRATCH</code> to <code>$OAK</code> at the end of the job campaign.</p> <p>We strongly recommend using <code>$OAK</code> to reference your group home directory in scripts, rather than its explicit path.</p>  <p><code>$OAK</code> is not backed up</p> <p><code>$OAK</code> is not backed up or replicated, by design, and deleted files cannot be recovered. We recommend all researchers to keep an additional copy of their important files (for instance, in Google Drive).</p>   <p>Cloud backup option</p> <p>For additional data security, SRCC now offers \"cloud backup\" of Oak data as a managed service option. For an additional monthly fee, data on Oak can be backed up to the cloud (researchers are responsible for cloud storage costs). Please contact us if you'd like additional information.</p>","title":"Recommended usage"},{"location":"docs/storage/filesystems/#checking-quota-usage_4","text":"<p>The <code>sh_quota</code> tool can be used to display quota usage on <code>$OAK</code></p> <pre><code>$ sh_quota -f OAK\n</code></pre> <p>See the Checking Quotas section for more details.</p>   <ol> <li> <p>Metadata are data such as a file's size, name, path, owner,   permissions, etc.\u00a0\u21a9</p> </li> <li> <p>An inode (index node) is a data structure in a Unix-style file   system that describes a file-system object such as a file or a directory.\u00a0\u21a9\u21a9</p> </li> </ol>","title":"Checking quota usage"},{"location":"docs/tech/","tags":["tech"],"text":"","title":"Technical specifications"},{"location":"docs/tech/#in-a-nutshell","tags":["tech"],"text":"<p>Sherlock features over 1,300 compute nodes, 44,000+ CPU cores and 700+ GPUs, for a total computing power of more than 3.3 Petaflops. That would rank it in the Top500 list of the most powerful supercomputers in the world.</p> <p>The cluster currently extends across 2 Infiniband fabrics (EDR, HDR). A 6.1 PB parallel, distributed filesystem, delivering over 200 GB/s of I/O bandwidth, provides scratch storage for more than 5,900 users, and 900 PI groups.</p>","title":"In a nutshell"},{"location":"docs/tech/#resources","tags":["tech"],"text":"<p>The Sherlock cluster has been initiated in January 2014 with a base of freely available computing resources (about 2,000 CPU cores) and the accompanying networking and storage infrastructure (about 1 PB of shared storage).</p> <p>Since then, it's been constantly expanding, spawning multiple cluster generations, with numerous contributions from many research groups on campus.</p>  <p>Cluster generations</p> <p>For more information about Sherlock's ongoing evolution and expansion, please see Cluster generations.</p>","title":"Resources"},{"location":"docs/tech/#interface","tags":["tech"],"text":"Type Qty Details     login nodes 12  <code>sherlock.stanford.edu</code> (load-balanced)   data transfer nodes 3  dedicated bandwidth for large data transfers","title":"Interface"},{"location":"docs/tech/#computing","tags":["tech"],"text":"<p>Access to computing resources</p> <p>Computing resources marked with  below are freely available to every Sherlock user. Resources marked with  are only accessible to Sherlock owners and their research teams.</p>      Type Access Nodes CPU cores Details     compute nodes<code>normal</code> partition  154  4,032  - 56x 20  (Intel E5-2640v4), 128 GB RAM, EDR IB- 28x 24  (Intel 5118), 191 GB RAM, EDR IB- 70x 32  (AMD 7502), 256 GB RAM, HDR IB   development nodes<code>dev</code> partition  2  40  - 2x 20  (Intel E5-2640v4), 128 GB RAM, EDR IB   large memory nodes<code>bigmem</code> partition  5  408  - 1x 32  (Intel E5-2697Av4), 512 GB RAM, EDR IB- 1x 56  (Intel E5-4650v4), 3072 GB RAM, EDR IB- 1x 64  (AMD 7502), 4096 GB RAM, HDR IB- 2x 128  (AMD 7742), 1024 GB RAM, HDR IB   GPU nodes<code>gpu</code> partition  26  748  - 1x 20  (Intel E5-2640v4), 256 GB RAM, EDR IB- 4x Tesla P100 PCIe - 1x 20  (Intel E5-2640v4), 256 GB RAM, EDR IB- 4x Tesla P40 - 3x 20  (Intel E5-2640v4), 256 GB RAM, EDR IB- 4x Tesla V100_SXM2 - 1x 24  (Intel 5118), 191 GB RAM, EDR IB- 4x Tesla V100_SXM2 - 2x 24  (Intel 5118), 191 GB RAM, EDR IB- 4x Tesla V100 PCIe - 16x 32  (AMD 7502P), 256 GB RAM, HDR IB- 4x Geforce RTX_2080Ti - 2x 32  (AMD 7502P), 256 GB RAM, HDR IB- 4x Tesla V100S PCIe    privately-owned nodes<code>owners</code> partition  1,203  36,284  36 different node configurations, including GPU and bigmem nodes   Total  1,394  44,032  712","title":"Computing"},{"location":"docs/tech/#storage","tags":["tech"],"text":"<p>More information</p> <p>For more information about storage options on Sherlock, please refer to the Storage section of the documentation.</p>  <p>Sherlock is architected around shared storage components, meaning that users can find the same files and directories from all of the Sherlock nodes.</p> <ul> <li>Highly-available NFS filesystem for user and group home directories (with   hourly snapshots and off-site replication)</li> <li>High-performance Lustre scratch filesystem (6.1 PB parallel, distributed filesystem, delivering over 200 GB/s of I/O bandwidth)</li> <li>Direct access to SRCC's Oak long-term research data storage system   (42.1 PB)</li> </ul>","title":"Storage"},{"location":"docs/tech/facts/","tags":["tech"],"text":"<p>as of March 2022</p>","title":"Sherlock facts"},{"location":"docs/tech/facts/#users","tags":["tech"],"text":"<ul> <li> <p>5,973 user accounts</p> </li> <li> <p>973 PI groups</p> <p>from all Stanford's seven Schools, SLAC, Stanford Institutes, etc.</p> </li> <li> <p>167 owner groups</p> </li> </ul>","title":"Users"},{"location":"docs/tech/facts/#interfaces","tags":["tech"],"text":"<ul> <li> <p>12 login nodes</p> </li> <li> <p>3 data transfer nodes (DTNs)</p> </li> </ul>","title":"Interfaces"},{"location":"docs/tech/facts/#computing","tags":["tech"],"text":"<ul> <li> <p>3.36 PFLOPs</p> <p>FP64</p> </li> <li> <p>1,394 compute nodes</p> <p>19 server models (from 3 different manufacturers)</p> </li> <li> <p>44,032 CPU cores</p> <p>4 CPU generations (12 CPU models)</p> </li> <li> <p>712 GPUs</p> <p>4 GPU generations (10 GPU models)</p> </li> <li> <p>31 racks</p> <p>49 PDUs (381.35 kW of average power)</p> </li> </ul>","title":"Computing"},{"location":"docs/tech/facts/#storage","tags":["tech"],"text":"<ul> <li> <p>6.1 PB <code>$SCRATCH</code></p> <p>parallel, distributed filesystem, delivering over 200 GB/s of I/O bandwidth</p> </li> <li> <p>42.1 PB <code>$OAK</code></p> <p>long term research data storage</p> </li> </ul>","title":"Storage"},{"location":"docs/tech/facts/#networking","tags":["tech"],"text":"<ul> <li> <p>95 Infiniband switches</p> <p>across 2 Infiniband fabrics (EDR, HDR)</p> </li> <li> <p>4,723 Infiniband cables</p> <p>spanning about 24.24 km</p> </li> <li> <p>47 Ethernet switches</p> </li> </ul>","title":"Networking"},{"location":"docs/tech/facts/#scheduler","tags":["tech"],"text":"<ul> <li> <p>149 Slurm partitions</p> </li> <li> <p>149,248,197 CPU.hours</p> <p>used in the last 6 months, that's over 17,037 years of computing!</p> </li> </ul>","title":"Scheduler"},{"location":"docs/tech/status/","text":"var statusWidget = new Status.Widget({     hostname: \"status.sherlock.stanford.edu\",     selector: \"#sh_status\",     display: {         ledPosition: \"left\",     }   });    <p>Scheduled maintenances</p> <p>Maintenance operations and upgrades are scheduled on Sherlock on a regular basis.  Per the University's Minimum Security policies, we deploy security patches on Sherlock as required for compliance.</p>","title":"Status"},{"location":"docs/tech/status/#components-and-services","text":"<p>Sherlock status is </p> <p>For more details about Sherlock components and services, see the status dashboard.</p>","title":"Components and services"},{"location":"docs/tech/status/#current-usage","text":"","title":"Current usage"},{"location":"docs/user-guide/gpu/","text":"<p>To support the latest computing evolutions in many fields of science, Sherlock features a number of compute nodes with [GPUs][url_gpus] that can be used to run a variety of GPU-accelerated applications. Those nodes are available to everyone, but are a scarce, highly-demanded resource, so getting access to them may require some wait time in queue.</p>  <p>Getting your own GPU nodes</p> <p>If you need frequent access to GPU nodes, we recommend considering becoming an owner on Sherlock, so you can have immediate access to your GPU nodes when you need them.</p>","title":"GPU nodes"},{"location":"docs/user-guide/gpu/#gpu-nodes","text":"<p>A limited number of GPU nodes are available in the <code>gpu</code> partition. Anybody running on Sherlock can submit a job there. As owners contribute to expand Sherlock, more GPU nodes are added to the <code>owners</code> partition, for use by PI groups which purchased their own compute nodes.</p> <p>There are a variety of different GPU configuration available in the <code>gpu</code> partition. To see the available GPU types, please see the GPU types section.</p>","title":"GPU nodes"},{"location":"docs/user-guide/gpu/#submitting-a-gpu-job","text":"<p>To submit a GPU job, you'll need to use the <code>--gpus</code> (or <code>-G</code>) option in your batch script or command line submission options.</p> <p>For instance, the following script will request one GPU for two hours in the <code>gpu</code> partition, and run the GPU-enabled version of <code>gromacs</code>:</p> <pre><code>#!/bin/bash\n#SBATCH -p gpu\n#SBATCH -c 10\n#SBATCH -G 1\n\nml load gromacs/2016.3\n\nsrun gmx_gpu ...\n</code></pre> <p>You can also directly run GPU processes on compute nodes with <code>srun</code>. For instance, the following command will display details about the GPUs allocated to your job:</p> <pre><code>$ srun -p gpu --gpus 2 nvidia-smi\nFri Jul 28 12:41:49 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 0000:03:00.0     Off |                    0 |\n| N/A   26C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla P40           On   | 0000:04:00.0     Off |                    0 |\n| N/A   24C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>  <p>GPU resources MUST be requested explicitly</p> <p>Jobs will be rejected at submission time if they don't explictly request GPU resources.</p>  <p>The <code>gpu</code> partition only accepts jobs explicitly requesting GPU resources. If they don't, they will be rejected with the following message:</p> <pre><code>$ srun -p gpu --pty bash\nsrun: error: Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre>","title":"Submitting a GPU job"},{"location":"docs/user-guide/gpu/#interactive-session","text":"<p>As for any other compute node, you can submit an interactive job and request a shell on a GPU node with the following command:</p> <pre><code>$ srun -p gpu --gpus 1 --pty bash\nsrun: job 38068928 queued and waiting for resources\nsrun: job 38068928 has been allocated resources\n$ nvidia-smi --query-gpu=index,name --format=csv,noheader\n0, Tesla V100-SXM2-16GB\n</code></pre>","title":"Interactive session"},{"location":"docs/user-guide/gpu/#gpu-types","text":"<p>Since Sherlock features many different types of GPUs, each with its own technical characteristics, performance profiles and specificities, you may want to ensure that your job runs on a specific type of GPU.</p> <p>To that end, Slurm allows users to specify constraints when submitting jobs, which will indicate the scheduler that only nodes having features matching the job constraints could be used to satisfy the request. Multiple constraints may be specified and combined with various operators (please refer to the official Slurm documentation for details).</p> <p>The list of available features on GPU nodes can be obtained with the <code>node_feat</code>1 command:</p> <pre><code>$ node_feat -p gpu | grep GPU_\nGPU_BRD:TESLA\nGPU_GEN:PSC\nGPU_MEM:16GB\nGPU_MEM:24GB\nGPU_SKU:TESLA_P100_PCIE\nGPU_SKU:TESLA_P40\n</code></pre> <p><code>node_feat</code> will only list the features of nodes from partitions you have access to, so output may vary depending on your group membership.</p> <p>The different characteristics2 of various GPU types are listed in the following table</p>    Slurm\u00a0feature Description Possible values Example job constraint     <code>GPU_BRD</code> GPU brand <code>GEFORCE</code>: GeForce / TITAN<code>TESLA</code>: Tesla <code>#SBATCH -C GPU_BRD:TESLA</code>   <code>GPU_GEN</code> GPU generation <code>PSC</code>: Pascal<code>MXW</code>: Maxwell <code>#SBATCH -C GPU_GEN:PSC</code>   <code>GPU_MEM</code> Amount of GPU memory <code>16GB</code>, <code>24GB</code> <code>#SBATCH -C GPU_MEM:16GB</code>   <code>GPU_SKU</code> GPU model <code>TESLA_P100_PCIE</code><code>TESLA_P40</code> <code>#SBATCH -C GPU_SKU:TESLA_P40</code>    <p>Depending on the partitions you have access to, more features may be available to be requested in your jobs.</p> <p>For instance, to request a Tesla GPU for you job, you can use the following submission options:</p> <pre><code>$ srun -p owners -G 1 -C GPU_BRD:TESLA nvidia-smi -L\nGPU 0: Tesla P100-SXM2-16GB (UUID: GPU-4f91f58f-f3ea-d414-d4ce-faf587c5c4d4)\n</code></pre>  <p>Unsatisfiable constraints</p> <p>If you specify a constraint that can't be satisfied in the partition you're submitting your job to, the job will be rejected by the scheduler.     For instance, requesting a GeForce GPU in the <code>gpu</code> partition, which only features Tesla GPUs, will result in an error:</p> <pre><code>$ srun -p gpu -G 1 -C GPU_BRD:GEFORCE nvidia-smi -L\nsrun: error: Unable to allocate resources: Requested node configuration is not available\n</code></pre>","title":"GPU types"},{"location":"docs/user-guide/gpu/#gpu-compute-modes","text":"<p>By default, GPUs on Sherlock are set in the Exclusive Process compute mode3, to provide the best performance and an isolated environment for jobs, out of the box.</p> <p>Some software may require GPUs to be set to a different compute mode, for instance to share a GPU across different processes within the same application.</p> <p>To handle that case, we developed a specific option, <code>--gpu_cmode</code>, that users can add to their <code>srun</code> and <code>sbatch</code> submission options, to choose the compute mode for the GPUs allocated to their job.</p> <p>Here's the list of the different compute modes supported on Sherlock's GPUs:</p>    GPU\u00a0compute\u00a0mode <code>--gpu_cmode</code> option Description     \"Default\" <code>shared</code> Multiple contexts are allowed per device (NVIDIA default)   \"Exclusive Process\" <code>exclusive</code> Only one context is allowed per device, usable from multiple threads at a time (Sherlock default)   \"Prohibited\" <code>prohibited</code> No CUDA context can be created on the device    <p>By default, or if the <code>--gpu_cmode</code> option is not specified, GPUs will be set in the \"Exclusive Process\" mode, as demonstrated by this example command:</p> <pre><code>$ srun -p gpu -G 1 nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <p>With the <code>--gpu_cmode</code> option, the scheduler will set the GPU compute mode to the desired value before execution:</p> <pre><code>$ srun -p gpu -G 1 --gpu_cmode=shared nvidia-smi\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:03:00.0 Off |                    0 |\n| N/A   22C    P8    10W / 250W |      0MiB / 22912MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre>  <p>Tip</p> <p>\"Default\" is the name that the NVIDIA System Management Interface (<code>nvidia-smi</code>) uses to describe the mode where a GPU can be shared between different processes. It does not represent the default GPU compute mode on Sherlock, which is \"Exclusive Process\".</p>","title":"GPU compute modes"},{"location":"docs/user-guide/gpu/#advanced-options","text":"<p>A number of submission options are available when submitting GPU jobs, to request specific resource mapping or task binding options.</p> <p>Here are some examples to allocate a set of resources as a function of the number of requested GPUs:</p> <ul> <li> <p><code>--cpus-per-gpu</code>: requests a number of CPUs per allocated GPU.</p> <p>For instance, the following options will allocate 2 GPUs and 4 CPUs:</p> <pre><code>$ salloc -p gpu -G 2 --cpus-per-gpu=2\n</code></pre> </li> <li> <p><code>--gpus-per-node</code>: requests a number of GPUs per node,</p> </li> <li><code>--gpus-per-task</code>: requests a number of GPUs per spawned task,</li> <li><code>--mem-per-gpu</code>: allocates (host) memory per allocated GPU.</li> </ul> <p>Other options can help set particular GPU properties (topology, frequency...):</p> <ul> <li> <p><code>--gpu-bind</code>: specify task/GPU binding mode.</p> <p>By default every spawned task can access every GPU allocated to the job. This option can help making sure that tasks are bound to the closest GPU, for better performance.</p> </li> <li> <p><code>--gpu-freq</code>: specify GPU and memory frequency. For instance:</p> <pre><code>$ srun -p test -G 1 --gpu-freq=highm1,verbose /bin/true\nGpuFreq=memory_freq:2600,graphics_freq:758\n</code></pre> </li> </ul> <p>Those options are all available to the <code>srun</code>/<code>sbatch</code>/<code>salloc</code> commands, and more details about each of them can be found in the Slurm documentation.</p>  <p>Conflicting options</p> <p>Given the multitude of options, it's very easy to submit a job with conflicting options.  In most cases the job will be rejected.</p> <p>For instance: <pre><code>$ sbatch --gpus-per-task=1 --cpus-per-gpu=2  --cpus-per-task=1 ...\n</code></pre> Here, the first two options implicitly set <code>cpu-per-task</code> to 2, while the third option explicitly sets <code>cpus-per-task</code> to 1. So the job's requirements are conflicting and can't be satisfied.</p>","title":"Advanced options"},{"location":"docs/user-guide/gpu/#environment-and-diagnostic-tools","text":"","title":"Environment and diagnostic tools"},{"location":"docs/user-guide/gpu/#nvtop","text":"<p>GPU usage information can be shown with the <code>nvtop</code> tool. <code>nvtop</code> is available as a module, which can be loaded like this:</p> <pre><code>$ ml load system nvtop\n</code></pre> <p><code>nvtop</code> provides an <code>htop</code>-like interactive view of GPU utilization.  Users can monitor, estimate and fine tune their GPU resource requests with this tool.  Percent GPU and memory utilization is shown as a user's GPU code is running.</p> <p></p>   <ol> <li> <p>See <code>node_feat -h</code> for more details.\u00a0\u21a9</p> </li> <li> <p>The lists of values provided in the table are non exhaustive.\u00a0\u21a9</p> </li> <li> <p>The list of available GPU compute modes and relevant details are   available in the CUDA Toolkit Documentation \u21a9</p> </li> </ol>","title":"<code>nvtop</code>"},{"location":"docs/user-guide/ondemand/","text":"","title":"OnDemand"},{"location":"docs/user-guide/ondemand/#introduction","text":"<p>The Sherlock OnDemand interface allows you to conduct your research on Sherlock through a web browser. You can manage files (create, edit and move them), submit and monitor your jobs, see their output, check the status of the job queue, run a Jupyter notebook and much more, without logging in to Sherlock the traditional way, via a SSH terminal connection.</p>  <p>Quote</p> <p>In neuroimaging there are a number of software pipelines that output HTML reports heavy on images files. Sherlock OnDemand allows users to check those as they appear on their <code>$SCRATCH</code> folder, for quick quality control, instead of having to mount remote filesystems, download data locally or move to any other storage location. Since the data itself is already quite big and costly to move, OnDemand is extremely helpful for fast assessment.</p> <p>-- Carolina Ramirez, Williams PANLab</p>","title":"Introduction"},{"location":"docs/user-guide/ondemand/#more-documentation","text":"<p>Open OnDemand was created by the Ohio Supercomputer Center. </p> <p>The following documentation is specifically intended for using OnDemand on Sherlock. For more complete documentation about OnDemand in general, please see the extensive documentation for OnDemand created by OSC, including many video tutorials.</p>","title":"More documentation"},{"location":"docs/user-guide/ondemand/#connecting","text":"<p>Connection information</p> <p>To connect to Sherlock OnDemand, simply point your browser to https://login.sherlock.stanford.edu</p>  <p>Sherlock OnDemand requires the same level of authentication than connecting to Sherlock over SSH. You will be prompted for your SUNet ID and password, and will go through the regular two-step authentication process.</p> <p>The Sherlock OnDemand Dashboard will then open. From there, you can use the menus across the top of the page to manage files, get a shell on Sherlock, submit jobs or open interactive applications such as Jupyter Notebooks or RStudio sessions.</p> <p></p> <p>To end your Sherlock OnDemand session, click on the \"Log Out\" link at the top right of the Dashboard window and close your browser.</p>","title":"Connecting"},{"location":"docs/user-guide/ondemand/#getting-a-shell","text":"<p>You can get shell access to Sherlock by choosing Clusters &gt; Sherlock Shell Access from the top menu in the OnDemand Dashboard.</p> <p>In the window that will open, you'll be logged in to one of Sherlock's login nodes, exactly as if you were using SSH to connect. Except you don't need to install any SSH client on your local machine, configure Kerberos or deal with your SSH client configuration to avoid endless two-factor prompts. How cool is that?</p> <p></p>","title":"Getting a shell"},{"location":"docs/user-guide/ondemand/#managing-files","text":"<p>To create, edit or move files, click on the Files menu from the Dashboard page. A dropdown menu will appear, listing your most common storage locations on Sherlock: <code>$HOME</code>, <code>$GROUP_HOME</code>, <code>$SCRATCH</code>. <code>$GROUP_SCRATCH</code> and <code>$OAK</code>1.</p> <p>Choosing one of the file spaces opens the File Explorer in a new browser tab. The files in the selected directory are listed.</p>  <p>Left panel will always display <code>$HOME</code></p> <p>No matter which directory you are in, your home directory is displayed in a panel on the left.</p>  <p>There are two sets of buttons in the File Explorer.</p> <ul> <li> <p>On the top left, just below the name of the current directory:      Those buttons allow you to   View, Edit, Rename, Download, Copy, Paste (after you   have moved to a different directory) or Delete a file, or you can toggle   the file selection with (Un)Select All.</p> </li> <li> <p>At the top of the window, on the right side:   </p>    Button Function     Go To Navigate to another directory or file system   Open in Terminal Open a terminal window on Sherlock in a new browser tab   New File Create a new, empty file   New Dir Create a new subdirectory   Upload Copy a file from your local machine to Sherlock   Show Dotfiles Toggle the display of dotfiles (files starting by a <code>.</code>, which are usually hidden)   Show Owner/Mode Toggle the display of owner and permisson settings    </li> </ul>","title":"Managing files"},{"location":"docs/user-guide/ondemand/#creating-and-editing-jobs","text":"<p>You can create new job scripts, edit existing scripts, and submit them to the scheduler throught the Sherlock OnDemand interface.</p> <p>From the top menus in the Dashboard, choose Jobs &gt; Job Composer. A Job Composer window will open. There are two tabs at the top: Jobs and Templates.</p> <p>In the Jobs tab, you'll find a list of the job you've submitted through OnDemand. The Templates tab will allow you to define your own job templates.</p>","title":"Creating and editing jobs"},{"location":"docs/user-guide/ondemand/#creating-a-new-job-script","text":"<p>To create a new job script. you'll need to follow the steps below.</p>","title":"Creating a new job script"},{"location":"docs/user-guide/ondemand/#select-a-template","text":"<p>Go to the Jobs tab in the Jobs Composer interface. You'll find a default template there: \"Simple Sequential Job\".</p> <p>To create a new job script, click the blue New Job &gt; From Default Template button in the upper left. You'll see a green message at the top of the page indicating: \"Job was successfully created\".</p> <p>At the right of the Jobs page, you can see the Job Details, including the location of the script and the script name (by default, <code>main_job.sh</code>). Under that, you will see the contents of the job script in a section named Submit Script.</p> <p></p>","title":"Select a template"},{"location":"docs/user-guide/ondemand/#edit-the-job-script","text":"<p>You'll need to edit the job script, so it contains the commands and workflow that you want to submit to the scheduler.</p> <p>If you need more resources than the defaults, you must include options to change them in the job script. For more details, see the Running jobs section.</p> <p>You can edit the script in several ways:</p> <ul> <li>click the blue Edit Files button at the top of the Jobs tab in the   Jobs Composer window,</li> <li>in the Jobs tab in the Jobs Composer window, find the Submit   Script section at the bottom right. Click the blue Open Editor button.</li> </ul> <p>After you save the file, the editor window remains open, but if you return to the Jobs Composer window, you will see that the content of  your script has changed.</p>","title":"Edit the job script"},{"location":"docs/user-guide/ondemand/#edit-the-job-options","text":"<p>In the Jobs tab in the Jobs Composer window, click the blue Job Options button. The options for the selected job such as name, the job script to run, and the account it run under are displayed and can be edited. Click Save or Cancel to return to the job listing.</p>","title":"Edit the job options"},{"location":"docs/user-guide/ondemand/#submitting-jobs","text":"<p>To submit a job, select in in the Jobs tab in the Jobs Composer page. Click the green Submit button to submit the selected job. A message at the top of the window shows whether the job submission was successful or not. If it is not, you can edit the job script or options and resubmit. When the job is submitted successfully, the status of the job in the Jobs Composer window will change to Queued or Running. When  the job completes, the status will change to Completed.</p> <p></p>","title":"Submitting jobs"},{"location":"docs/user-guide/ondemand/#monitoring-jobs","text":"<p>From the Dashboard page, The Jobs &gt; Active Jobs top-level menu will bring you to a live view of Sherlock's scheduler queue. You'll be able to see all the jobs currently in queue, including running and pending jobs, as well as eome details about individual jobs.</p> <p></p> <p>At the bottom of the detailled view, you'll find two button that will bring you to the directory where that job's files are located, either in the File Manager or in a Shell session.</p>","title":"Monitoring jobs"},{"location":"docs/user-guide/ondemand/#interactive-applications","text":"<p>One of the main features of Sherlock OnDemand is the ability to run interactive applications difrectly from the web interface, without leaving your web browser.</p>","title":"Interactive applications"},{"location":"docs/user-guide/ondemand/#jupyter-notebooks","text":"<p>You can run Jupyter Notebooks (using Python, Julia or other languages) through Sherlock OnDemand.</p>  <p>Some preliminary setup may be required</p> <p>Before running your first Jupyter Notebook with <code>IJulia</code>, you'll need to run the following steps (this only need to be done once):</p> <pre><code>$ ml julia\n$ julia\njulia&gt; using Pkg;\njulia&gt; Pkg.add(\"IJulia\")\n</code></pre> <p>When you see the message that <code>IJulia</code> has been installed, you can end your interactive session.</p>  <p>To start a Jupyter session from Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; Jupyter Notebook from the top menu in the    Dashboard page,</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your notebook start.</p> </li> </ol> <p></p> <ol> <li> <p>Click the blue Launch button to start your JupyterHub session. You may    have to wait in the queue for resources to become available for you.</p> </li> <li> <p>When your session starts, you can click on the blue Connect to Jupyter    button to open your Jupyter Notebook. The Dashboard window will display    information about your Jupyter session, including the name of the compute    node it is running on, when it started, and how much time remains.    </p> </li> <li> <p>In your new Jupyter Notebook tab, you'll see 3 tabs: Files, Running and    Clusters.    </p> </li> </ol> <p>By default, you are in the Files tab, that displays the contents of your    <code>$HOME</code> directory on Sherlock. You can navigate through your files there.</p> <p>Under the Running tab, you will see the list of all the notebooks or    terminal sessions that you have currently running.</p> <ol> <li> <p>You can now start a Jupyter Notebook:</p> <ol> <li>To open an exiting Jupyter Notebook, which is already stored on    Sherlock, navigate to its location in the Files tab and click on its    name. A new window running the notebook will open.</li> <li>To create a new Jupyter Notebook, click on the New button at the top    right of the file listing, and choose the kernel of your choice from the    drop down.</li> </ol> </li> </ol> <p>To terminate your Jupyter Notebook session, go back to the Dashboard, and click on the My Interactive Sessions in the top menu. This will bring you to a page listing all your currently active interactive session. Identify the one you'd like to terminate and click on the red Delete button.</p>","title":"Jupyter Notebooks"},{"location":"docs/user-guide/ondemand/#rstudio","text":"<p>To run RStudio via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; RStudio Server from the top menu in the    Dashboard page,</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your notebook start.</p> </li> <li> <p>Click the blue Launch button to start your RStudio session. You may have    to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to RStudio Server    button. A new window opens with the RStudio interface.</p> </li> </ol> <p></p>","title":"RStudio"},{"location":"docs/user-guide/ondemand/#tensorboard","text":"<p>To run Tensorboard via Sherlock OnDemand:</p> <ol> <li> <p>Select Interactive Apps &gt; Tensorboard from the top menu in the    Dashboard page,</p> </li> <li> <p>In the screen that opens, specify the different parameters for your job    (time limit, number of nodes, CPUs, partition to use, etc.). You can also    choose to be notified by email when your notebook start.</p> </li> <li> <p>Click the blue Launch button to start your Tensorboard session. You may have    to wait in the queue for resources to become available.</p> </li> <li> <p>When your session starts, click the blue Connect to Tensorboard    button. A new window opens with the Tensorboard interface.</p> </li> </ol> <p></p>   <ol> <li> <p>if you have access to the Oak storage system.\u00a0\u21a9</p> </li> </ol>","title":"Tensorboard"},{"location":"docs/user-guide/running-jobs/","tags":["slurm"],"text":"","title":"Running jobs"},{"location":"docs/user-guide/running-jobs/#login-nodes","tags":["slurm"],"text":"<p>Login nodes are not for computing</p> <p>Login nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.</p>  <p>The key principle of a shared computing environment is that resources are shared among users and must be scheduled. It is mandatory to schedule work by submitting jobs to the scheduler on Sherlock. And since login nodes are a shared resource, they must not be used to execute computing tasks.</p> <p>Acceptable use of login nodes include:</p> <ul> <li>lightweight file transfers,</li> <li>script and configuration file editing,</li> <li>job submission and monitoring.</li> </ul>  <p>Resource limits are enforced</p> <p>To minimize disruption and ensure a confortable working environment for users, resource limits are enforced on login nodes, and processes started there will automatically be terminated if their resource usage (including CPU time, memory and run time) exceed those limits.</p>","title":"Login nodes"},{"location":"docs/user-guide/running-jobs/#slurm-commands","tags":["slurm"],"text":"<p>Slurm allows requesting resources and submitting jobs in a variety of ways. The main Slurm commands to submit jobs are listed in the table below:</p>    Command Description Behavior     <code>salloc</code> Request resources and allocates them to a job Starts a new shell, but does not execute anything   <code>srun</code> Request resources and runs a command on the allocated compute node(s) Blocking command: will not return until the job ends   <code>sbatch</code> Request resources and runs a script on the allocated compute node(s) Asynchronous command: will return as soon as the job is submitted","title":"Slurm commands"},{"location":"docs/user-guide/running-jobs/#interactive-jobs","tags":["slurm"],"text":"","title":"Interactive jobs"},{"location":"docs/user-guide/running-jobs/#dedicated-nodes","tags":["slurm"],"text":"<p>Interactive jobs allow users to log in to a compute node to run commands interactively on the command line. They could be an integral part of an interactive programming and debugging workflow. The simplest way to establish an interactive session on Sherlock is to use the <code>sdev</code> command:</p> <pre><code>$ sdev\n</code></pre> <p>This will open a login shell using one core and 4 GB of memory on one node for one hour. The <code>sdev</code> sessions run on dedicated compute nodes. This ensures minimal wait times when you need to access a node for testing script, debug code or any kind of interactive work.</p> <p><code>sdev</code> also provides X11 forwarding via the submission host (typically the login node you're connected to) and can thus be used to run GUI applications.</p>","title":"Dedicated nodes"},{"location":"docs/user-guide/running-jobs/#compute-nodes","tags":["slurm"],"text":"<p>If you need more resources1, you can pass options to <code>sdev</code>, to request more CPU cores, more nodes, or even run in a different partition. <code>sdev -h</code> will provide more information:</p> <pre><code>$ sdev -h\nsdev: start an interactive shell on a compute node.\n\nUsage: sdev [OPTIONS]\n    Optional arguments:\n        -c      number of CPU cores to request (OpenMP/pthreads, default: 1)\n        -n      number of tasks to request (MPI ranks, default: 1)\n        -N      number of nodes to request (default: 1)\n        -m      memory amount to request (default: 4GB)\n        -p      partition to run the job in (default: dev)\n        -t      time limit (default: 01:00:00)\n        -r      allocate resources from the named reservation (default: none)\n        -J      job name (default: sdev)\n        -q      quality of service to request for the job (default: normal)\n\n    Note: the default partition only allows for limited amount of resources.\n    If you need more, your job will be rejected unless you specify an\n    alternative partition with -p.\n</code></pre> <p>Another way to get an interactive session on a compute node is to use <code>srun</code> to execute a shell through the scheduler. For instance, to start a <code>bash</code> session on a compute node, with the default resource requirements (one core for 2 hours), you can run:</p> <pre><code>$ srun --pty bash\n</code></pre> <p>The main advantage of this approach is that it will allow you to specify the whole range of submission options that <code>sdev</code> may not support.</p> <p>Finally, if you prefer to submit an existing job script or other executable as an interactive job, you can use the <code>salloc</code> command:</p> <pre><code>$ salloc script.sh\n</code></pre> <p>If you don't provide a command to execute, <code>salloc</code> will start a Slurm job and allocate resources for it, but it will not automatically connect you to the allocated node(s). It will only start a new shell on the same node you launched <code>salloc</code> from, and set up the appropriate <code>$SLURM_*</code> environment variables. So you will typically need to look at them to see what nodes have been assigned to your job. For instance:</p> <pre><code>$ salloc\nsalloc: Granted job allocation 655914\n$ echo $SLURM_NODELIST\nsh02-01n55\n$ ssh sh02-01n55\n[...]\nsh02-01n55 ~ $\n</code></pre>","title":"Compute nodes"},{"location":"docs/user-guide/running-jobs/#connecting-to-nodes","tags":["slurm"],"text":"<p>Login to compute nodes</p> <p>Users are not allowed to login to compute nodes unless they have a job running there.</p>  <p>If you SSH to a compute node without any active job allocation, you'll be greeted by the following message:</p> <pre><code>$ ssh sh02-01n01\nAccess denied by pam_slurm_adopt: you have no active jobs on this node\nConnection closed\n$\n</code></pre> <p>Once you have a job running on a node, you can SSH directly to it and run additional processes2, or observe how you application behaves, debug issues, and so on.</p> <p>The <code>salloc</code> command supports the same parameters as <code>sbatch</code>, and can override any default configuration. Note that any <code>#SBATCH</code> directive in your job script will not be interpreted by <code>salloc</code> when it is executed in this way. You must specify all arguments directly on the command line for them to be taken into account.</p>","title":"Connecting to nodes"},{"location":"docs/user-guide/running-jobs/#batch-jobs","tags":["slurm"],"text":"<p>It's easy to to schedule batch jobs on Sherlock. A job is simply an instance of your program, for example your R, Python or Matlab script that is submitted to and executed by the scheduler (Slurm). When you submit a job with the sbatch command it's called a batch job and it will either run immediately or will pend (wait) in the queue.</p> <p>The length of time a job will pend is determined by several factors; how many other jobs are in the queue ahead or your job and how many resources your job is requesting are most the most important factors. One key principle when requesting resources is to always try to request as few resources as you need to get your job done. This will ensure your job pends in the queue for as little time as necessary. To get a rough idea of what resources are needed, you can profile your code/jobs in an sdev session in real-time with <code>htop</code>, <code>nvtop</code>, <code>sacct</code> etc. The basic concept is to tell the scheduler what resources your job needs and how long is should run. These resources are:</p> <p>CPUs: How many CPUs the program your are calling the in the sbatch script needs, unless it can utilize multiple CPUs at once you should request a single CPU. Check your code's documentation or try running in an interactive session with <code>sdev</code> and run htop if you are unsure.</p> <p>GPUs: If your code is GPU enabled, how many GPUs does your code need? Use the diagnostic tool <code>nvtop</code> to see if your code is capable of running on multiple GPUs and how much GPU memory it's using in real-time.</p> <p>memory (RAM): How much memory your job will consume. Some things to consider, will it load a large file or matrix into memory? Does it consume a lot of memory on your laptop? Often the default memory is sufficient for many jobs.</p> <p>time: How long will it take for you code to run to completion?</p> <p>partition: What set of compute nodes on Sherlock will you run on, normal, gpu, owners, bigmem? Use the <code>sh_part</code> command to see what partitions you are allowed to run on. The default partition on Sherlock is the normal partition.</p> <p>Next, you tell the scheduler what your job should should do: load modules and run your code. Note that any logic you can code into a bash script with the bash scripting language can also be coded into an sbatch script.</p> <p>This example job, will run the Python script mycode.py for 10 minutes on the normal partition using 1 CPU and 8 GB of memory. To aid in debugging we are naming this job \"test_job\" and appending the Job ID (%j) to the two output files that Slurm creates when a job is run. The output files are written to the directory in which you launched your job in, you can also specify a different path. One file will contain any errors and the other will contain non-error output.</p> <p>Because it's a Python 3 script that uses some Numpy code, we need to load the python/3.6.1 and the py-numpy/1.19.2_py36 modules. The Python script is then called just as you would on the command line at the end of the sbatch script:</p> <p>sbatch script:</p> <p><pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_job\n#SBATCH --output=test_job.%j.out\n#SBATCH --error=test_job.%j.err\n#SBATCH --time=10:00\n#SBATCH -p normal\n#SBATCH -c 1\n#SBATCH --mem=8GB\nmodule load python/3.6.1\nmodule load py-numpy/1.19.2_py36\npython3 mycode.py\n</code></pre> Create and edit the sbatch script with a text editor like vim/nano or the OnDemand file manager. Then save the file, in this example we call it \"test.sbatch\".</p> <p>Submit to the scheduler with the <code>sbatch</code> command:</p> <p><pre><code>$sbatch test.sbatch\n</code></pre> Monitor your job and job ID in the queue with the squeue command:</p> <pre><code>$squeue -u $USER\n   JOBID     PARTITION     NAME     USER    ST       TIME  NODES  NODELIST(REASON)\n   44915821    normal    test_job  &lt;userID&gt;  PD       0:00      1 (Priority)\n</code></pre> <p>Notice that the jobs state (ST) in pending (PD)</p> <p>Once the job starts to run that will chage to R:</p> <pre><code>$squeue -u $USER\n    JOBID     PARTITION     NAME     USER     ST      TIME  NODES   NODELIST(REASON)\n    44915854    normal test_job  &lt;userID&gt;     R      0:10     1     sh02-01n49\n</code></pre> <p>Here you can see it has been running (R) on the compute node sh02-01n49 for 10 seconds.  While your job is running you have ssh access to that node and can run diagnostic tools such as <code>htop</code> and <code>nvtop</code> in order to monitor your job's memory and CPU/GPU utilization in real-time. You can also manage this job based on the JobID assigned to it (44915854). For example the job can be cancelled with the <code>scancel</code> command.</p>","title":"Batch jobs"},{"location":"docs/user-guide/running-jobs/#available-resources","tags":["slurm"],"text":"<p>Whether you are submitting a batch job, or an or interactive job, it's important to know the resources that are available to you. For this reason, we provide <code>sh_part</code>, a command-line tool to help answer questions such as:</p> <ul> <li>which partitions do I have access to?</li> <li>how many jobs are running on them?</li> <li>how many CPUs can I use?</li> <li>where should I submit my jobs?</li> </ul> <p><code>sh_part</code> can be executed on any login or compute node to see what partitions are available to you, and its output looks like this:</p> <pre><code>$ sh_part\n     QUEUE STA   FREE  TOTAL   FREE  TOTAL RESORC  OTHER MAXJOBTIME    CORES       NODE   GRES\n PARTITION TUS  CORES  CORES  NODES  NODES PENDNG PENDNG  DAY-HR:MN    /NODE     MEM-GB (COUNT)\n    normal   *    153   1792      0     84    23k    127    7-00:00    20-24    128-191 -\n    bigmem         29     88      0      2      0      8    1-00:00    32-56   512-3072 -\n       dev         31     40      0      2      0      0    0-02:00       20        128 -\n       gpu         47    172      0      8    116      1    7-00:00    20-24    191-256 gpu:4(S:0-1)(2),gpu:4(S:0)(6)\n</code></pre> <p>The above example shows four possible partitions where jobs can be submitted: <code>normal,</code> <code>bigmem,</code> <code>dev,</code> or <code>gpu.</code> It also provides additional information such as the maximum amount of time allowed in each partition (<code>MAXJOBTIME</code>), the number of other jobs already in queue, along with the ranges of memory available on nodes in each partition.</p> <ul> <li>in the <code>QUEUE PARTITION</code> column, the <code>*</code> character indicates the default   partition.</li> <li>the <code>RESOURCE PENDING</code> column shows the core count of pending jobs that are   waiting on resources,</li> <li>the <code>OTHER PENDING</code> column lists core counts for jobs that are pending for   other reasons, such as licenses, user, group or any other limit,</li> <li>the <code>GRES</code> column shows the number and type of Generic RESsources available   in that partition (typically, GPUs), which CPU socket they're available   from, and the number of nodes that feature that specific GRES combination.   So for instance, in the output above, <code>gpu:4(S:0-1)(2)</code> means that the <code>gpu</code>   partition features 2 nodes with 4 GPUs each, and that those GPUs are   accessible from both CPU sockets (<code>S:0-1</code>).</li> </ul>","title":"Available resources"},{"location":"docs/user-guide/running-jobs/#recurring-jobs","tags":["slurm"],"text":"<p>Warning</p> <p><code>Cron</code> tasks are not supported on Sherlock.</p>  <p>Users are not allowed to create <code>cron</code> jobs on Sherlock, for a variety of reasons:</p> <ul> <li>resources limits cannot be easily enforced in <code>cron</code> jobs, meaning that a   single user can end up monopolizing all the resources of a login node,</li> <li>no amount of resources can be guaranteed when executing a <code>cron</code> job, leading   to unreliable runtime and performance,</li> <li>user <code>cron</code> jobs have the potential of bringing down whole   nodes by creating fork bombs, if they're not carefully crafted and tested,</li> <li>compute and login nodes could be redeployed at any time, meaning that   <code>cron</code> jobs scheduled there could go away without the user being notified,   and cause all sorts of unexpected results,</li> <li><code>cron</code> jobs could be mistakenly scheduled on several nodes and run multiple   times, which could result in corrupted files.</li> </ul> <p>As an alternative, if you need to run recurring tasks at regular intervals, we recommend the following approach: by using the <code>--begin</code> job submission option, and creating a job that resubmits itself once it's done, you can virtually emulate the behavior and benefits of a <code>cron</code> job, without its disadvantages: your task will be scheduled on a compute node, and use all of the resources it requested, without being impacted by anything else.</p> <p>Depending on your recurring job's specificities, where you submit it and the state of the cluster at the time of execution, the starting time of that task may not be guaranteed and result in a delay in execution, as it will be scheduled by Slurm like any other jobs. Typical recurring jobs, such as file synchronization, database updates or backup tasks don't require strict starting times, though, so most users find this an acceptable trade-off.</p> <p>The table below summarizes the advantages and inconvenients of each approach:</p>     Cron tasks Recurring jobs     Authorized on Sherlock     Dedicated resources for the task     Persistent across node redeployments     Unique, controlled execution     Precise schedule","title":"Recurring jobs"},{"location":"docs/user-guide/running-jobs/#recurrent-job-example","tags":["slurm"],"text":"<p>The script below presents an example of such a recurrent job, that would emulate a <code>cron</code> task. It will append a timestamped line to a <code>cron.log</code> file in your <code>$HOME</code> directory and run every 7 days.</p> cron.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=cron\n#SBATCH --begin=now+7days\n#SBATCH --dependency=singleton\n#SBATCH --time=00:02:00\n#SBATCH --mail-type=FAIL\n\n\n## Insert the command to run below. Here, we're just storing the date in a\n## cron.log file\ndate -R &gt;&gt; $HOME/cron.log\n\n## Resubmit the job for the next execution\nsbatch $0\n</code></pre> <p>If the job payload (here the <code>date</code> command) fails for some reason and generates and error, the job will not be resubmitted, and the user will be notified by email.</p> <p>We encourage users to get familiar with the submission options used in this script by giving a look at the <code>sbatch</code> man page, but some details are given below:</p>    Submission\u00a0option\u00a0or\u00a0command Explanation     <code>--job-name=cron</code> makes it easy to identify the job, is used by the  <code>--dependency=singleton</code> option to identify identical jobs, and will allow  cancelling the job by name (because its jobid will change each time it's  submitted)   <code>--begin=now+7days</code> will instruct the scheduler to not even consider the job   for scheduling before 7 days after it's been submitted   <code>--dependency=singleton</code> will make sure that only one <code>cron</code> job runs at any given time   <code>--time=00:02:00</code> runtime limit for the job (here 2 minutes). You'll need to adjust the value   depending on the task you need to run (shorter runtime requests usually   result in the job running closer to the clock mark)   <code>--mail-type=FAIL</code> will send an email notification to the user if the job ever fails   <code>sbatch $0</code> will resubmit the job script by calling its own name (<code>$0</code>)   after successful execution    <p>You can save the script as <code>cron.sbatch</code> or any other name, and submit it with:</p> <pre><code>$ sbatch cron.sbatch\n</code></pre> <p>It will start running for the first time 7 days after you submit it, and it will continue to run until you cancel it with the following command (using the job name, as defined by the <code>--job-name</code> option):</p> <pre><code>$ scancel -n cron\n</code></pre>","title":"Recurrent job example"},{"location":"docs/user-guide/running-jobs/#persistent-jobs","tags":["slurm"],"text":"<p>Recurring jobs described above are a good way to emulate <code>cron</code> jobs on Sherlock, but don't fit all needs, especially when a persistent service is required.</p> <p>For instance, workflows that require a persistent database connection would benefit from an ever-running database server instance. We don't provide persistent database services on Sherlock, but instructions and examples on how to submit database server jobs are provided for MariaDB or PostgreSQL.</p> <p>In case those database instances need to run pretty much continuously (within the limits of available resources and runtime maximums), the previous approach described in the recurring jobs section could fall a bit short.  Recurring jobs are mainly designed for jobs that have a fixed execution time and don't reach their time limit, but need to run at given intervals (like synchronization or backup jobs, for instance).</p> <p>Because a database server process will never end within the job, and will continue until the job reaches its time limit, the last resubmission command (<code>sbatch $0</code>) will actually never be executed, and the job won't be resubmitted.</p> <p>To work around this, a possible approach is to catch a specific signal sent by the scheduler at a predefined time, before the time limit is reached, and then re-queue the job. This is easily done with the Bash <code>trap</code> command, which can be instructed to re-submit a job when it receives the <code>SIGUSR1</code> signal.</p>  <p>Automatically resubmitting a job doesn't make it immediately runnable</p> <p>Jobs that are automatically re-submitted using this technique won't restart right away: the will get back in queue and stay pending until their execution conditions (priority, resources, usage limits...) are satisfied.</p>","title":"Persistent jobs"},{"location":"docs/user-guide/running-jobs/#persistent-job-example","tags":["slurm"],"text":"<p>Here's the recurring job example from above, modified to:</p> <ol> <li>instruct the scheduler to send a <code>SIGUSR1</code> signal to the job 90    seconds3 before reaching its time limit (with the <code>#SBATCH    --signal</code> option),</li> <li>re-submit itself upon receiving that <code>SIGUSR1</code> signal (with the <code>trap</code>    command)</li> </ol> persistent.sbatch<pre><code>#!/bin/bash\n#\n#SBATCH --job-name=persistent\n#SBATCH --dependency=singleton\n#SBATCH --time=00:05:00\n#SBATCH --signal=B:SIGUSR1@90\n\n# catch the SIGUSR1 signal\n_resubmit() {\n    ## Resubmit the job for the next execution\n    echo \"$(date): job $SLURM_JOBID received SIGUSR1 at $(date), re-submitting\"\n    sbatch $0\n}\ntrap _resubmit SIGUSR1\n\n## Insert the command to run below. Here, we're just outputing the date every\n## 10 seconds, forever\n\necho \"$(date): job $SLURM_JOBID starting on $SLURM_NODELIST\"\nwhile true; do\n    echo \"$(date): normal execution\"\n    sleep 60\ndone\n</code></pre>  <p>Long running processes need to run in the background</p> <p>If your job's actual payload (the application or command you want to run) is running continuously for the whole duration of the job, it needs to be executed in the background, so the trap can be processed.</p> <p>To run your application in the background, just add a <code>&amp;</code> at the end of the command and then add a <code>wait</code> statement at the end of the script, to make the shell wait until the end of the job.</p> <p>For instance, if you were to run a PostgreSQL database server, the <code>while true ... done</code> loop in the previous example could be replaced by something like this:</p> <pre><code>postgres -i -D $DB_DIR &amp;\nwait\n</code></pre>","title":"Persistent job example"},{"location":"docs/user-guide/running-jobs/#persistent-jobid","tags":["slurm"],"text":"<p>One potential issue with having a persistent job re-submit itself when it reaches its runtime limit is that it will get a different <code>$JOBID</code> each time it's (re-)submitted.</p> <p>This could be particularly challenging when other jobs depend on it, like in the database server scenario, where client jobs would need to start only if the database server is running. This can be achieved with job dependencies, but those dependencies have to be expressed using jobids, so having the server job's id changing at each re-submission will be difficult to handle.</p> <p>To avoid this, the re-submission command (<code>sbatch $0</code>) can be replaced by a re-queuing command:</p> <pre><code>scontrol requeue $SLURM_JOBID\n</code></pre> <p>The benefit of that change is that the job will keep the same <code>$JOBID</code> across all re-submissions. And now, dependencies can be added to other jobs using that specific <code>$JOBID</code>, without having to worry about it changing. And there will be only one <code>$JOBID</code> to track for that database server job.</p> <p>The previous example can then be modified as follows:</p> persistent.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=persistent\n#SBATCH --dependency=singleton\n#SBATCH --time=00:05:00\n#SBATCH --signal=B:SIGUSR1@90\n\n# catch the SIGUSR1 signal\n_requeue() {\n    echo \"$(date): job $SLURM_JOBID received SIGUSR1, re-queueing\"\n    scontrol requeue $SLURM_JOBID\n}\ntrap '_requeue' SIGUSR1\n\n## Insert the command to run below. Here, we're just outputing the date every\n## 60 seconds, forever\n\necho \"$(date): job $SLURM_JOBID starting on $SLURM_NODELIST\"\nwhile true; do\n    echo \"$(date): normal execution\"\n    sleep 60\ndone\n</code></pre> <p>Submitting that job will produce an output similar to this:</p> <pre><code>Mon Nov  5 10:30:59 PST 2018: Job 31182239 starting on sh-06-34\nMon Nov  5 10:30:59 PST 2018: normal execution\nMon Nov  5 10:31:59 PST 2018: normal execution\nMon Nov  5 10:32:59 PST 2018: normal execution\nMon Nov  5 10:33:59 PST 2018: normal execution\nMon Nov  5 10:34:59 PST 2018: Job 31182239 received SIGUSR1, re-queueing\nslurmstepd: error: *** JOB 31182239 ON sh-06-34 CANCELLED AT 2018-11-05T10:35:06 DUE TO JOB REQUEUE ***\nMon Nov  5 10:38:11 PST 2018: Job 31182239 starting on sh-06-34\nMon Nov  5 10:38:11 PST 2018: normal execution\nMon Nov  5 10:39:11 PST 2018: normal execution\n</code></pre> <p>The job runs for 5 minutes, then received the <code>SIGUSR1</code> signal, is re-queued, restarts for 5 minutes, and so on, until it's properly <code>scancel</code>led.</p>   <ol> <li> <p>The dedicated partition that <code>sdev</code> uses by default only allows   up to 2 cores and 8 GB or memory per user at any given time. So if you need   more resources for your interactive session, you may have to specify a   different partition. See the Partitions section for more   details.\u00a0\u21a9</p> </li> <li> <p>Please note that your SSH session will be attached to your   running job, and that resources used by that interactive shell will count   towards your job's resource limits. So if you start a process using large   amounts of memory via SSH while your job is running, you may hit the job's   memory limits, which will trigger its termination.\u00a0\u21a9</p> </li> <li> <p>Due to the resolution of event handling by the scheduler, the   signal may be sent up to 60 seconds earlier than specified.\u00a0\u21a9</p> </li> </ol>","title":"Persistent <code>$JOBID</code>"},{"location":"docs/user-guide/troubleshoot/","text":"<p>Sherlock is a resource for research, and as such, it is in perpetual evolution, as hardware, applications, libraries, and modules are added, updated, and/or modified on a regular basis.  Sometimes issues can appear where none existed before. When you find something missing or a behavior that seems odd, please let us know.</p>","title":"Troubleshooting"},{"location":"docs/user-guide/troubleshoot/#how-to-submit-a-support-request","text":"<p>Google it first!</p> <p>When encountering issues with software, if the misbehavior involves an error message, the first step should always be to look up the error message online.  There's a good chance somebody stumbled upon the same hurdles before, and may even provide some fix or workaround.</p> <p>One of the most helpful Google searches is <code>your_application sbatch</code>.  For example if you're having trouble submitting jobs or allocating resources (CPUs, time, memory) with Cell Ranger, search for <code>cell ranger sbatch</code> to see how others have successfully run your application on a cluster.</p>  <p>If you're facing issues you can't figure out, we're here to help. Feel free to email us at srcc-support@stanford.edu, but please keep the following points in mind to ensure a timely and relevant response to your support requests.</p>  <p>Please provide relevant information</p> <p>We need to understand the issue you're facing, and in most cases, we need to be able to reproduce it, so it could be diagnosed and addressed. Please make sure to provide enough information so we could help you in the best possible way.</p>  <p>This typically involves providing the following information:</p> <ul> <li>your SUNet ID,</li> <li>some context about your problem (were you submitting a job, copying a file,   compiling an application?),</li> <li>if relevant, the full path to the files involved in your question or problem,</li> <li>the name of node where you received the error (usually displayed in your   command-line prompt),</li> <li>the command(s) you ran, and/or the job submission script(s) you used,</li> <li>the relevant job ID(s),</li> <li>the exact, entire error message (or trace) you received.</li> </ul>  <p>Error messages are critical</p> <p>This is very important. Without proper error messages, there is nothing we can do to help. And \"it doesn't work\" is not a proper error message.  Also, please cut and paste the actual text of the output, commands, and error messages rather than screenshots in your tickets. That way it is much easier for us to try to replicate your errors.</p>  <p>You can avoid email back and forth where we ask for all the relevant details, and thus delay the problem resolution, by providing all this information from the start. This will help us get to your problem immediately.</p>","title":"How to submit a support request"},{"location":"docs/tags/","text":"<p>Here is a list of documentation tags:</p>","title":"Tags"},{"location":"docs/tags/#connection","text":"<ul> <li>Connection options</li> <li>Connecting</li> <li>Data transfer</li> </ul>","title":"connection"},{"location":"docs/tags/#slurm","text":"<ul> <li>Job management</li> <li>Submitting jobs</li> <li>Running jobs</li> </ul>","title":"slurm"},{"location":"docs/tags/#tech","text":"<ul> <li>Technical specifications</li> <li>Facts</li> </ul>","title":"tech"}]}